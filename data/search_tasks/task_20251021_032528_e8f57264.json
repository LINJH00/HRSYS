{
  "task_id": "task_20251021_032528_e8f57264",
  "spec": {
    "top_n": 6,
    "years": [
      2025,
      2024,
      2026
    ],
    "venues": [
      "ICLR",
      "ICML",
      "NeurIPS",
      "ACL",
      "EMNLP",
      "NAACL"
    ],
    "keywords": [
      "text-generation",
      "diffusion model"
    ],
    "research_field": "Natural Language Processing",
    "must_be_current_student": true,
    "degree_levels": [
      "PhD"
    ],
    "author_priority": [
      "first",
      "last"
    ],
    "extra_constraints": []
  },
  "pos": 16,
  "terms": [
    "text generation, diffusion model ICLR 2026",
    "text generation, diffusion model ICML 2026",
    "text generation, diffusion model NeurIPS 2026",
    "text generation, diffusion model ACL 2026",
    "text generation, diffusion model EMNLP 2026",
    "text generation, diffusion model NAACL 2026",
    "text generation, diffusion model ICLR 2025",
    "text generation, diffusion model ICML 2025",
    "text generation, diffusion model NeurIPS 2025",
    "text generation, diffusion model ACL 2025",
    "text generation, diffusion model EMNLP 2025",
    "text generation, diffusion model NAACL 2025",
    "text generation, diffusion model ICLR 2024",
    "text generation, diffusion model ICML 2024",
    "text generation, diffusion model NeurIPS 2024",
    "text generation, diffusion model ACL 2024",
    "text generation, diffusion model EMNLP 2024",
    "text generation, diffusion model NAACL 2024"
  ],
  "rounds_completed": 2,
  "candidates_accum": {},
  "all_serp": [
    {
      "title": "GitHub - YangLing0818/RPG-DiffusionMaster: [ICML 2024] Mastering Text-to-Image Diffusion: Recaptioning, Planning, and Generating with Multimodal LLMs (RPG)",
      "url": "https://github.com/YangLing0818/RPG-DiffusionMaster",
      "snippet": "Mastering Text-to-Image Diffusion: Recaptioning, Planning, and Generating with Multimodal LLMs - ICML 2024 This repository contains the official implementation of our RPG, accepted by ICML 2024.",
      "engine": "duckduckgo",
      "authors": [],
      "term": "text generation, diffusion model ICML 2024"
    },
    {
      "title": "ICML 2024 Papers",
      "url": "https://icml.cc/virtual/2024/papers.html",
      "snippet": "Unified Generation, Reconstruction, and Representation: Generalized Diffusion with Adaptive Latent Encoding-Decoding CogBench: a large language model walks into a psychology lab Human-like Category Learning by Injecting Ecological Priors from Large Language Models into Neural Networks",
      "engine": "duckduckgo",
      "authors": [],
      "term": "text generation, diffusion model ICML 2024"
    },
    {
      "title": "Diffusion models in text generation: a survey - PMC",
      "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC10909201/",
      "snippet": "(2022a).Yu P, Xie S, Ma X, Jia B, Pang B, Gao R, Zhu Y, Zhu S-C, Wu Y. Latent diffusion energy-based model for interpretable text modeling. International Conference on Machine Learning (ICML 2022).2022a. [Google Scholar] Yu et al. (2022b).Yu W, Zhu C, Li Z, Hu Z, Wang Q, Ji H, Jiang M. A survey of knowledge-enhanced text generation.",
      "engine": "duckduckgo",
      "authors": [],
      "term": "text generation, diffusion model ICML 2024"
    },
    {
      "title": "Mastering Text-to-Image Diffusion: Recaptioning, Planning, and ...",
      "url": "https://arxiv.org/abs/2401.11708",
      "snippet": "Diffusion models have exhibit exceptional performance in text-to-image generation and editing. However, existing methods often face challenges when handling complex text prompts that involve multiple objects with multiple attributes and relationships. In this paper, we propose a brand new training-free text-to-image generation/editing framework, namely Recaption, Plan and Generate (RPG ...",
      "engine": "duckduckgo",
      "authors": [],
      "term": "text generation, diffusion model ICML 2024"
    },
    {
      "title": "GitHub - YangLing0818/ContextDiff: [ICLR 2024] Contextualized Diffusion Models for Text-Guided Image and Video Generation",
      "url": "https://github.com/YangLing0818/ContextDiff",
      "snippet": "This repository contains the official implementation of ContextDiff published in ICLR 2024. Cross-Modal Contextualized Diffusion Models for Text-Guided Visual Generation and Editing Ling Yang, Zhilong Zhang, Zhaochen Yu, Jingwei Liu, Minkai Xu, Stefano Ermon, Bin Cui Peking University, Stanford University",
      "engine": "brave",
      "authors": [],
      "term": "text generation, diffusion model ICML 2024"
    },
    {
      "title": "GenLaw ICML 2024 - Accepted Papers â€” The GenLaw Center",
      "url": "https://www.genlaw.org/2024-icml-papers",
      "snippet": "We assemble a dataset of creative commons licensed images and train a set of open diffusion models on that dataset that are competitive with Stable Diffusion 2. This task presents two challenges: high-resolution CC images 1) lack the captions ...",
      "engine": "brave",
      "authors": [],
      "term": "text generation, diffusion model ICML 2024"
    },
    {
      "title": "Empowering Diffusion Models on the Embedding Space for Text Generation",
      "url": "https://aclanthology.org/2024.naacl-long.261/",
      "snippet": "Based on the above analysis, we propose Difformer, an embedding diffusion model based on Transformer. Experiments on varieties of seminal text generation tasks show the effectiveness of the proposed methods and the superiority of Difformer over previous state-of-the-art embedding diffusion baselines.",
      "engine": "duckduckgo",
      "authors": [],
      "term": "text generation, diffusion model ICML 2024"
    },
    {
      "title": "ICML 2024 Orals",
      "url": "https://icml.cc/virtual/2024/events/oral",
      "snippet": "For comparable model sizes, SEDD beats existing language diffusion paradigms (reducing perplexity by $25$-$75$%) and is competitive with autoregressive models, in particular outperforming GPT-2. Furthermore, compared to autoregressive mdoels, SEDD generates faithful text without requiring distribution annealing techniques like temperature scaling (around $6$-$8\\times$ better generative perplexity than un-annealed GPT-2), can trade compute and quality (similar quality with $32\\times$ fewer network evaluations), and enables controllable infilling (matching nucleus sampling quality while enabling other strategies besides left to right prompting).",
      "engine": "brave",
      "authors": [],
      "term": "text generation, diffusion model ICML 2024"
    },
    {
      "title": "ICML Poster Discrete Diffusion Modeling by Estimating the Ratios of the Data Distribution",
      "url": "https://icml.cc/virtual/2024/poster/34686",
      "snippet": "For comparable model sizes, SEDD beats existing language diffusion paradigms (reducing perplexity by $25$-$75$%) and is competitive with autoregressive models, in particular outperforming GPT-2. Furthermore, compared to autoregressive mdoels, SEDD generates faithful text without requiring distribution annealing techniques like temperature scaling (around $6$-$8\\times$ better generative perplexity than un-annealed GPT-2), can trade compute and quality (similar quality with $32\\times$ fewer network evaluations), and enables controllable infilling (matching nucleus sampling quality while enabling other strategies besides left to right prompting).",
      "engine": "brave",
      "authors": [],
      "term": "text generation, diffusion model ICML 2024"
    },
    {
      "title": "[2408.04220] Diffusion Guided Language Modeling - arXiv.org",
      "url": "https://arxiv.org/abs/2408.04220",
      "snippet": "In this paper we use a guided diffusion model to produce a latent proposal that steers an auto-regressive language model to generate text with desired properties. Our model inherits the unmatched fluency of the auto-regressive approach and the plug-and-play flexibility of diffusion.",
      "engine": "duckduckgo",
      "authors": [],
      "term": "text generation, diffusion model ICML 2024"
    },
    {
      "title": "[2410.21357] Energy-Based Diffusion Language Models for Text Generation",
      "url": "https://arxiv.org/abs/2410.21357",
      "snippet": "We further show that, without any generation performance drop, our framework offers a 1.3$\\times$ sampling speedup over existing diffusion models. Reproduced code is available at this https URL. From: Minkai Xu [view email] [v1] Mon, 28 Oct 2024 17:25:56 UTC (808 KB) [v2] Thu, 20 Feb 2025 01:23:30 UTC (813 KB) [v3] Fri, 28 Feb 2025 08:41:03 UTC (815 KB) [v4] Fri, 7 Mar 2025 04:28:45 UTC (813 KB)",
      "engine": "duckduckgo",
      "authors": [],
      "term": "text generation, diffusion model EMNLP 2025"
    },
    {
      "title": "The 2025 Conference on Empirical Methods in Natural Language Processing - EMNLP 2025",
      "url": "https://2025.emnlp.org/",
      "snippet": "The 2025 Conference on Empirical Methods in Natural Language Processing (EMNLP 2025) will be held in Suzhou, China from November 4th to November 9th, 2025. Given the expected popularity of EMNLP 2025, we may need to limit registration.",
      "engine": "duckduckgo",
      "authors": [],
      "term": "text generation, diffusion model EMNLP 2025"
    },
    {
      "title": "Multimodal LLMs for AI DevOps | NEC Labs",
      "url": "https://www.nec-labs.com/research/media-analytics/projects/multimodal-llms-for-ai-devops/",
      "snippet": "March 12, 2025/Applications, Libraries, and Tools for Computational Science and Machine Learning on Heterogeneous HPC Environments Workshop at PDP 2025 Â· Marine litter detection is crucial for environmental monitoring, yet the imbalance in existing datasets limits model performance in identifying various types of waste accurately. This paper presents an efficient data augmentation pipeline that combines generative diffusion models (e.g., Stable Diffusion)",
      "engine": "brave",
      "authors": [],
      "term": "text generation, diffusion model EMNLP 2025"
    },
    {
      "title": "Awesome Diffusion Language Models - GitHub",
      "url": "https://github.com/VILA-Lab/Awesome-DLMs",
      "snippet": "Surveys [12 Aug 2025] A Survey on Parallel Text Generation: From Parallel Decoding to Diffusion Language Models [16 Jun 2025] Discrete Diffusion in Large Language and Multimodal Models: A Survey [23 Feb 2024] Diffusion models in text generation: a survey (PeerJ Computer Science) [29 Jun 2023] An Overview of Diffusion Models for Text Generation ...",
      "engine": "duckduckgo",
      "authors": [],
      "term": "text generation, diffusion model EMNLP 2025"
    },
    {
      "title": "A Comparative Analysis of Diffusion and Autoregressive Models for Text Generation: Architectures, Capabilities, and Frontiers | by Greg Robison | Medium",
      "url": "https://gregrobison.medium.com/a-comparative-analysis-of-diffusion-and-autoregressive-models-for-text-generation-architectures-99fb24fa390c",
      "snippet": "A minimal implementation of diffusion models for text generation â€” GitHub, accessed June 13, 2025, https://github.com/madaan/minimal-text-diffusion",
      "engine": "brave",
      "authors": [],
      "term": "text generation, diffusion model EMNLP 2025"
    },
    {
      "title": "Energy-Based Diffusion Language Models for Text Generation",
      "url": "https://arxiv.org/html/2410.21357v2",
      "snippet": "In this paper, we introduced Energy-based Diffusion Language Model (EDLM), which integrates energy-based models with discrete diffusion models to address the limitations of parallel text generation.",
      "engine": "duckduckgo",
      "authors": [],
      "term": "text generation, diffusion model EMNLP 2025"
    },
    {
      "title": "EMNLP: Empirical Methods in Natural Language Processing 2026 2025 2024 ...",
      "url": "http://www.wikicfp.com/cfp/program?id=883",
      "snippet": "",
      "engine": "brave",
      "authors": [],
      "term": "text generation, diffusion model EMNLP 2025"
    },
    {
      "title": "bansky-cl/Diffusion-LM-Papers - GitHub",
      "url": "https://github.com/bansky-cl/Diffusion-LM-Papers",
      "snippet": "Listing some diffusion papers in NLP domain I have read, text generation is main, table will continue to be updated. - bansky-cl/Diffusion-LM-Papers",
      "engine": "duckduckgo",
      "authors": [],
      "term": "text generation, diffusion model EMNLP 2025"
    },
    {
      "title": "EMNLP 23 Tutorial on Creative Natural Language Generation",
      "url": "https://emnlp2023-creative-nlg.github.io",
      "snippet": "Incorporating Domain Knowledge DOMAIN KNOWLEDGE FROM WEB Comprehending and Generating Apt Metaphors: A Web-driven, Case-based Approach to Figurative Language Veale et al (2007) DOMAIN KNOWLEDGE FROM EXTERNAL KNOWLEDGE MODELS Generating similes effortlessly like a Pro : A Style Transfer Approach for Simile Generation Chakrabarty et al (2020) MERMAID: Metaphor Generation with Symbolism and Discriminative Decoding Chakrabarty et al (2021) DOMAIN KNOWLEDGE FROM PROMPTING LLMS I Spy a Metaphor: Large Language Models and Diffusion Models Co-Create Visual Metaphors Chakrabarty et al (2023)",
      "engine": "brave",
      "authors": [],
      "term": "text generation, diffusion model EMNLP 2025"
    },
    {
      "title": "Energy-based Diffusion Language Models for Text Generation",
      "url": "https://research.nvidia.com/labs/lpr/publication/xu2024eddm/",
      "snippet": "Energy-based Diffusion Language Models for Text Generation Minkai Xu, Tomas Geffner, Karsten Kreis, Weili Nie, Yilun Xu, Jure Leskovec, Stefano Ermon, Arash Vahdat April 2025 Cite arXiv Type",
      "engine": "duckduckgo",
      "authors": [],
      "term": "text generation, diffusion model EMNLP 2025"
    },
    {
      "title": "Text Diffusion Model with Encoder-Decoder Transformers for Sequence-to-Sequence Generation - ACL Anthology",
      "url": "https://aclanthology.org/2024.naacl-long.2/",
      "snippet": "In this work, we propose SeqDiffuSeq, a text diffusion model, to approach sequence-to-sequence text generation with an encoder-decoder Transformer architecture.To improve the generation performance, SeqDiffuSeq is equipped with the self-conditioning technique and our newly proposed adaptive noise schedule technique.",
      "engine": "duckduckgo",
      "authors": [],
      "term": "text generation, diffusion model ACL 2024"
    },
    {
      "title": "GitHub - Yuanhy1997/SeqDiffuSeq: Text Diffusion Model with Encoder ...",
      "url": "https://github.com/Yuanhy1997/SeqDiffuSeq",
      "snippet": "Text Diffusion Model with Encoder-Decoder Transformers for Sequence-to-Sequence Generation [NAACL 2024] - Yuanhy1997/SeqDiffuSeq",
      "engine": "duckduckgo",
      "authors": [],
      "term": "text generation, diffusion model ACL 2024"
    },
    {
      "title": "Text Diffusion Model with Encoder-Decoder Transformers for Sequence-to ...",
      "url": "https://papers.cool/venue/2024.naacl-long.2@ACL",
      "snippet": "The diffusion model, a new generative modeling paradigm, has achieved great success in image, audio, and video generation.However, considering the discrete categorical nature of the text, it is not trivial to extend continuous diffusion models to natural language. In this work, we propose SeqDiffuSeq, a text diffusion model, to approach sequence-to-sequence text generation with an encoder ...",
      "engine": "duckduckgo",
      "authors": [],
      "term": "text generation, diffusion model ACL 2024"
    },
    {
      "title": "Diffusion Guided Language Modeling - ACL Anthology",
      "url": "https://aclanthology.org/2024.findings-acl.887/",
      "snippet": "In this paper we use a guided diffusion model to produce a latent proposal that steers an auto-regressive language model to generate text with desired properties. Our model inherits the unmatched fluency of the auto-regressive approach and the ...",
      "engine": "brave",
      "authors": [],
      "term": "text generation, diffusion model ACL 2024"
    },
    {
      "title": "DiffusPoll: Conditional Text Diffusion Model for Poll Generation - ACL ...",
      "url": "https://aclanthology.org/2024.findings-acl.54/",
      "snippet": "To achieve this, we introduce DiffusPoll, a novel paradigm for poll generation based on a non-autoregressive diffusion model that can generate diversified and high-quality samples. Under the new paradigm, we design a task-specific mask strategy tailored to the inherent logic of polls to optimize controlled generation.",
      "engine": "duckduckgo",
      "authors": [],
      "term": "text generation, diffusion model ACL 2024"
    },
    {
      "title": "ACL 2024 Tutorial on Automatic and Human-AI Interactive Text Generation",
      "url": "https://acl2024-text-generation-tutorial.github.io/",
      "snippet": "With a special focus on text ... (1) Evaluation of LLM-generated text: from BLEU to reward models and LLM evaluators. (2) Advances in text simplification and rewriting models. (3) Innovations in decoding methods, distillation, and diffusion language models....",
      "engine": "brave",
      "authors": [],
      "term": "text generation, diffusion model ACL 2024"
    },
    {
      "title": "Text generation with diffusion language models | Proceedings of the ...",
      "url": "https://dl.acm.org/doi/10.5555/3618408.3619275",
      "snippet": "In this paper, we introduce a novel dIffusion language modEl pre-training framework for text generation, which we call GENIE. GENIE is a large-scale pre-trained diffusion language model that consists of an encoder and a diffusion-based decoder, which can generate text by gradually transforming a random noise sequence into a coherent text sequence.",
      "engine": "duckduckgo",
      "authors": [],
      "term": "text generation, diffusion model ACL 2024"
    },
    {
      "title": "Next Semantic Scale Prediction via Hierarchical Diffusion Language Models",
      "url": "https://arxiv.org/pdf/2510.08632",
      "snippet": "modeling: Scalable image generation via next-scale prediction. Advances in neural information ... Hofmann. Generalized interpolating discrete diffusion. arXiv preprint arXiv:2503.04482, 2025. [35] Minkai Xu, Tomas Geffner, Karsten Kreis, Weili Nie, Yilun Xu, Jure Leskovec, Stefano Ermon, and Arash Vahdat. Energy-based diffusion language models for text generation.",
      "engine": "brave",
      "authors": [],
      "term": "text generation, diffusion model NeurIPS 2025"
    },
    {
      "title": "[2505.22165] Unifying Continuous and Discrete Text Diffusion with Non ...",
      "url": "https://arxiv.org/abs/2505.22165",
      "snippet": "Diffusion models have emerged as a promising approach for text generation, with recent works falling into two main categories: discrete and continuous diffusion models. Discrete diffusion models apply token corruption independently using categorical distributions, allowing for different diffusion progress across tokens but lacking fine-grained control. Continuous diffusion models map tokens to ...",
      "engine": "duckduckgo",
      "authors": [],
      "term": "text generation, diffusion model NeurIPS 2025"
    },
    {
      "title": "Our paper on CaDDi, a new framework for text generation ... - LinkedIn",
      "url": "https://www.linkedin.com/posts/david-van-dijk-b440286_neurips2025-ai-machinelearning-activity-7378420437879324672-rkaS",
      "snippet": "ðŸš¨ I'm excited to share that our paper, \"Non-Markovian Discrete Diffusion with Causal Language Models\" has been accepted at NeurIPS 2025! ðŸŽ‰ We introduce CaDDi, a new framework that ...",
      "engine": "duckduckgo",
      "authors": [],
      "term": "text generation, diffusion model NeurIPS 2025"
    },
    {
      "title": "Diffusion-LM Improves Controllable Text Generation | OpenReview",
      "url": "https://openreview.net/forum?id=3s9IrEsjLyk",
      "snippet": "Published: 31 Oct 2022, Last Modified: 04 Aug 2025 NeurIPS 2022 Accept Readers: Everyone Keywords: controllable text generation, controlled generation, infilling, language model, diffusion model TL;DR: We propose a non-autoregressive language model based on continuous diffusions, which demonstrate strong performance in controllable text generation.",
      "engine": "brave",
      "authors": [],
      "term": "text generation, diffusion model NeurIPS 2025"
    },
    {
      "title": "NeurIPS 2024: Diffusion Themes and Memes â€“ Joshua Bambrickâ€™s Blog",
      "url": "https://joshbambrick.com/blog/posts/neurips-2024/",
      "snippet": "@article{bambrick2025neurips2024, title = \"NeurIPS 2024: Diffusion Themes and Memes\", author = \"Bambrick, Joshua\", journal = \"Joshua Bambrick's Blog\", year = \"2025\", month = \"Jan\", url = \"https://joshbambrick.com/blog/posts/neurips-2025/\" } ... Abramson J, Adler J, Dunger J, et al (2024) Accurate structure prediction of biomolecular interactions with AlphaFold 3. Nature ... Luo C (2022) Understanding diffusion models: A unified perspective.",
      "engine": "brave",
      "authors": [],
      "term": "text generation, diffusion model NeurIPS 2025"
    },
    {
      "title": "GitHub - keshik6/grafting: [NeurIPS 2025 Oral] Exploring Diffusion ...",
      "url": "https://github.com/keshik6/grafting",
      "snippet": "image-generation post-training self-attention convolutions diffusion-models grafting linear-attention text-to-image-generation architecture-research diffusion-transformer sub-quadratic-attention model-grafting hyena-operator model-architecture-editing diffusion-transformers architecture-editing hyena-x hyena-y mamba-2 Readme Apache-2.0 license",
      "engine": "duckduckgo",
      "authors": [],
      "term": "text generation, diffusion model NeurIPS 2025"
    },
    {
      "title": "VIPL's 9 papers on text-to-image diffusion models and other aspects are accepted by NeurIPS 2025----Visual Information Processing and Learning (VIPL)",
      "url": "https://vipl.ict.ac.cn/en/news/researchevents/202510/t20251014_782727.html",
      "snippet": "VIPL's 9 papers are accepted by NeurIPS 2025. NeurIPS, officially known as Annual Conference on Neural Information Processing Systems, is a top-tier conference in the field of artificial intelligence. The conference will be held in San Diego, USA from December 2nd to December 7th. The accepted papers are summarized as follows. 1. LightFair: Towards an Efficient Alternative for Fair T2I Diffusion via Debiasing Pre-trained Text Encoders (Boyu Han, Qianqian Xu, Shilong Bao, Zhiyong Yang, Kangli Zi, Qingming Huang)",
      "engine": "brave",
      "authors": [],
      "term": "text generation, diffusion model NeurIPS 2025"
    },
    {
      "title": "NeurIPS",
      "url": "https://neurips.cc/",
      "snippet": "About the Conference. The conference was founded in 1987 and is now a multi-track interdisciplinary annual meeting that includes invited talks, demonstrations, symposia, and oral",
      "engine": "duckduckgo",
      "authors": [],
      "term": "text generation, diffusion model NeurIPS 2025"
    },
    {
      "title": "NeurIPS Poster Next Semantic Scale Prediction via Hierarchical Diffusion Language Models",
      "url": "https://neurips.cc/virtual/2025/poster/116380",
      "snippet": "1 month ago - We derive closed-form expressions for the diffusion Evidence Lower Bound (ELBO), and show that HDLM can be implemented in a flexible manner and that it includes an existing discrete diffusion model as a special case. Extensive text generation experiments demonstrate the effectiveness of HDLM.",
      "engine": "brave",
      "authors": [],
      "term": "text generation, diffusion model NeurIPS 2025"
    },
    {
      "title": "NeurIPS Poster AR-Diffusion: Auto-Regressive Diffusion Model for Text Generation",
      "url": "https://neurips.cc/virtual/2023/poster/73068",
      "snippet": "This results in tokens on the left undergoing fewer denoising steps than those on the right, thereby enabling them to generate earlier and subsequently influence the generation of tokens on the right.In a series of experiments on various text generation tasks, including text summarization, machine translation, and common sense generation, AR-Diffusion clearly demonstrated its superiority over existing diffusion language models and that it can be $100\\times\\sim600\\times$ faster when achieving comparable results.",
      "engine": "brave",
      "authors": [],
      "term": "text generation, diffusion model NeurIPS 2025"
    },
    {
      "title": "ICLR 2024 Orals",
      "url": "https://iclr.cc/virtual/2024/events/oral",
      "snippet": "The performance improvement was obvious, however, they often fail to address inherent ambiguities of matching, such as textureless regions, repetitive patterns, large displacements, or noises. To address this, we propose DiffMatch, a novel conditional diffusion-based framework designed to explicitly model both the data and prior terms for dense matching. This is accomplished by leveraging a conditional denoising diffusion model that explic- itly takes matching cost and injects the prior within generative process.",
      "engine": "brave",
      "authors": [],
      "term": "text generation, diffusion model ICLR 2024"
    },
    {
      "title": "ICLR Poster Cross-Modal Contextualized Diffusion Models for Text-Guided Visual Generation and Editing",
      "url": "https://iclr.cc/virtual/2024/poster/17865",
      "snippet": "We propagate this context to all timesteps in the two processes to adapt their trajectories, thereby facilitating cross-modal conditional modeling. We generalize our contextualized diffusion to both DDPMs and DDIMs with theoretical derivations, and demonstrate the effectiveness of our model in evaluations with two challenging tasks: text-to-image generation, and text-to-video editing.",
      "engine": "brave",
      "authors": [],
      "term": "text generation, diffusion model ICLR 2024"
    },
    {
      "title": "ICLR Poster Let 2D Diffusion Model Know 3D-Consistency for Robust Text-to-3D Generation",
      "url": "https://iclr.cc/virtual/2024/poster/18523",
      "snippet": "Specifically, we introduce a ... diffusion model, through its generative capability, robustly infers dense structure from the sparse point cloud depth map and generates a geometrically consistent and coherent 3D scene....",
      "engine": "brave",
      "authors": [],
      "term": "text generation, diffusion model ICLR 2024"
    },
    {
      "title": "ICLR Poster AnyText: Multilingual Visual Text Generation and Editing",
      "url": "https://iclr.cc/virtual/2024/poster/18202",
      "snippet": "To address this issue, we introduce AnyText, a diffusion-based multilingual visual text generation and editing model, that focuses on rendering accurate and coherent text in the image.",
      "engine": "brave",
      "authors": [],
      "term": "text generation, diffusion model ICLR 2024"
    },
    {
      "title": "USC at ICLR 2024 - USC Viterbi | School of Engineering",
      "url": "https://viterbischool.usc.edu/news/2024/05/usc-at-iclr-2024/",
      "snippet": "We exploit these new relations to measure the compositional understanding of diffusion models, to do unsupervised localization of objects in images, and to measure effects when selectively editing images through prompt interventions. ... USC School of Advanced Computing and USC Viterbi researchers showcase breakthroughs in generative modeling, safety in imitation learning, human-aware planning... ... USC researchers present cutting-edge advances in AI and machine learning at ICLR 2025, highlighting breakthroughs in 3D vision, decision-making, and...",
      "engine": "brave",
      "authors": [],
      "term": "text generation, diffusion model ICLR 2024"
    },
    {
      "title": "Downloads 2024",
      "url": "https://iclr.cc/Downloads/2024",
      "snippet": "Differentially Private Synthetic Data via Foundation Model APIs 1: Images Â· DIFFTACTILE: A Physics-based Differentiable Tactile Simulator for Contact-rich Robotic Manipulation Â· Diffusion Generative Flow Samplers: Improving learning signals through partial trajectory optimization Â· Diffusion in Diffusion: Cyclic One-Way Diffusion for Text-Vision-Conditioned Generation",
      "engine": "brave",
      "authors": [],
      "term": "text generation, diffusion model ICLR 2024"
    },
    {
      "title": "ICLR 2024 Schedule",
      "url": "https://iclr.cc/virtual/2024/calendar",
      "snippet": "Neural Network-Based Score Estimation in Diffusion Models: Optimization and Generalization Â· FreeNoise: Tuning-Free Longer Video Diffusion via Noise Rescheduling Â· SE(3)-Stochastic Flow Matching for Protein Backbone Generation Â· LLM Blueprint: Enabling Text-to-Image Generation with Complex and Detailed Prompts",
      "engine": "brave",
      "authors": [],
      "term": "text generation, diffusion model ICLR 2024"
    },
    {
      "title": "ICLR Poster Localizing and Editing Knowledge In Text-to-Image Generative Models",
      "url": "https://iclr.cc/virtual/2024/poster/18667",
      "snippet": "Text-to-Image Diffusion Models such as Stable-Diffusion and Imagen have achieved unprecedented quality of photorealism with state-of-the-art FID scores on MS-COCO and other generation benchmarks. Given a caption, image generation requires fine-grained knowledge about attributes such as object ...",
      "engine": "brave",
      "authors": [],
      "term": "text generation, diffusion model ICLR 2024"
    },
    {
      "title": "NeurIPS Text-to-Audio Generation via Bridging Audio Language Model and Latent Diffusion",
      "url": "https://neurips.cc/virtual/2024/105743",
      "snippet": "These autoregressive models offer flexibility by predicting discrete audio tokens, but they often fail to achieve high fidelity. In this work, we propose an advanced system that integrates the autoregressive language model with the diffusion model, achieving flexible and refined audio generation.",
      "engine": "brave",
      "authors": [],
      "term": "text generation, diffusion model NeurIPS 2024"
    },
    {
      "title": "Meta-Diffu$B$: A Contextualized Sequence-to-Sequence Text Diffusion Model with Meta-Exploration",
      "url": "https://proceedings.neurips.cc/paper_files/paper/2024/hash/91d193b65d0b120d29503590827de1ea-Abstract-Conference.html",
      "snippet": "Part of Advances in Neural Information ... success in generating images, audio, video, and text. It has been adapted for sequence-to-sequence text generation (Seq2Seq) through DiffuSeq, termed the S2S-Diffusion model....",
      "engine": "brave",
      "authors": [],
      "term": "text generation, diffusion model NeurIPS 2024"
    },
    {
      "title": "DiffPano: Scalable and Consistent Text to Panorama Generation with Spherical Epipolar-Aware Diffusion",
      "url": "https://proceedings.neurips.cc/paper_files/paper/2024/hash/02c1d1d33dbfbaf03b3971bb542e72e2-Abstract-Conference.html",
      "snippet": "Then, we propose a novel text-driven panoramic generation framework, termed DiffPano, to achieve scalable, consistent, and diverse panoramic scene generation. Specifically, benefiting from the powerful generative capabilities of stable diffusion, we fine-tune a single-view text-to-panorama diffusion model with LoRA on the established panoramic video-text dataset.",
      "engine": "brave",
      "authors": [],
      "term": "text generation, diffusion model NeurIPS 2024"
    },
    {
      "title": "GitHub - Leiii-Cao/Text-DiFuse: This is the official code of the NeurIPS 2024 paper \"Text-DiFuse: An Interactive Multi-Modal Image Fusion Framework based on Text-modulated Diffusion Model\"",
      "url": "https://github.com/Leiii-Cao/Text-DiFuse",
      "snippet": "This is the official code of the NeurIPS 2024 paper \"Text-DiFuse: An Interactive Multi-Modal Image Fusion Framework based on Text-modulated Diffusion Model\" - Leiii-Cao/Text-DiFuse",
      "engine": "brave",
      "authors": [],
      "term": "text generation, diffusion model NeurIPS 2024"
    },
    {
      "title": "Self-Play Fine-Tuning of Diffusion Models for Text-to-Image Generation",
      "url": "https://proceedings.neurips.cc/paper_files/paper/2024/file/860c1c657deafe09f64c013c2888bd7b-Paper-Conference.pdf",
      "snippet": "When applying the self-play fine-tuning technique (Chen et al., 2024) to diffusion models, there Â· are two challenges: (a) an exponential or even infinite number of possible trajectories can lead to Â· the same image. The generator in a diffusion model operates through a sequence of intermediate",
      "engine": "brave",
      "authors": [],
      "term": "text generation, diffusion model NeurIPS 2024"
    },
    {
      "title": "NeurIPS 2024 Papers from Microsoft Research Asia - Microsoft Research",
      "url": "https://www.microsoft.com/en-us/research/articles/msra-neurips-2024-papers/",
      "snippet": "In this work, we propose a novel paradigm that generates each concept in 3D representation separately and then composes them with priors from Large Language Models (LLM) and 2D diffusion models. Specifically, given an input textual prompt, our scheme consists of three stages: 1) We leverage ...",
      "engine": "brave",
      "authors": [],
      "term": "text generation, diffusion model NeurIPS 2024"
    },
    {
      "title": "DiscDiff: Latent Diffusion Model for DNA Sequence Generation",
      "url": "https://arxiv.org/html/2402.06079v1",
      "snippet": "(2023) Lal, A., Biancalani, T., ... language models. In NeurIPS 2023 Generative AI and Biology (GenBio) Workshop, 2023. Li et al. (2022) Li, X., Thickstun, J., Gulrajani, I., Liang, P. S., and Hashimoto, T. B. Diffusion-lm improves controllable text generation....",
      "engine": "brave",
      "authors": [],
      "term": "text generation, diffusion model NeurIPS 2024"
    },
    {
      "title": "DNA-Diffusion: Leveraging Generative Models for Controlling Chromatin Accessibility and Gene Expression via Synthetic Regulatory Elements - PMC",
      "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC10862870/",
      "snippet": "Diffusion models have emerged as a powerful class of generative models that have achieved strong performance in generating synthetic images, audio, and text6â€“9 and more recently also adapted in the biological context to generate protein structures10 (Fig. 1a).",
      "engine": "brave",
      "authors": [],
      "term": "text generation, diffusion model NeurIPS 2024"
    },
    {
      "title": "Accepted Papers - NAACL-HLT 2025",
      "url": "https://2025.naacl.org/program/accepted_papers/",
      "snippet": "Enhancing Language Model Hypernetworks with Restart: A Study on Optimization Yihan Zhang, Jie Fu, Rongrong Ji, Jie Chen Â· Private Synthetic Text Generation with Diffusion Models Sebastian Ochs, Ivan Habernal",
      "engine": "brave",
      "authors": [],
      "term": "text generation, diffusion model NAACL 2025"
    },
    {
      "title": "Discrete Diffusion Language Model for Efficient Text Summarization - ACL Anthology",
      "url": "https://aclanthology.org/2025.findings-naacl.352/",
      "snippet": "@inproceedings{dat-etal-2025-discrete, title = \"Discrete Diffusion Language Model for Efficient Text Summarization\", author = \"Dat, Do Huu and Do, Duc Anh and Luu, Anh Tuan and Buntine, Wray\", editor = \"Chiruzzo, Luis and Ritter, Alan and Wang, Lu\", booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2025\", month = apr, year = \"2025\", address = \"Albuquerque, New Mexico\", publisher = \"Association for Computational Linguistics\", url = \"https://aclanthology.org/2025.findings-naacl.352/\", doi = \"10.18653/v1/2025.findings-naacl.352\", pages = \"6278--6290\", ISBN = \"979-8-89176-195-7\", abstract = \"While diffusion models excel at conditionally generating high-quality images, prior works in discrete diffusion models were not evaluated on conditional long-text generation.",
      "engine": "brave",
      "authors": [],
      "term": "text generation, diffusion model NAACL 2025"
    },
    {
      "title": "Discrete Diffusion Language Model for Efficient Text ...",
      "url": "https://aclanthology.org/2025.findings-naacl.352.pdf",
      "snippet": "ACL materials are Copyright Â© 1963â€“2025 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License. Permission is granted to make copies for ...",
      "engine": "brave",
      "authors": [],
      "term": "text generation, diffusion model NAACL 2025"
    },
    {
      "title": "Text Compression for Efficient Language Generation - ACL Anthology",
      "url": "https://aclanthology.org/2025.naacl-srw.18/",
      "snippet": "Our experiments show that GPTHF achieves an up to an order of magnitude improvement in FLOPs efficiency and a threefold increase in runtime speed compared to equally-sized GPT models in the low-size regime. This is achieved through a unique generation method that caches and reuses sentence embeddings, allowing significant portions of the input to bypass large parts of the network. %R 10.18653/v1/2025.naacl-srw.18 %U https://aclanthology.org/2025.naacl-srw.18/ %U https://doi.org/10.18653/v1/2025.naacl-srw.18 %P 186-192",
      "engine": "brave",
      "authors": [],
      "term": "text generation, diffusion model NAACL 2025"
    },
    {
      "title": "Generate and Instantiate What You Prefer: Text-Guided Diffusion for Sequential Recommendation",
      "url": "https://arxiv.org/html/2410.13428v3",
      "snippet": "To address these limitations, we propose iDreamRec to involve more concrete prior knowledge to establish item embeddings, particularly through detailed item text descriptions and advanced Text Embedding Models (TEM). More importantly, by converting item descriptions into embeddings aligned with TEM, we enable the integration of intention instructions as control signals to guide the generation of oracle items. Experimental results on four datasets demonstrate that iDreamRec not only outperforms existing diffusion-based generative recommenders but also facilitates the incorporation of intention instructions for more precise and effective recommendation generation.",
      "engine": "brave",
      "authors": [],
      "term": "text generation, diffusion model NAACL 2025"
    },
    {
      "title": "Diffusion models for non-autoregressive text generation | Proceedings of the Thirty-Second International Joint Conference on Artificial Intelligence",
      "url": "https://dl.acm.org/doi/10.24963/ijcai.2023/750",
      "snippet": "In this survey, we review the recent progress in diffusion models for NAR text generation. As the background, we first present the general definition of diffusion models and the text diffusion models, and then discuss their merits for NAR generation. As the core content, we further introduce two mainstream diffusion models in existing work of text diffusion, and review the key designs of the diffusion process.",
      "engine": "brave",
      "authors": [],
      "term": "text generation, diffusion model NAACL 2025"
    },
    {
      "title": "Diffusion models in text generation: a survey - PubMed",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38435628/",
      "snippet": "Recently, they have drawn wide interest in natural language generation (NLG), a sub-field of natural language processing (NLP), due to their capability to generate varied and high-quality text outputs. In this article, we conduct a comprehensive survey on the application of diffusion models in text generation.",
      "engine": "brave",
      "authors": [],
      "term": "text generation, diffusion model NAACL 2025"
    }
  ],
  "sources": {
    "https://dl.acm.org/doi/10.5555/3618408.3619275": "SNIPPET: In this paper, we introduce a novel dIffusion language modEl pre-training framework for text generation, which we call GENIE. GENIE is a large-scale pre-trained diffusion language model that consists of an encoder and a diffusion-based decoder, which can generate text by gradually transforming a random noise sequence into a coherent text sequence.\n\n[FetchError] HTTP 403 for https://dl.acm.org/doi/10.5555/3618408.3619275\n\nSOURCE: https://dl.acm.org/doi/10.5555/3618408.3619275",
    "https://arxiv.org/abs/2408.04220": "SNIPPET: In this paper we use a guided diffusion model to produce a latent proposal that steers an auto-regressive language model to generate text with desired properties. Our model inherits the unmatched fluency of the auto-regressive approach and the plug-and-play flexibility of diffusion.\n\nTITLE: Diffusion Guided Language Modeling\n\nBODY:\nComputer Science > Computation and Language\n[Submitted on 8 Aug 2024]\nTitle:Diffusion Guided Language Modeling\nView PDF HTML (experimental)Abstract:Current language models demonstrate remarkable proficiency in text generation. However, for many applications it is desirable to control attributes, such as sentiment, or toxicity, of the generated language -- ideally tailored towards each specific use case and target audience. For auto-regressive language models, existing guidance methods are prone to decoding errors that cascade during generation and degrade performance. In contrast, text diffusion models can easily be guided with, for example, a simple linear sentiment classifier -- however they do suffer from significantly higher perplexity than auto-regressive alternatives. In this paper we use a guided diffusion model to produce a latent proposal that steers an auto-regressive language model to generate text with desired properties. Our model inherits the unmatched fluency of the auto-regressive approach and the plug-and-play flexibility of diffusion. We show that it outperforms previous plug-and-play guidance methods across a wide range of benchmark data sets. Further, controlling a new attribute in our framework is reduced to training a single logistic regression classifier.\nReferences & Citations\nexport BibTeX citation\nLoading...\nBibliographic and Citation Tools\nBibliographic Explorer (What is the Explorer?)\nConnected Papers (What is Connected Papers?)\nLitmaps (What is Litmaps?)\nscite Smart Citations (What are Smart Citations?)\nCode, Data and Media Associated with this Article\nalphaXiv (What is alphaXiv?)\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\nDagsHub (What is DagsHub?)\nGotit.pub (What is GotitPub?)\nHugging Face (What is Huggingface?)\nPapers with Code (What is Papers with Code?)\nScienceCast (What is ScienceCast?)\nDemos\nRecommenders and Search Tools\nInfluence Flower (What are Influence Flowers?)\nCORE Recommender (What is CORE?)\narXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.\n\nSOURCE: https://arxiv.org/abs/2408.04220",
    "https://papers.cool/venue/2024.naacl-long.2@ACL": "SNIPPET: The diffusion model, a new generative modeling paradigm, has achieved great success in image, audio, and video generation.However, considering the discrete categorical nature of the text, it is not trivial to extend continuous diffusion models to natural language. In this work, we propose SeqDiffuSeq, a text diffusion model, to approach sequence-to-sequence text generation with an encoder ...\n\nTITLE: 2024.naacl-long.2@ACL\n\nBODY:\nTotal: 1\nThe diffusion model, a new generative modeling paradigm, has achieved great success in image, audio, and video generation.However, considering the discrete categorical nature of the text, it is not trivial to extend continuous diffusion models to natural language. In this work, we propose SeqDiffuSeq, a text diffusion model, to approach sequence-to-sequence text generation with an encoder-decoder Transformer architecture.To improve the generation performance, SeqDiffuSeq is equipped with the self-conditioning technique and our newly proposed adaptive noise schedule technique. Self-conditioning enables SeqDiffuSeq to better use the predicted sequence information during the generation process.The adaptive noise schedule balances the difficulty of denoising across time steps at the token level.Experiment results illustrate the improved performance on five sequence-to-sequence generation tasks compared to other diffusion-based models regarding text quality and inference time.\n\nSOURCE: https://papers.cool/venue/2024.naacl-long.2@ACL",
    "https://dl.acm.org/doi/10.24963/ijcai.2023/750": "SNIPPET: In this survey, we review the recent progress in diffusion models for NAR text generation. As the background, we first present the general definition of diffusion models and the text diffusion models, and then discuss their merits for NAR generation. As the core content, we further introduce two mainstream diffusion models in existing work of text diffusion, and review the key designs of the diffusion process.\n\n[FetchError] HTTP 403 for https://dl.acm.org/doi/10.24963/ijcai.2023/750\n\nSOURCE: https://dl.acm.org/doi/10.24963/ijcai.2023/750",
    "https://arxiv.org/abs/2505.22165": "SNIPPET: Diffusion models have emerged as a promising approach for text generation, with recent works falling into two main categories: discrete and continuous diffusion models. Discrete diffusion models apply token corruption independently using categorical distributions, allowing for different diffusion progress across tokens but lacking fine-grained control. Continuous diffusion models map tokens to ...\n\nTITLE: Unifying Continuous and Discrete Text Diffusion with Non-simultaneous Diffusion Processes\n\nBODY:\nComputer Science > Computation and Language\n[Submitted on 28 May 2025]\nTitle:Unifying Continuous and Discrete Text Diffusion with Non-simultaneous Diffusion Processes\nView PDF HTML (experimental)Abstract:Diffusion models have emerged as a promising approach for text generation, with recent works falling into two main categories: discrete and continuous diffusion models. Discrete diffusion models apply token corruption independently using categorical distributions, allowing for different diffusion progress across tokens but lacking fine-grained control. Continuous diffusion models map tokens to continuous spaces and apply fine-grained noise, but the diffusion progress is uniform across tokens, limiting their ability to capture semantic nuances. To address these limitations, we propose \\textbf{\\underline{N}}on-simultan\\textbf{\\underline{e}}ous C\\textbf{\\underline{o}}ntinuous \\textbf{\\underline{Diff}}usion Models (NeoDiff), a novel diffusion model that integrates the strengths of both discrete and continuous approaches. NeoDiff introduces a Poisson diffusion process for the forward process, enabling a flexible and fine-grained noising paradigm, and employs a time predictor for the reverse process to adaptively modulate the denoising progress based on token semantics. Furthermore, NeoDiff utilizes an optimized schedule for inference to ensure more precise noise control and improved performance. Our approach unifies the theories of discrete and continuous diffusion models, offering a more principled and effective framework for text generation. Experimental results on several text generation tasks demonstrate NeoDiff's superior performance compared to baselines of non-autoregressive continuous and discrete diffusion models, iterative-based methods and autoregressive diffusion-based methods. These results highlight NeoDiff's potential as a powerful tool for generating high-quality text and advancing the field of diffusion-based text generation.\nCurrent browse context:\ncs.CL\nReferences & Citations\nexport BibTeX citation\nLoading...\nBibliographic and Citation Tools\nBibliographic Explorer (What is the Explorer?)\nConnected Papers (What is Connected Papers?)\nLitmaps (What is Litmaps?)\nscite Smart Citations (What are Smart Citations?)\nCode, Data and Media Associated with this Article\nalphaXiv (What is alphaXiv?)\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\nDagsHub (What is DagsHub?)\nGotit.pub (What is GotitPub?)\nHugging Face (What is Huggingface?)\nPapers with Code (What is Papers with Code?)\nScienceCast (What is ScienceCast?)\nDemos\nRecommenders and Search Tools\nInfluence Flower (What are Influence Flowers?)\nCORE Recommender (What is CORE?)\narXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.\n\nSOURCE: https://arxiv.org/abs/2505.22165",
    "https://arxiv.org/html/2410.13428v3": "SNIPPET: To address these limitations, we propose iDreamRec to involve more concrete prior knowledge to establish item embeddings, particularly through detailed item text descriptions and advanced Text Embedding Models (TEM). More importantly, by converting item descriptions into embeddings aligned with TEM, we enable the integration of intention instructions as control signals to guide the generation of oracle items. Experimental results on four datasets demonstrate that iDreamRec not only outperforms existing diffusion-based generative recommenders but also facilitates the incorporation of intention instructions for more precise and effective recommendation generation.\n\n[FetchError] ConnectionError(ProtocolError('Connection aborted.', ConnectionAbortedError(10053, 'ä½ çš„ä¸»æœºä¸­çš„è½¯ä»¶ä¸­æ­¢äº†ä¸€ä¸ªå·²å»ºç«‹çš„è¿žæŽ¥ã€‚', None, 10053, None)))\n\nSOURCE: https://arxiv.org/html/2410.13428v3",
    "https://neurips.cc/virtual/2023/poster/73068": "SNIPPET: This results in tokens on the left undergoing fewer denoising steps than those on the right, thereby enabling them to generate earlier and subsequently influence the generation of tokens on the right.In a series of experiments on various text generation tasks, including text summarization, machine translation, and common sense generation, AR-Diffusion clearly demonstrated its superiority over existing diffusion language models and that it can be $100\\times\\sim600\\times$ faster when achieving comparable results.\n\nTITLE: Main Navigation\n\nBODY:\nPoster\nAR-Diffusion: Auto-Regressive Diffusion Model for Text Generation\nTong Wu Â· Zhihao Fan Â· Xiao Liu Â· Hai-Tao Zheng Â· Yeyun Gong Â· yelong shen Â· Jian Jiao Â· Juntao Li Â· zhongyu wei Â· Jian Guo Â· Nan Duan Â· Weizhu Chen\nAbstract:\nDiffusion models have gained significant attention in the realm of image generation due to their exceptional performance. Their success has been recently expanded to text generation via generating all tokens within a sequence concurrently. However, natural language exhibits a far more pronounced sequential dependency in comparison to images, and the majority of existing language models are trained with a left-to-right auto-regressive approach.To account for the inherent sequential characteristic of natural language, we introduce Auto-Regressive Diffusion (AR-Diffusion). AR-Diffusion ensures that the generation of tokens on the right depends on the generated ones on the left, a mechanism achieved through employing a dynamic number of denoising steps that vary based on token position. This results in tokens on the left undergoing fewer denoising steps than those on the right, thereby enabling them to generate earlier and subsequently influence the generation of tokens on the right.In a series of experiments on various text generation tasks, including text summarization, machine translation, and common sense generation, AR-Diffusion clearly demonstrated its superiority over existing diffusion language models and that it can be $100\\times\\sim600\\times$ faster when achieving comparable results. Our code is available at https://github.com/microsoft/ProphetNet/tree/master/AR-diffusion.\nChat is not available.\n\nSOURCE: https://neurips.cc/virtual/2023/poster/73068",
    "https://arxiv.org/abs/2410.21357": "SNIPPET: We further show that, without any generation performance drop, our framework offers a 1.3$\\times$ sampling speedup over existing diffusion models. Reproduced code is available at this https URL. From: Minkai Xu [view email] [v1] Mon, 28 Oct 2024 17:25:56 UTC (808 KB) [v2] Thu, 20 Feb 2025 01:23:30 UTC (813 KB) [v3] Fri, 28 Feb 2025 08:41:03 UTC (815 KB) [v4] Fri, 7 Mar 2025 04:28:45 UTC (813 KB)\n\nTITLE: Energy-Based Diffusion Language Models for Text Generation\n\nBODY:\nComputer Science > Computation and Language\n[Submitted on 28 Oct 2024 (v1), last revised 7 Mar 2025 (this version, v4)]\nTitle:Energy-Based Diffusion Language Models for Text Generation\nView PDF HTML (experimental)Abstract:Despite remarkable progress in autoregressive language models, alternative generative paradigms beyond left-to-right generation are still being actively explored. Discrete diffusion models, with the capacity for parallel generation, have recently emerged as a promising alternative. Unfortunately, these models still underperform the autoregressive counterparts, with the performance gap increasing when reducing the number of sampling steps. Our analysis reveals that this degradation is a consequence of an imperfect approximation used by diffusion models. In this work, we propose Energy-based Diffusion Language Model (EDLM), an energy-based model operating at the full sequence level for each diffusion step, introduced to improve the underlying approximation used by diffusion models. More specifically, we introduce an EBM in a residual form, and show that its parameters can be obtained by leveraging a pretrained autoregressive model or by finetuning a bidirectional transformer via noise contrastive estimation. We also propose an efficient generation algorithm via parallel important sampling. Comprehensive experiments on language modeling benchmarks show that our model can consistently outperform state-of-the-art diffusion models by a significant margin, and approaches autoregressive models' perplexity. We further show that, without any generation performance drop, our framework offers a 1.3$\\times$ sampling speedup over existing diffusion models. Reproduced code is available at this https URL.\nSubmission history\nFrom: Minkai Xu [view email][v1] Mon, 28 Oct 2024 17:25:56 UTC (808 KB)\n[v2] Thu, 20 Feb 2025 01:23:30 UTC (813 KB)\n[v3] Fri, 28 Feb 2025 08:41:03 UTC (815 KB)\n[v4] Fri, 7 Mar 2025 04:28:45 UTC (813 KB)\nReferences & Citations\nexport BibTeX citation\nLoading...\nBibliographic and Citation Tools\nBibliographic Explorer (What is the Explorer?)\nConnected Papers (What is Connected Papers?)\nLitmaps (What is Litmaps?)\nscite Smart Citations (What are Smart Citations?)\nCode, Data and Media Associated with this Article\nalphaXiv (What is alphaXiv?)\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\nDagsHub (What is DagsHub?)\nGotit.pub (What is GotitPub?)\nHugging Face (What is Huggingface?)\nPapers with Code (What is Papers with Code?)\nScienceCast (What is ScienceCast?)\nDemos\nRecommenders and Search Tools\nInfluence Flower (What are Influence Flowers?)\nCORE Recommender (What is CORE?)\narXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.\n\nSOURCE: https://arxiv.org/abs/2410.21357",
    "https://icml.cc/virtual/2024/poster/34686": "SNIPPET: For comparable model sizes, SEDD beats existing language diffusion paradigms (reducing perplexity by $25$-$75$%) and is competitive with autoregressive models, in particular outperforming GPT-2. Furthermore, compared to autoregressive mdoels, SEDD generates faithful text without requiring distribution annealing techniques like temperature scaling (around $6$-$8\\times$ better generative perplexity than un-annealed GPT-2), can trade compute and quality (similar quality with $32\\times$ fewer network evaluations), and enables controllable infilling (matching nucleus sampling quality while enabling other strategies besides left to right prompting).\n\nTITLE: Main Navigation\n\nBODY:\nAbstract:\nDespite their groundbreaking performance for many generative modeling tasks, diffusion models have fallen short on discrete data domains such as natural language. Crucially, standard diffusion models rely on the well-established theory of score matching, but efforts to generalize this to discrete structures have not yielded the same empirical gains. In this work, we bridge this gap by proposing score entropy, a novel loss that naturally extends score matching to discrete spaces, integrates seamlessly to build discrete diffusion models, and significantly boosts performance. Experimentally, we test our Score Entropy Discrete Diffusion models (SEDD) on standard language modeling tasks. For comparable model sizes, SEDD beats existing language diffusion paradigms (reducing perplexity by $25$-$75$%) and is competitive with autoregressive models, in particular outperforming GPT-2. Furthermore, compared to autoregressive mdoels, SEDD generates faithful text without requiring distribution annealing techniques like temperature scaling (around $6$-$8\\times$ better generative perplexity than un-annealed GPT-2), can trade compute and quality (similar quality with $32\\times$ fewer network evaluations), and enables controllable infilling (matching nucleus sampling quality while enabling other strategies besides left to right prompting).\nChat is not available.\n\nSOURCE: https://icml.cc/virtual/2024/poster/34686",
    "https://openreview.net/forum?id=3s9IrEsjLyk": "SNIPPET: Published: 31 Oct 2022, Last Modified: 04 Aug 2025 NeurIPS 2022 Accept Readers: Everyone Keywords: controllable text generation, controlled generation, infilling, language model, diffusion model TL;DR: We propose a non-autoregressive language model based on continuous diffusions, which demonstrate strong performance in controllable text generation.\n\nTITLE: Diffusion-LM Improves Controllable Text Generation\n\nBODY:\nKeywords: controllable text generation, controlled generation, infilling, language model, diffusion model\nTL;DR: We propose a non-autoregressive language model based on continuous diffusions, which demonstrate strong performance in controllable text generation.\nAbstract: Controlling the behavior of language models (LMs) without re-training is a major open problem in natural language generation. While recent works have demonstrated successes on controlling simple sentence attributes (e.g., sentiment), there has been little progress on complex, fine-grained controls (e.g., syntactic structure). To address this challenge, we develop a new non-autoregressive language model based on continuous diffusions that we call Diffusion-LM. Building upon the recent successes of diffusion models in continuous domains, Diffusion-LM iteratively denoises a sequence of Gaussian vectors into word vectors, yielding a sequence of intermediate latent variables. The continuous, hierarchical nature of these intermediate variables enables a simple gradient-based algorithm to perform complex, controllable generation tasks. We demonstrate successful control of Diffusion-LM for six challenging fine-grained control tasks, significantly outperforming prior work.\nSupplementary Material: pdf\nCommunity Implementations: [![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/diffusion-lm-improves-controllable-text/code)\n14 Replies\nLoading\n\nSOURCE: https://openreview.net/forum?id=3s9IrEsjLyk",
    "https://proceedings.neurips.cc/paper_files/paper/2024/hash/91d193b65d0b120d29503590827de1ea-Abstract-Conference.html": "SNIPPET: Part of Advances in Neural Information ... success in generating images, audio, video, and text. It has been adapted for sequence-to-sequence text generation (Seq2Seq) through DiffuSeq, termed the S2S-Diffusion model....\n\nTITLE: Meta-Diffu$B$: A Contextualized Sequence-to-Sequence Text Diffusion Model with Meta-Exploration\n\nBODY:\nPart of Advances in Neural Information Processing Systems 37 (NeurIPS 2024) Main Conference Track\nYun-Yen Chuang, Hung-Min Hsu, Kevin Lin, Chen-Sheng Gu, Ling Zhen Li, Ray-I Chang, Hung-yi Lee\nThe diffusion model, a new generative modeling paradigm, has achieved significant success in generating images, audio, video, and text. It has been adapted for sequence-to-sequence text generation (Seq2Seq) through DiffuSeq, termed the S2S-Diffusion model. Existing S2S-Diffusion models predominantly rely on fixed or hand-crafted rules to schedule noise during the diffusion and denoising processes. However, these models are limited by non-contextualized noise, which fails to fully consider the characteristics of Seq2Seq tasks. In this paper, we propose the Meta-Diffu$B$ frameworkâ€”a novel scheduler-exploiter S2S-Diffusion paradigm designed to overcome the limitations of existing S2S-Diffusion models. We employ Meta-Exploration to train an additional scheduler model dedicated to scheduling contextualized noise for each sentence. Our exploiter model, an S2S-Diffusion model, leverages the noise scheduled by our scheduler model for updating and generation. Meta-Diffu$B$ achieves state-of-the-art performance compared to previous S2S-Diffusion models and fine-tuned pre-trained language models (PLMs) across four Seq2Seq benchmark datasets. We further investigate and visualize the impact of Meta-Diffu$B$'s noise scheduling on the generation of sentences with varying difficulties. Additionally, our scheduler model can function as a \"plug-and-play\" model to enhance DiffuSeq without the need for fine-tuning during the inference stage.\n\nSOURCE: https://proceedings.neurips.cc/paper_files/paper/2024/hash/91d193b65d0b120d29503590827de1ea-Abstract-Conference.html",
    "https://neurips.cc/virtual/2025/poster/116380": "SNIPPET: 1 month ago - We derive closed-form expressions for the diffusion Evidence Lower Bound (ELBO), and show that HDLM can be implemented in a flexible manner and that it includes an existing discrete diffusion model as a special case. Extensive text generation experiments demonstrate the effectiveness of HDLM.\n\nTITLE: Main Navigation\n\nBODY:\nPoster\nNext Semantic Scale Prediction via Hierarchical Diffusion Language Models\nCai Zhou Â· Chenyu Wang Â· Dinghuai Zhang Â· Shangyuan Tong Â· Yifei Wang Â· Stephen Bates Â· Tommi Jaakkola\nIn this paper we introduce Hierarchical Diffusion Language Models (HDLM) -- a novel family of discrete diffusion models for language modeling. HDLM builds on a hierarchical vocabulary where low-level tokens with detailed semantics are surjectively mapped to high-level tokens with coarse-grained meanings. In the forward process, each token is independently perturbed to its higher-level ancestor with more abstract semantics according to the scheduler, while in the reverse process the model progressively predicts the next, more detailed semantics. Taken together, HDLM provides a general time-varying next semantic scale prediction process for language modeling. We derive closed-form expressions for the diffusion Evidence Lower Bound (ELBO), and show that HDLM can be implemented in a flexible manner and that it includes an existing discrete diffusion model as a special case. Extensive text generation experiments demonstrate the effectiveness of HDLM.\nLive content is unavailable. Log in and register to view live content\n\nSOURCE: https://neurips.cc/virtual/2025/poster/116380",
    "https://iclr.cc/virtual/2024/poster/18202": "SNIPPET: To address this issue, we introduce AnyText, a diffusion-based multilingual visual text generation and editing model, that focuses on rendering accurate and coherent text in the image.\n\nTITLE: Main Navigation\n\nBODY:\nSpotlight Poster\nAnyText: Multilingual Visual Text Generation and Editing\nYuxiang Tuo Â· Wangmeng Xiang Â· Jun-Yan He Â· Yifeng Geng Â· Xuansong Xie\nDiffusion model based Text-to-Image has achieved impressive achievements recently. Although current technology for synthesizing images is highly advanced and capable of generating images with high fidelity, it is still possible to give the show away when focusing on the text area in the generated image, as synthesized text often contains blurred, unreadable, or incorrect characters, making visual text generation one of the most challenging issues in this field. To address this issue, we introduce AnyText, a diffusion-based multilingual visual text generation and editing model, that focuses on rendering accurate and coherent text in the image. AnyText comprises a diffusion pipeline with two primary elements: an auxiliary latent module and a text embedding module. The former uses inputs like text glyph, position, and masked image to generate latent features for text generation or editing. The latter employs an OCR model for encoding stroke data as embeddings, which blend with image caption embeddings from the tokenizer to generate texts that seamlessly integrate with the background. We employed text-control diffusion loss and text perceptual loss for training to further enhance writing accuracy. AnyText can write characters in multiple languages, to the best of our knowledge, this is the first work to address multilingual visual text generation. It is worth mentioning that AnyText can be plugged into existing diffusion models from the community for rendering or editing text accurately. After conducting extensive evaluation experiments, our method has outperformed all other approaches by a significant margin. Additionally, we contribute the first large-scale multilingual text images dataset, AnyWord-3M, containing 3 million image-text pairs with OCR annotations in multiple languages. Based on AnyWord-3M dataset, we propose AnyText-benchmark for the evaluation of visual text generation accuracy and quality. Our project will be open-sourced soon to improve and promote the development of text generation technology.\n\nSOURCE: https://iclr.cc/virtual/2024/poster/18202",
    "https://iclr.cc/virtual/2024/poster/18667": "SNIPPET: Text-to-Image Diffusion Models such as Stable-Diffusion and Imagen have achieved unprecedented quality of photorealism with state-of-the-art FID scores on MS-COCO and other generation benchmarks. Given a caption, image generation requires fine-grained knowledge about attributes such as object ...\n\nTITLE: Main Navigation\n\nBODY:\nPoster\nLocalizing and Editing Knowledge In Text-to-Image Generative Models\nSamyadeep Basu Â· Nanxuan Zhao Â· Vlad Morariu Â· Soheil Feizi Â· Varun Manjunatha\nText-to-Image Diffusion Models such as Stable-Diffusion and Imagen have achieved unprecedented quality of photorealism with state-of-the-art FID scores on MS-COCO and other generation benchmarks. Given a caption, image generation requires fine-grained knowledge about attributes such as object structure, style, and viewpoint amongst others. Where does this information reside in text-to-image generative models? In our paper, we tackle this question and understand how knowledge corresponding to distinct visual attributes is stored in large-scale text-to-image diffusion models. We adapt Causal Mediation Analysis for text-to-image models and trace knowledge about distinct visual attributes to various (causal) components in the (i) UNet and (ii) text-encoder of the diffusion model. In particular, we show that unlike large-language models, knowledge about different attributes is not localized in isolated components, but is instead distributed amongst a set of components in the conditional UNet. These sets of components are often distinct for different visual attributes (e.g., style} / objects). Remarkably, we find that the text-encoder in public text-to-image models such as Stable-Diffusion contains {\\it only} one causal state across different visual attributes, and this is the first self-attention layer corresponding to the last subject token of the attribute in the caption. This is in stark contrast to the causal states in other language models which are often the mid-MLP layers. Based on this observation of only one causal state in the text-encoder, we introduce a fast, data-free model editing method DiffQuickFix which can effectively edit concepts (remove or update knowledge) in text-to-image models. DiffQuickFix can edit (ablate) concepts in under a second with a closed-form update, providing a significant 1000x speedup and comparable editing performance to existing fine-tuning based editing methods.\n\nSOURCE: https://iclr.cc/virtual/2024/poster/18667",
    "https://proceedings.neurips.cc/paper_files/paper/2024/hash/02c1d1d33dbfbaf03b3971bb542e72e2-Abstract-Conference.html": "SNIPPET: Then, we propose a novel text-driven panoramic generation framework, termed DiffPano, to achieve scalable, consistent, and diverse panoramic scene generation. Specifically, benefiting from the powerful generative capabilities of stable diffusion, we fine-tune a single-view text-to-panorama diffusion model with LoRA on the established panoramic video-text dataset.\n\nTITLE: DiffPano: Scalable and Consistent Text to Panorama Generation with Spherical Epipolar-Aware Diffusion\n\nBODY:\nPart of Advances in Neural Information Processing Systems 37 (NeurIPS 2024) Main Conference Track\nWeicai Ye, Chenhao Ji, Zheng Chen, Junyao Gao, Xiaoshui Huang, Song-Hai Zhang, Wanli Ouyang, Tong He, Cairong Zhao, Guofeng Zhang\nDiffusion-based methods have achieved remarkable achievements in 2D image or 3D object generation, however, the generation of 3D scenes and even $360^{\\circ}$ images remains constrained, due to the limited number of scene datasets, the complexity of 3D scenes themselves, and the difficulty of generating consistent multi-view images. To address these issues, we first establish a large-scale panoramic video-text dataset containing millions of consecutive panoramic keyframes with corresponding panoramic depths, camera poses, and text descriptions. Then, we propose a novel text-driven panoramic generation framework, termed DiffPano, to achieve scalable, consistent, and diverse panoramic scene generation. Specifically, benefiting from the powerful generative capabilities of stable diffusion, we fine-tune a single-view text-to-panorama diffusion model with LoRA on the established panoramic video-text dataset. We further design a spherical epipolar-aware multi-view diffusion model to ensure the multi-view consistency of the generated panoramic images. Extensive experiments demonstrate that DiffPano can generate scalable, consistent, and diverse panoramic images with given unseen text descriptions and camera poses.\n\nSOURCE: https://proceedings.neurips.cc/paper_files/paper/2024/hash/02c1d1d33dbfbaf03b3971bb542e72e2-Abstract-Conference.html",
    "https://neurips.cc/virtual/2024/105743": "SNIPPET: These autoregressive models offer flexibility by predicting discrete audio tokens, but they often fail to achieve high fidelity. In this work, we propose an advanced system that integrates the autoregressive language model with the diffusion model, achieving flexible and refined audio generation.\n\nTITLE: Main Navigation\n\nBODY:\nPoster+Demo Session\nin\nWorkshop: Audio Imagination: NeurIPS 2024 Workshop AI-Driven Speech, Music, and Sound Generation\nText-to-Audio Generation via Bridging Audio Language Model and Latent Diffusion\nZHENYU WANG Â· Chenxing Li Â· YONG XU Â· Chunlei Zhang Â· John H. L. Hansen Â· Dong Yu\nDiffusion models have become the foundation for most text-to-audio generation methods. These approaches rely on a large text encoder to process the textual description, serving as a semantic condition to guide the audio generation process. Meanwhile, autoregressive language model-based methods for audio generation have also emerged. These autoregressive models offer flexibility by predicting discrete audio tokens, but they often fail to achieve high fidelity. In this work, we propose an advanced system that integrates the autoregressive language model with the diffusion model, achieving flexible and refined audio generation. The auto-regressive language model is used to predict the discrete audio tokens conditioned on text prompts. Then, audio tokens are fed into the diffusion model to further purify the details of the generated audio. Consequently, compared to baseline systems, our proposed approach can deliver better results on most objective and subjective metrics on the AudioCaps test set. Audio demos generated by our proposed best system are available at https://dcldmdemo.github.io.\n\nSOURCE: https://neurips.cc/virtual/2024/105743",
    "https://iclr.cc/virtual/2024/poster/17865": "SNIPPET: We propagate this context to all timesteps in the two processes to adapt their trajectories, thereby facilitating cross-modal conditional modeling. We generalize our contextualized diffusion to both DDPMs and DDIMs with theoretical derivations, and demonstrate the effectiveness of our model in evaluations with two challenging tasks: text-to-image generation, and text-to-video editing.\n\nTITLE: Main Navigation\n\nBODY:\nPoster\nCross-Modal Contextualized Diffusion Models for Text-Guided Visual Generation and Editing\nLing Yang Â· Zhilong Zhang Â· Zhaochen Yu Â· Jingwei Liu Â· Minkai Xu Â· Stefano Ermon Â· Bin CUI\nConditional diffusion models have exhibited superior performance in high-fidelity text-guided visual generation and editing. Nevertheless, prevailing text-guided visual diffusion models primarily focus on incorporating text-visual relationships exclusively into the reverse process, often disregarding their relevance in the forward process. This inconsistency between forward and reverse processes maylimit the precise conveyance of textual semantics in visual synthesis results. To address this issue, we propose a novel and general contextualized diffusion model (ContextDiff) by incorporating the cross-modal context encompassing interactions and alignments between text condition and visual sample into forward and reverse processes. We propagate this context to all timesteps in the two processes to adapt their trajectories, thereby facilitating cross-modal conditional modeling. We generalize our contextualized diffusion to both DDPMs and DDIMs with theoretical derivations, and demonstrate the effectiveness of our model in evaluations with two challenging tasks: text-to-image generation, and text-to-video editing. In each task, our ContextDiff achieves new state-of-the-art performance, significantly enhancing the semantic alignment between text condition and generated samples, as evidenced by quantitative and qualitative evaluations. Our code is available at https://github.com/YangLing0818/ContextDiff\n\nSOURCE: https://iclr.cc/virtual/2024/poster/17865",
    "https://arxiv.org/abs/2401.11708": "SNIPPET: Diffusion models have exhibit exceptional performance in text-to-image generation and editing. However, existing methods often face challenges when handling complex text prompts that involve multiple objects with multiple attributes and relationships. In this paper, we propose a brand new training-free text-to-image generation/editing framework, namely Recaption, Plan and Generate (RPG ...\n\nTITLE: Mastering Text-to-Image Diffusion: Recaptioning, Planning, and Generating with Multimodal LLMs\n\nBODY:\nComputer Science > Computer Vision and Pattern Recognition\n[Submitted on 22 Jan 2024 (v1), last revised 5 May 2024 (this version, v3)]\nTitle:Mastering Text-to-Image Diffusion: Recaptioning, Planning, and Generating with Multimodal LLMs\nView PDF HTML (experimental)Abstract:Diffusion models have exhibit exceptional performance in text-to-image generation and editing. However, existing methods often face challenges when handling complex text prompts that involve multiple objects with multiple attributes and relationships. In this paper, we propose a brand new training-free text-to-image generation/editing framework, namely Recaption, Plan and Generate (RPG), harnessing the powerful chain-of-thought reasoning ability of multimodal LLMs to enhance the compositionality of text-to-image diffusion models. Our approach employs the MLLM as a global planner to decompose the process of generating complex images into multiple simpler generation tasks within subregions. We propose complementary regional diffusion to enable region-wise compositional generation. Furthermore, we integrate text-guided image generation and editing within the proposed RPG in a closed-loop fashion, thereby enhancing generalization ability. Extensive experiments demonstrate our RPG outperforms state-of-the-art text-to-image diffusion models, including DALL-E 3 and SDXL, particularly in multi-category object composition and text-image semantic alignment. Notably, our RPG framework exhibits wide compatibility with various MLLM architectures (e.g., MiniGPT-4) and diffusion backbones (e.g., ControlNet). Our code is available at: this https URL\nSubmission history\nFrom: Ling Yang [view email][v1] Mon, 22 Jan 2024 06:16:29 UTC (35,646 KB)\n[v2] Tue, 6 Feb 2024 03:10:25 UTC (37,855 KB)\n[v3] Sun, 5 May 2024 04:50:54 UTC (37,855 KB)\nCurrent browse context:\ncs.CV\nReferences & Citations\nexport BibTeX citation\nLoading...\nBibliographic and Citation Tools\nBibliographic Explorer (What is the Explorer?)\nConnected Papers (What is Connected Papers?)\nLitmaps (What is Litmaps?)\nscite Smart Citations (What are Smart Citations?)\nCode, Data and Media Associated with this Article\nalphaXiv (What is alphaXiv?)\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\nDagsHub (What is DagsHub?)\nGotit.pub (What is GotitPub?)\nHugging Face (What is Huggingface?)\nPapers with Code (What is Papers with Code?)\nScienceCast (What is ScienceCast?)\nDemos\nRecommenders and Search Tools\nInfluence Flower (What are Influence Flowers?)\nCORE Recommender (What is CORE?)\narXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.\n\nSOURCE: https://arxiv.org/abs/2401.11708",
    "https://arxiv.org/html/2410.21357v2": "SNIPPET: In this paper, we introduced Energy-based Diffusion Language Model (EDLM), which integrates energy-based models with discrete diffusion models to address the limitations of parallel text generation.\n\nTITLE: Energy-Based Diffusion Language Models for Text Generation\n\nBODY:\nEnergy-Based Diffusion Language Models\nfor Text Generation\nAbstract\nDespite remarkable progress in autoregressive language models, alternative generative paradigms beyond left-to-right generation are still being actively explored. Discrete diffusion models, with the capacity for parallel generation, have recently emerged as a promising alternative. Unfortunately, these models still underperform the autoregressive counterparts, with the performance gap increasing when reducing the number of sampling steps. Our analysis reveals that this degradation is a consequence of an imperfect approximation used by diffusion models. In this work, we propose Energy-based Diffusion Language Model (EDLM), an energy-based model operating at the full sequence level for each diffusion step, introduced to improve the underlying approximation used by diffusion models. More specifically, we introduce an EBM in a residual form, and show that its parameters can be obtained by leveraging a pretrained autoregressive model or by finetuning a bidirectional transformer via noise contrastive estimation. We also propose an efficient generation algorithm via parallel important sampling. Comprehensive experiments on language modeling benchmarks show that our model can consistently outperform state-of-the-art diffusion models by a significant margin, and approaches autoregressive modelsâ€™ perplexity. We further show that, without any generation performance drop, our framework offers a 1.3 sampling speedup over existing diffusion models.\n1 Introduction\nIn recent years, autoregressive (AR) models have achieved remarkable advances in modern large language modeling (Vaswani et al., 2017; Wolf et al., 2020; Brown et al., 2020; Chowdhery et al., 2023; Touvron et al., 2023), showing impressive results in tasks such as expert-level coding (Roziere et al., 2023) and reasoning (Wei et al., 2022; Trinh et al., 2024). Despite the considerable progress achieved, the left-to-right generative paradigm exhibits several long-standing drawbacks. For example, training and sampling of autoregressive models require a fixed ordering of the sequence, constraining their generation flexibility (Hoogeboom et al., 2022; Shih et al., 2022). Furthermore, at test time, the generation is heavily conditioned on previous generations, which leads to accumulated error a.k.a exposure bias (Bengio et al., 2015). As a result, alternative generative paradigms beyond left-to-right generation are actively being explored.\nDiscrete diffusion models (DMs) have recently emerged as a promising competitor for discrete sequence generation (Austin et al., 2021; Dieleman et al., 2022; Gat et al., 2024). Unlike AR models, discrete DMs conduct generation by progressively decoding the full sequence in parallel starting from a fully masked sequence, offering great potential in bidirectional controllable generation and sampling acceleration. Most recently, great efforts have been invested in improving their performance by extending discrete diffusions to operate in continuous time (Campbell et al., 2022; 2024; Sahoo et al., 2024; Shi et al., 2024). Unfortunately, despite their promise, these models still underperform AR models by a large margin, resulting in limited practical usage. Developing principled and powerful discrete DMs remains an open problem.\nIn this work, we take a closer look at the existing discrete diffusion models and reveal a vital mismatch issue between the training and sampling distribution in its current form. Specifically, the existing discrete diffusion models aim to predict all missing tokens in parallel at each intermediate diffusion step, but the denoising joint distribution is simply parameterized as a product of token-wise independent distributions. As a result, intermediate denoising steps ignore sequence-level correlations, which results in serious accumulated decoding errors and prevents users from efficient parallel decoding with fewer timesteps. To this end, in this paper, we propose Energy-based Diffusion Language Model (EDLM), an unnormalized energy-based model (EBM) that learns to jointly denoise the full sequence at each diffusion step. Our key innovation is to learn an EBM for each denoising distribution , where the energy directly operates on the sequence level and captures the correlation between tokens. The fundamental challenge of our framework is to develop efficient training and sampling methods for the unnormalized model. To overcome these challenges, we design the EBM in a novel residual form\n| (1) |\napplied over pretrained diffusion models . Such formulation enjoys several distinct advantages. First, we analytically show that the EBM parameters can be easily obtained by leveraging pretrained autoregressive models or finetuning from bidirectional transformers via noise contrastive estimation, bypassing expensive maximum likelihood training. Furthermore, the framework corrects the decoding error and enables fast generation by conducting efficient important sampling in parallel over samples from the diffusion proposal distribution. Importantly, we highlight that, when leveraging pretraiend AR as the energy function, our approach can be interpreted as parallel sampling from pretrained language models using diffusion models as the proposal distribution.\nEDLM can be viewed as a new family of discrete generative models that marries energy-based and diffusion-based models. The novel combination addresses the training and sampling distribution mismatch problem, enjoys better generation quality with less accumulated error, and improves sampling efficiency by reducing the number of sampling steps. We further provide a formal estimator to calculate the perplexity of EDLM, allowing our model to be compared against other models in a standard way. We conduct comprehensive experiments on two common language modeling benchmarks to evaluate the performance of our proposed method. Results show that on the perplexity metric, EDLM can consistently achieve state-of-the-art performance among diffusion-based counterparts, and approaches or matches AR models. On the generation quality metric, compared over the most competitive diffusion baseline, EDLM shows up to generative perplexity gain with the same number of sampling timesteps, and can achieve up to 1.3 sampling speedup when keeping the same sampling performance.\n2 Related Work\nDiffusion models are powerful models with surprising results in generating high-quality images (Sohl-Dickstein et al., 2015; Song & Ermon, 2019; Ho et al., 2020; Dhariwal & Nichol, 2021). These models were originally designed for continuous data, with both forward and backward reverse processes parameterized as Gaussian Markov chains. In recent years, several methods have been developed to extend the diffusion framework to generate discrete data (Austin et al., 2021; Hoogeboom et al., 2022), with Campbell et al. (2022); Zhao et al. (2024) further extending the framework to model discrete data in continuous time. Most recently, several concurrent works (Lou et al., 2024; Sahoo et al., 2024; Shi et al., 2024) archived impressive progress in large-scale language modeling by scaling the model and simplifying the training and sampling processes. Additionally, Campbell et al. (2024); Gat et al. (2024) developed flow matching methods for discrete data, which rely on a formulation similar to that of diffusion models. Despite the considerable progress in the area, we observe that all these approaches are subject to the training and sampling distribution mismatch issue, where the learned joint denoising distribution is simplified as independent distributions for each token. This problem results in a significant accumulated decoding error during the parallel sampling process and prevents users from conducting fast sampling with a small number of denoising timesteps. Deng et al. (2020) studies similar accumulated error problem (a.k.a exposure bias) in the autoregressive model setting and proposes energy-based models for modeling global context, but the methodology focuses on improving autoregressive models and therefore fundamentally different from ours. Other work (Gu et al., 2018; Ghazvininejad et al., 2019; Gu & Kong, 2021; Savinov et al., 2022; Zheng et al., 2023) study similar non-autoregressive text generation but combined with reranking and/or remasking methods. However, these methods typically result in biased sampling, which can generate high-quality samples but fail to model the distribution and suffer from low sampling diversity. Therefore, these methods mainly focus on different tasks such as machine translation. Notably, Lezama et al. (2023) notices a similar issue (focusing on diffusion models in the image generation domain) and proposes to learn a corrector network to correct the independent decoding error. However, this work concentrates on the image generation domain, and the correction process needs to be run sequentially, which takes additional inference time. In contrast, we propose an energy-based denoising process that directly captures the correlation at each denoising step and enables efficient parallel decoding via an importance sampling scheme.\n3 Discrete Diffusion Models\nGeneral Discrete Diffusion. Assume our data lives in a finite discrete space of size . In this paper, we augment the categorical space with an additional mask state with index . In a general discrete diffusion framework (Austin et al., 2021), the diffusion process is defined as a Markov chain , which repeatedly multiplies with matrices over discrete time steps. Given these transitions, the marginal distributions at each timestep can be written in closed-form as . Such forward process can also be viewed as an interpolation between a clean data sample and a reference distribution induced by :\n| (2) |\nwhere is a strictly decreasing function w.r.t , with and .\nIn the continuous time limit, for two arbitrary times , the transition distributions can be written as , where (Zhao et al., 2024; Shi et al., 2024). This implies that during each diffusion step the token will jump to a sample from the prior distribution with a probability of . The forward process allows us to compute many distributions in closed form. One particular distribution of interest is the reversal of the forward process given , that is, the posterior distribution given by\n| (3) |\nMasked Diffusion. In this paper, we focus on masked (i.e., absorbing state) diffusion models, where the target distribution is set as . At each diffusion step each token transitions to the â€˜maskedâ€™ state with some probability. Under such masking framework, the forward marginals (eq. 2) are given by , and the posterior (eq. 3) can be simplified as\n| (4) |\nDiffusion models aim to learn a backward model to approximate the reversal of the forward process. Leveraging Equation 4 we can parameterize the model as\n| (5) |\nwhere is known as predictor, since it predicts the mean of the distribution over given . However, as explained next, the practical implementation of this predictor results in a mismatch between training and sampling distributions.\nProblem Statement. Existing discrete DMs learn a denoising distribution to match the true reversal . Let denote the tokens of a full sequence in this section. In practice, the model is parameterized as a factorized denoising model:\n| (6) |\nand the predictor factorizes each token in independently. This factorization enables us to conduct an efficient denoising step by first sampling all tokens from in parallel and then masking certain tokens according to the forward . However, this parameterization ignores dependencies between tokens in the sequence, a fundamental limitation which implies that can never match the exact backward . As a result, this parallel sampling introduces accumulated errors, as the factorized denoising step does not match the original generative model for the joint distribution of the elements of .\n4 Method\nIn this section, we formally introduce Energy-based Diffusion Language Model (EDLM). Our work aims to design a new family of energy-based discrete generative models that address the fundamental mismatch problem between and in existing models. We first describe the general EBM formulation in Section 4.1, and then elaborate on how to obtain the energy function by either leveraging pretrained AR models or finetuning via noise contrastive estimation in Section 4.2. Then we discuss how to estimate the likelihood (perplexity) of EDLM in Section 4.3, and finally introduce the efficient parallel sampling algorithm in Section 4.4\n4.1 Residual Energy-based Models\nDiscrete DMs can be viewed as learning a conditional predictor for each denoising step . For example, original discrete DMs learn to directly predict independent distributions for each token in . In our framework, given diffused data at timestep , we introduce the generative denoising kernel as an unnormalized density:\n| (7) |\nwhere is the pretrained diffusion model, is the energy introduced to capture the correlation in the sequence, and is a normalizing factor known as the partition function. In the following text, we use to denote the joint model, the (residual) energy function, and keep the pretrained fixed. Computing the partition function is intractable since it requires summing over the whole space, which is exponential in the sequence length. In our language modeling experiments, the vocabulary size is around k and the generation length is tokens, resulting in a space of size around . We aim to design solutions to efficiently train the parameters of the energy function so that the joint model distribution gets close to the true reversal , while avoiding computing the partition function.\n4.2 Implementation of Energy Function\nTraining energy-based models with the intractable partition function is a long-standing challenge in machine learning (Hinton, 2002; Carreira-Perpinan & Hinton, 2005; LeCun et al., 2006). Typical maximum likelihood estimation training requires approximation of the participation function using Markov chain Monte Carlo (MCMC) sampling, which is computationally infeasible for high-dimensional data. We aim to find efficient ways to train the energy functionâ€™s parameters for the predictor . In the following, we describe two methods to do this. One involves taking pretrained autoregressive language models as energy functions without any training, and efficiently running sampling by taking all tokens as inputs in parallel. The second solution involves fine-tuning the pretrained diffusion model via noise contrastive estimation, where the model is parameterized with bidirectional transformers and potentially captures richer corre\n...[truncated]",
    "https://proceedings.neurips.cc/paper_files/paper/2024/file/860c1c657deafe09f64c013c2888bd7b-Paper-Conference.pdf": "SNIPPET: When applying the self-play fine-tuning technique (Chen et al., 2024) to diffusion models, there Â· are two challenges: (a) an exponential or even infinite number of possible trajectories can lead to Â· the same image. The generator in a diffusion model operates through a sequence of intermediate\n\nTITLE: (from PDF)\n\nBODY:\nSelf-Play Fine-Tuning of Diffusion Models for\nText-to-Image Generation\n\nHuizhuo Yuanâˆ— Zixiang Chenâˆ— Kaixuan Jiâˆ— Quanquan Gu\nDepartment of Computer Science\nUniversity of California, Los Angeles\nLos Angeles, CA 90095\n{hzyuan,chenzx19,kaixuanji,qgu}@cs.ucla.edu\n\nAbstract\n\nFine-tuning Diffusion Models remains an underexplored frontier in generative\nartificial intelligence (GenAI), especially when compared with the remarkable\nprogress made in fine-tuning Large Language Models (LLMs). While cutting-edge\ndiffusion models such as Stable Diffusion (SD) and SDXL rely on supervised\nfine-tuning, their performance inevitably plateaus after seeing a certain volume\nof data. Recently, reinforcement learning (RL) has been employed to fine-tune\ndiffusion models with human preference data, but it requires at least two images\n(â€œwinnerâ€ and â€œloserâ€ images) for each text prompt. In this paper, we introduce\nan innovative technique called self-play fine-tuning for diffusion models (SPIN-\nDiffusion), where the diffusion model engages in competition with its earlier\nversions, facilitating an iterative self-improvement process. Our approach offers an\nalternative to conventional supervised fine-tuning and RL strategies, significantly\nimproving both model performance and alignment. Our experiments on the Pick-\na-Pic dataset reveal that SPIN-Diffusion outperforms the existing supervised fine-\ntuning method in aspects of human preference alignment and visual appeal right\nfrom its first iteration. By the second iteration, it exceeds the performance of\nRLHF-based methods across all metrics, achieving these results with less data.\nCodes are available at https://github.com/uclaml/SPIN-Diffusion/.\n\n1\n\nIntroduction\n\nDiffusion models (Ho et al., 2020; Peebles and Xie, 2023; Podell et al., 2023; Nichol et al., 2021;\nRombach et al., 2022a; Song et al., 2020a) have rapidly emerged as critical entities within the realm\nof generative AIs (Creswell et al., 2018; Kingma and Welling, 2013), demonstrating exceptional\ncapabilities in generating high-fidelity outputs. Their versatility spans a diverse area of applications,\nranging from image generation (Rombach et al., 2022a; Podell et al., 2023; Ramesh et al., 2022)\nto more complex tasks like structure-based drug design (Corso et al., 2022; Guan et al., 2023),\nprotein structure prediction (Watson et al., 2021), text generation (Austin et al., 2021; Zheng et al.,\n2023; Chen et al., 2023), and more. Prominent diffusion models in image generation, including\nDALL-E (Ramesh et al., 2022), Stable Diffusion (Rombach et al., 2022b), SDXL (Podell et al., 2023),\nand Dreamlike, etc., typically undergo a fine-tuning process following their initial pre-training phase.\nStandard fine-tuning method for diffusion models suffers from low alignment with human preferences\nand low data efficiency due to two main reasons: (1) it does not directly optimize for alignment\nwith human preferences, and (2) only one round of training can be performed. Recently, using\nReinforcement Learning (RL) for fine-tuning diffusion models has received increasing attention. Lee\net al. (2023) first studied the alignment of text-image diffusion models to human preferences using\nreward-weighted likelihood maximization with a reward function trained on human preference data.\n\nâˆ—Equal contribution\n\n38th Conference on Neural Information Processing Systems (NeurIPS 2024).\n\n\fBlack et al. (2023) formulated the fine-tuning of diffusion models as a RL problem solved by policy\ngradient optimization. In a concurrent work, Fan et al. (2023) studied a similar formulation but with\na KL regularization. Very recently, Wallace et al. (2023) have bypassed the need for training reward\nfunctions by using Direct Preference Optimization (DPO) (Rafailov et al., 2023) for fine-tuning\ndiffusion models. Similar approach was proposed in Yang et al. (2023) as well.\nWhile RL fine-tuning of diffusion methods has been proven effective, its dependency on human\npreference data, often necessitating multiple images per prompt, poses a significant challenge. In\nmany datasets including the community-sourced ones featuring custom content, it is often the case to\nhave only one image associated with each prompt. This makes RL fine-tuning infeasible.\nIn this paper, drawing inspiration from the recently proposed self-play fine-tuning (SPIN) technique\n(Chen et al., 2024) for large language models (LLM), we introduce a new supervised fine-tuning (SFT)\nmethod for diffusion models, eliminating the necessity for human preference data in the fine-tuning\nprocess. Central to our method is a general-sum minimax game, where both the participating players,\nnamely the main player and the opponent player, are diffusion models. The main playerâ€™s goal is to\ndiscern between samples drawn from the target data distribution and those generated by the opponent\nplayer. The opponent playerâ€™s goal is to garner the highest score possible, as assessed by the main\nplayer. A self-play mechanism can be made possible, if and only if the main player and the opponent\nplayer have the same structure, and therefore the opponent player can be designed to be previous\ncopies of the main player (Chen et al., 2024). The proposed algorithm SPIN-Diffusion overcomes\nthe drawbacks of both supervised fine-tuning (SFT) and RL fine-tuning. Compared with SFT, our\nmethod is more data-efficient, by repeatedly using the prompts from the SFT dataset to improve the\nmodel through self-play. Compared with RL fine-tuning methods, our method does not need external\nreward models or expensive human-annotated winner/loser pairs.\nWhen applying the self-play fine-tuning technique (Chen et al., 2024) to diffusion models, there\nare two challenges: (a) an exponential or even infinite number of possible trajectories can lead to\nthe same image. The generator in a diffusion model operates through a sequence of intermediate\nsteps, but the performance of the generator is only determined by the quality of the image in the\nlast step; and (b) diffusion models are parameterized by a sequence of score functions, which are\nthe gradient of the probabilities rather than probabilities in LLMs. Our algorithm design effectively\nsurmounts these challenges by (a) designing an objective function that considers all intermediate\nimages generated during the reverse sampling process; and (b) decomposing and approximating\nthe probability function step-by-step into products related to the score function. We also employ\nthe Gaussian reparameterization technique in DDIM (Song et al., 2020a) to support the advanced\nsampling method. All these techniques together lead to an unbiased objective function that can\nbe effectively calculated based on intermediate samples. For computational efficiency, we further\npropose an approximate objective function, which eliminates the need for intermediate images used\nin our model.\nContributions. Our contributions are summarized below:\n\nâ€¢ We propose a novel fine-tuning method for diffusion models based on the self-play mechanism,\ncalled SPIN-Diffusion. The proposed algorithm iteratively improves upon a diffusion model\nuntil converging to the target distribution. Theoretically, we prove that the model obtained by\nSPIN-Diffusion cannot be further improved via standard SFT. Moreover, the stationary point of our\nself-play mechanism is achieved when the diffusion model aligns with the target distribution.\n\nâ€¢ Empirically, we evaluate the performance of SPIN-Diffusion on text-to-image generation\ntasks (Ramesh et al., 2022; Rombach et al., 2022a; Saharia et al., 2022a). Our experiments\non the Pick-a-Pic dataset (Kirstain et al., 2023), with base model being Stable Diffusion-1.5 (Rom-\nbach et al., 2022b), demonstrate that SPIN-Diffusion surpasses SFT from the very first iteration.\nNotably, by the second iteration, SPIN-Diffusion outperforms Diffusion-DPO (Wallace et al., 2023)\nthat utilizes additional data from â€˜loserâ€™ samples. By the third iteration, the images produced by\nSPIN-Diffusion achieve a higher PickScore (Kirstain et al., 2023) than the base model SD-1.5\n79.8% of the times, and a superior Aesthetic score 88.4% of the times.\n\nSPIN-Diffusion exhibits a remarkable performance improvement over current state-of-the-art fine-\ntuning algorithms, retaining this advantage even against models trained with more extensive data\nusage. This highlights its exceptional efficiency in dataset utilization. It is beneficial for the general\npublic, particularly those with restricted access to datasets containing multiple images per prompt.\n\n2\n\n\fNotation. We use lowercase letters and lowercase boldface letters to denote scalars and vectors,\nrespectively. We use 0 : T to denote the index set {0, . . . , T }. In the function space, let F be\nthe function class. We use the symbol q to denote the real distribution in a diffusion process,\nwhile pÎ¸ represents the distribution parameterized by a nueral network during sampling. The\nGaussian distribution is represented as N (Âµ, Î£), where Âµ and Î£ are the mean and covariance matrix,\nrespectively. Lastly, Uniform{1, . . . , T } denotes the uniform distribution over the set {1, . . . , T }.\n\n2 Related Work\n\nDiffusion Models. Diffusion-based generative models (Sohl-Dickstein et al., 2015) have recently\ngained prominence, attributed to their ability to produce high-quality and diverse samples. A popular\ndiffusion model is denoising diffusion probabilistic modeling (DDPM) (Ho et al., 2020). Song et al.\n(2020a) proposed a denoising diffusion implicit model (DDIM), which extended DDPM to a non-\nMarkov diffusion process, enabling a deterministic sampling process and the accelerated generation\nof high-quality samples. In addition to DDPM and DDIM, diffusion models have also been studied\nwith a score-matching probabilistic model using Langevin dynamics (Song and Ermon, 2019; Song\net al., 2020b). Diffusion models evolved to encompass guided diffusion models, which are designed\nto generate conditional distributions. When the conditioning input is text and the output is image,\nthese models transform into text-to-image diffusion models (Rombach et al., 2022a; Ramesh et al.,\n2022; Ho et al., 2022; Saharia et al., 2022b). They bridge the gap between textual descriptions and\nimage synthesis, offering exciting possibilities for content generation. A significant advancement in\ntext-to-image generation is the introduction of Stable Diffusion (SD) (Rombach et al., 2022a). SD\nhas expanded the potential of diffusion models by integrating latent variables into the generation\nprocess. This innovation in latent diffusion models enables the exploration of latent spaces and\nimproves the diversity of generated content. Despite the introduction of latent spaces, generating\nimages with desired content from text prompts remains a significant challenge (Gal et al., 2022; Ruiz\net al., 2023). This is due to the difficulty in learning the semantic properties of text prompts with\nlimited high-quality data.\nFine-Tuning Diffusion Models. Efforts to improve diffusion models have focused on aligning\nthem more closely with human preferences. Rombach et al. (2022a) fine-tuned a pre-trained model\nusing the COCO dataset (Caesar et al., 2018), demonstrating superior performance compared to a\ngenerative model directly trained on the same dataset. Podell et al. (2023) expanded the model size\nof Stable Diffusion (SD) to create the SDXL model, which was fine-tuned on a high-quality but\nprivate dataset, leading to a significant improvement in the aesthetics of the generated images. Dai\net al. (2023) further demonstrated the effectiveness of fine-tuning and highlighted the importance\nof the supervised fine-tuning (SFT) dataset. In addition to using datasets with high-quality images,\nBetker et al. (2023); Segalis et al. (2023) found that SFT on a data set with high text fidelity can\nalso improve the performance of the diffusion model. The aforementioned methods only requires\na high-quality SFT dataset. Recently, preference datasets have been studied in finetuing diffusion\nmodels (Lee et al., 2023). Concurrently, DDPO (Black et al., 2023) and DPOK (Fan et al., 2023)\nproposed to use the preference dataset to train a reward model and then fine-tune diffusion models\nusing reinforcement learning. Drawing inspiration from the recent Direct Preference Optimization\n(DPO) (Rafailov et al., 2023), Diffusion-DPO (Wallace et al., 2023) and D3PO (Yang et al., 2023)\nused the implicit reward to fine-tune diffusion models directly on the preference dataset. Furthermore,\nwhen a differentiable reward model is available, Clark et al. (2023); Prabhudesai et al. (2023) applied\nreward backpropagation for fine-tuning diffusion models. Our SPIN-Diffusion is most related to the\nSFT method, as it only assumes access to high-quality image-text pairs. However, the high-quality\nimage-text dataset can be obtained from various sources, including selecting the winner from a\npreference dataset or identifying high-reward image-text pairs through a reward model.\n\n3 Problem Setting and Preliminaries\n\nIn this section, we introduce basic settings for text-to-image generation by diffusion models and the\nself-play fine-tuning (SPIN) method.\n\n3.1 Text-to-Image Diffusion Model\n\nDenoising diffusion implicit models (DDIM) (Song et al., 2020a) is a generalized framework of\ndenoising diffusion probabilistic models (DDPM) (Sohl-Dickstein et al., 2015; Ho et al., 2020).\nDDIM enables the fast generation of high-quality samples and has been widely used in text-to-image\n\n3\n\n\fdiffusion models such as Stable Diffusion (Rombach et al., 2022a). We formulate our method building\nupon DDIM, which makes it more general.\nForwrd Process. Following Saharia et al. (2022b), the problem of text-to-image generation can be\nformulated as conditional diffusion models. We use x0 âˆˆ Rd to denote the value of image pixels\nwhere d is the dimension and use c to denote the text prompt. Given a prompt c, image x0 is drawn\nfrom a target data distribution pdata(Â·|c). The diffusion process is characterized by the following\ndynamic parameterized by a positive decreasing sequence {Î±t}T\n\nt=1 with Î±0 = 1,\n\nq(x1:T |x0) := q(xT |x0)\n\nT\n(cid:89)\n\nt=2\n\nq(xtâˆ’1|xt, x0),\n\n(3.1)\n\nwhere q(xtâˆ’1|xt, x0) represents a Gaussian distribution N (Âµt, Ïƒ2\nt I). Here, Âµt is the mean of\nGaussian defined as\nâˆš\n\nâˆš\n\n(cid:113)\n\nÂµt :=\n\nÎ±tâˆ’1x0 +\n\n1 âˆ’ Î±tâˆ’1 âˆ’ Ïƒ2\nt Â·\n\nxt âˆ’\nâˆš\n\nÎ±tx0\n\n.\n\n1 âˆ’ Î±t\n\nâˆš\n\nIt can be derived from (3.1) that q(xt|x0) = N (\nÎ±tx0, (1 âˆ’ Î±t)I) for all t (Song et al., 2020a). As\na generalized diffusion process of DDPM, (3.1) reduces to DDPM (Ho et al., 2020) with a special\nchoice of Ïƒt = (cid:112)(1 âˆ’ Î±tâˆ’1)/(1 âˆ’ Î±t)(cid:112)(1 âˆ’ Î±t/Î±tâˆ’1).\nGenerative Process\n...[truncated]"
  },
  "all_scored_papers": {
    "https://openreview.net/forum?id=3s9IrEsjLyk": {
      "url": "https://openreview.net/forum?id=3s9IrEsjLyk",
      "title": "Diffusion-LM Improves Controllable Text Generation | OpenReview",
      "abstract": "Published: 31 Oct 2022, Last Modified: 04 Aug 2025 NeurIPS 2022 Accept Readers: Everyone Keywords: controllable text generation, controlled generation, infilling, language model, diffusion model TL;DR: We propose a non-autoregressive language model based on continuous diffusions, which demonstrate strong performance in controllable text generation.",
      "score": 8,
      "relevant_tier": 0,
      "completeness_score": 6.0,
      "associated_candidates": []
    },
    "https://arxiv.org/abs/2401.11708": {
      "url": "https://arxiv.org/abs/2401.11708",
      "title": "Mastering Text-to-Image Diffusion: Recaptioning, Planning, and ...",
      "abstract": "Diffusion models have exhibit exceptional performance in text-to-image generation and editing. However, existing methods often face challenges when handling complex text prompts that involve multiple objects with multiple attributes and relationships. In this paper, we propose a brand new training-free text-to-image generation/editing framework, namely Recaption, Plan and Generate (RPG ...",
      "score": 7,
      "relevant_tier": 0,
      "completeness_score": 6.0,
      "associated_candidates": []
    },
    "https://arxiv.org/abs/2505.22165": {
      "url": "https://arxiv.org/abs/2505.22165",
      "title": "[2505.22165] Unifying Continuous and Discrete Text Diffusion with Non ...",
      "abstract": "Diffusion models have emerged as a promising approach for text generation, with recent works falling into two main categories: discrete and continuous diffusion models. Discrete diffusion models apply token corruption independently using categorical distributions, allowing for different diffusion progress across tokens but lacking fine-grained control. Continuous diffusion models map tokens to ...",
      "score": 7,
      "relevant_tier": 0,
      "completeness_score": 6.0,
      "associated_candidates": []
    },
    "https://arxiv.org/html/2410.21357v2": {
      "url": "https://arxiv.org/html/2410.21357v2",
      "title": "Energy-Based Diffusion Language Models for Text Generation",
      "abstract": "In this paper, we introduced Energy-based Diffusion Language Model (EDLM), which integrates energy-based models with discrete diffusion models to address the limitations of parallel text generation.",
      "score": 8,
      "relevant_tier": 0,
      "completeness_score": 5.0,
      "associated_candidates": []
    },
    "https://dl.acm.org/doi/10.5555/3618408.3619275": {
      "url": "https://dl.acm.org/doi/10.5555/3618408.3619275",
      "title": "Text generation with diffusion language models | Proceedings of the ...",
      "abstract": "In this paper, we introduce a novel dIffusion language modEl pre-training framework for text generation, which we call GENIE. GENIE is a large-scale pre-trained diffusion language model that consists of an encoder and a diffusion-based decoder, which can generate text by gradually transforming a random noise sequence into a coherent text sequence.",
      "score": 8,
      "relevant_tier": 0,
      "completeness_score": 5.0,
      "associated_candidates": []
    },
    "https://neurips.cc/virtual/2025/poster/116380": {
      "url": "https://neurips.cc/virtual/2025/poster/116380",
      "title": "NeurIPS Poster Next Semantic Scale Prediction via Hierarchical Diffusion Language Models",
      "abstract": "1 month ago - We derive closed-form expressions for the diffusion Evidence Lower Bound (ELBO), and show that HDLM can be implemented in a flexible manner and that it includes an existing discrete diffusion model as a special case. Extensive text generation experiments demonstrate the effectiveness of HDLM.",
      "score": 8,
      "relevant_tier": 0,
      "completeness_score": 5.0,
      "associated_candidates": []
    },
    "https://neurips.cc/virtual/2023/poster/73068": {
      "url": "https://neurips.cc/virtual/2023/poster/73068",
      "title": "NeurIPS Poster AR-Diffusion: Auto-Regressive Diffusion Model for Text Generation",
      "abstract": "This results in tokens on the left undergoing fewer denoising steps than those on the right, thereby enabling them to generate earlier and subsequently influence the generation of tokens on the right.In a series of experiments on various text generation tasks, including text summarization, machine translation, and common sense generation, AR-Diffusion clearly demonstrated its superiority over existing diffusion language models and that it can be $100\\times\\sim600\\times$ faster when achieving comparable results.",
      "score": 8,
      "relevant_tier": 0,
      "completeness_score": 5.0,
      "associated_candidates": []
    },
    "https://dl.acm.org/doi/10.24963/ijcai.2023/750": {
      "url": "https://dl.acm.org/doi/10.24963/ijcai.2023/750",
      "title": "Diffusion models for non-autoregressive text generation | Proceedings of the Thirty-Second International Joint Conference on Artificial Intelligence",
      "abstract": "In this survey, we review the recent progress in diffusion models for NAR text generation. As the background, we first present the general definition of diffusion models and the text diffusion models, and then discuss their merits for NAR generation. As the core content, we further introduce two mainstream diffusion models in existing work of text diffusion, and review the key designs of the diffusion process.",
      "score": 8,
      "relevant_tier": 0,
      "completeness_score": 5.0,
      "associated_candidates": []
    },
    "https://arxiv.org/abs/2408.04220": {
      "url": "https://arxiv.org/abs/2408.04220",
      "title": "[2408.04220] Diffusion Guided Language Modeling - arXiv.org",
      "abstract": "In this paper we use a guided diffusion model to produce a latent proposal that steers an auto-regressive language model to generate text with desired properties. Our model inherits the unmatched fluency of the auto-regressive approach and the plug-and-play flexibility of diffusion.",
      "score": 7,
      "relevant_tier": 0,
      "completeness_score": 5.0,
      "associated_candidates": []
    },
    "https://icml.cc/virtual/2024/poster/34686": {
      "url": "https://icml.cc/virtual/2024/poster/34686",
      "title": "ICML Poster Discrete Diffusion Modeling by Estimating the Ratios of the Data Distribution",
      "abstract": "For comparable model sizes, SEDD beats existing language diffusion paradigms (reducing perplexity by $25$-$75$%) and is competitive with autoregressive models, in particular outperforming GPT-2. Furthermore, compared to autoregressive mdoels, SEDD generates faithful text without requiring distribution annealing techniques like temperature scaling (around $6$-$8\\times$ better generative perplexity than un-annealed GPT-2), can trade compute and quality (similar quality with $32\\times$ fewer network evaluations), and enables controllable infilling (matching nucleus sampling quality while enabling other strategies besides left to right prompting).",
      "score": 7,
      "relevant_tier": 0,
      "completeness_score": 5.0,
      "associated_candidates": []
    },
    "https://papers.cool/venue/2024.naacl-long.2@ACL": {
      "url": "https://papers.cool/venue/2024.naacl-long.2@ACL",
      "title": "Text Diffusion Model with Encoder-Decoder Transformers for Sequence-to ...",
      "abstract": "The diffusion model, a new generative modeling paradigm, has achieved great success in image, audio, and video generation.However, considering the discrete categorical nature of the text, it is not trivial to extend continuous diffusion models to natural language. In this work, we propose SeqDiffuSeq, a text diffusion model, to approach sequence-to-sequence text generation with an encoder ...",
      "score": 7,
      "relevant_tier": 0,
      "completeness_score": 5.0,
      "associated_candidates": []
    },
    "https://iclr.cc/virtual/2024/poster/17865": {
      "url": "https://iclr.cc/virtual/2024/poster/17865",
      "title": "ICLR Poster Cross-Modal Contextualized Diffusion Models for Text-Guided Visual Generation and Editing",
      "abstract": "We propagate this context to all timesteps in the two processes to adapt their trajectories, thereby facilitating cross-modal conditional modeling. We generalize our contextualized diffusion to both DDPMs and DDIMs with theoretical derivations, and demonstrate the effectiveness of our model in evaluations with two challenging tasks: text-to-image generation, and text-to-video editing.",
      "score": 7,
      "relevant_tier": 0,
      "completeness_score": 5.0,
      "associated_candidates": []
    },
    "https://proceedings.neurips.cc/paper_files/paper/2024/hash/91d193b65d0b120d29503590827de1ea-Abstract-Conference.html": {
      "url": "https://proceedings.neurips.cc/paper_files/paper/2024/hash/91d193b65d0b120d29503590827de1ea-Abstract-Conference.html",
      "title": "Meta-Diffu$B$: A Contextualized Sequence-to-Sequence Text Diffusion Model with Meta-Exploration",
      "abstract": "Part of Advances in Neural Information ... success in generating images, audio, video, and text. It has been adapted for sequence-to-sequence text generation (Seq2Seq) through DiffuSeq, termed the S2S-Diffusion model....",
      "score": 7,
      "relevant_tier": 0,
      "completeness_score": 5.0,
      "associated_candidates": []
    },
    "https://neurips.cc/virtual/2024/105743": {
      "url": "https://neurips.cc/virtual/2024/105743",
      "title": "NeurIPS Text-to-Audio Generation via Bridging Audio Language Model and Latent Diffusion",
      "abstract": "These autoregressive models offer flexibility by predicting discrete audio tokens, but they often fail to achieve high fidelity. In this work, we propose an advanced system that integrates the autoregressive language model with the diffusion model, achieving flexible and refined audio generation.",
      "score": 8,
      "relevant_tier": 0,
      "completeness_score": 4.0,
      "associated_candidates": []
    },
    "https://iclr.cc/virtual/2024/poster/18202": {
      "url": "https://iclr.cc/virtual/2024/poster/18202",
      "title": "ICLR Poster AnyText: Multilingual Visual Text Generation and Editing",
      "abstract": "To address this issue, we introduce AnyText, a diffusion-based multilingual visual text generation and editing model, that focuses on rendering accurate and coherent text in the image.",
      "score": 7,
      "relevant_tier": 0,
      "completeness_score": 4.0,
      "associated_candidates": []
    },
    "https://iclr.cc/virtual/2024/poster/18667": {
      "url": "https://iclr.cc/virtual/2024/poster/18667",
      "title": "ICLR Poster Localizing and Editing Knowledge In Text-to-Image Generative Models",
      "abstract": "Text-to-Image Diffusion Models such as Stable-Diffusion and Imagen have achieved unprecedented quality of photorealism with state-of-the-art FID scores on MS-COCO and other generation benchmarks. Given a caption, image generation requires fine-grained knowledge about attributes such as object ...",
      "score": 7,
      "relevant_tier": 0,
      "completeness_score": 4.0,
      "associated_candidates": []
    },
    "https://arxiv.org/abs/2410.21357": {
      "url": "https://arxiv.org/abs/2410.21357",
      "title": "[2410.21357] Energy-Based Diffusion Language Models for Text Generation",
      "abstract": "We further show that, without any generation performance drop, our framework offers a 1.3$\\times$ sampling speedup over existing diffusion models. Reproduced code is available at this https URL. From: Minkai Xu [view email] [v1] Mon, 28 Oct 2024 17:25:56 UTC (808 KB) [v2] Thu, 20 Feb 2025 01:23:30 UTC (813 KB) [v3] Fri, 28 Feb 2025 08:41:03 UTC (815 KB) [v4] Fri, 7 Mar 2025 04:28:45 UTC (813 KB)",
      "score": 6,
      "relevant_tier": 0,
      "completeness_score": 6.0,
      "associated_candidates": []
    },
    "https://arxiv.org/html/2410.13428v3": {
      "url": "https://arxiv.org/html/2410.13428v3",
      "title": "Generate and Instantiate What You Prefer: Text-Guided Diffusion for Sequential Recommendation",
      "abstract": "To address these limitations, we propose iDreamRec to involve more concrete prior knowledge to establish item embeddings, particularly through detailed item text descriptions and advanced Text Embedding Models (TEM). More importantly, by converting item descriptions into embeddings aligned with TEM, we enable the integration of intention instructions as control signals to guide the generation of oracle items. Experimental results on four datasets demonstrate that iDreamRec not only outperforms existing diffusion-based generative recommenders but also facilitates the incorporation of intention instructions for more precise and effective recommendation generation.",
      "score": 6,
      "relevant_tier": 0,
      "completeness_score": 6.0,
      "associated_candidates": []
    },
    "https://proceedings.neurips.cc/paper_files/paper/2024/hash/02c1d1d33dbfbaf03b3971bb542e72e2-Abstract-Conference.html": {
      "url": "https://proceedings.neurips.cc/paper_files/paper/2024/hash/02c1d1d33dbfbaf03b3971bb542e72e2-Abstract-Conference.html",
      "title": "DiffPano: Scalable and Consistent Text to Panorama Generation with Spherical Epipolar-Aware Diffusion",
      "abstract": "Then, we propose a novel text-driven panoramic generation framework, termed DiffPano, to achieve scalable, consistent, and diverse panoramic scene generation. Specifically, benefiting from the powerful generative capabilities of stable diffusion, we fine-tune a single-view text-to-panorama diffusion model with LoRA on the established panoramic video-text dataset.",
      "score": 5,
      "relevant_tier": 0,
      "completeness_score": 6.0,
      "associated_candidates": []
    },
    "https://proceedings.neurips.cc/paper_files/paper/2024/file/860c1c657deafe09f64c013c2888bd7b-Paper-Conference.pdf": {
      "url": "https://proceedings.neurips.cc/paper_files/paper/2024/file/860c1c657deafe09f64c013c2888bd7b-Paper-Conference.pdf",
      "title": "Self-Play Fine-Tuning of Diffusion Models for Text-to-Image Generation",
      "abstract": "When applying the self-play fine-tuning technique (Chen et al., 2024) to diffusion models, there Â· are two challenges: (a) an exponential or even infinite number of possible trajectories can lead to Â· the same image. The generator in a diffusion model operates through a sequence of intermediate",
      "score": 5,
      "relevant_tier": 0,
      "completeness_score": 5.0,
      "associated_candidates": []
    }
  },
  "search_candidate_set": [
    [
      "Xiang Lisa Li",
      "32551341",
      "Diffusion-LM Improves Controllable Text Generation",
      "https://www.semanticscholar.org/paper/1386b8a11929cf02da291c56aca353e33bbc22ed"
    ],
    [
      "Minkai Xu",
      "2257420991",
      "Energy-Based Diffusion Language Models for Text Generation",
      "https://arxiv.org/html/2410.21357v2"
    ],
    [
      "Xiang Lisa Li",
      "32551341",
      "Diffusion-LM Improves Controllable Text Generation",
      "https://openreview.net/forum?id=3s9IrEsjLyk"
    ],
    [
      "Ling Yang",
      "2280286886",
      "Mastering Text-to-Image Diffusion: Recaptioning, Planning, and Generating with Multimodal LLMs",
      "https://arxiv.org/abs/2401.11708"
    ],
    [
      "Bocheng Li",
      "2301596441",
      "Unifying Continuous and Discrete Text Diffusion with Non-simultaneous Diffusion Processes",
      "https://arxiv.org/abs/2505.22165"
    ],
    [
      "Ling Yang",
      "2280286886",
      "Mastering Text-to-Image Diffusion: Recaptioning, Planning, and Generating with Multimodal LLMs",
      "https://www.semanticscholar.org/paper/140cfda71bfff852c3e205b7ad61854b78c76982"
    ],
    [
      "Yuxiang Tuo",
      "2265493108",
      "AnyText: Multilingual Visual Text Generation and Editing",
      "https://www.semanticscholar.org/paper/b7b7a549629bbe7fa325572339938980e77d155c"
    ],
    [
      "Justin Lovelace",
      "1398104959",
      "Diffusion Guided Language Modeling",
      "https://www.semanticscholar.org/paper/bd6dec82ad241d0079a76ef618e7670e2e1cc7aa"
    ],
    [
      "Yun-Yen Chuang",
      "28948169",
      "Meta-Diffu$B$: A Contextualized Sequence-to-Sequence Text Diffusion Model with Meta-Exploration",
      "https://proceedings.neurips.cc/paper_files/paper/2024/hash/91d193b65d0b120d29503590827de1ea-Abstract-Conference.html"
    ],
    [
      "Huizhuo Yuan",
      "2269043785",
      "Self-Play Fine-Tuning of Diffusion Models for Text-to-Image Generation",
      "https://proceedings.neurips.cc/paper_files/paper/2024/file/860c1c657deafe09f64c013c2888bd7b-Paper-Conference.pdf"
    ],
    [
      "Huizhuo Yuan",
      "2269043785",
      "Self-Play Fine-Tuning of Diffusion Models for Text-to-Image Generation",
      "https://www.semanticscholar.org/paper/613a32f18388958cc60dbb906d87fc7f206c0e66"
    ],
    [
      "Cai Zhou",
      "2363525958",
      "Next Semantic Scale Prediction via Hierarchical Diffusion Language Models",
      "https://neurips.cc/virtual/2025/poster/116380"
    ],
    [
      "Justin Lovelace",
      "1398104959",
      "Diffusion Guided Language Modeling",
      "https://arxiv.org/abs/2408.04220"
    ],
    [
      "Samyadeep Basu",
      "2114710333",
      "Localizing and Editing Knowledge In Text-to-Image Generative Models",
      "https://www.semanticscholar.org/paper/61872f11e5b09c165e1231624ce433a0652741ea"
    ],
    [
      "Yun-Yen Chuang",
      "28948169",
      "Meta-Diffu$B$: A Contextualized Sequence-to-Sequence Text Diffusion Model with Meta-Exploration",
      "https://www.semanticscholar.org/paper/e41d21e30a2066de22d0e721487aae7c64b405c4"
    ],
    [
      "Bocheng Li",
      "2301596441",
      "Unifying Continuous and Discrete Text Diffusion with Non-simultaneous Diffusion Processes",
      "https://www.semanticscholar.org/paper/75a784c633d75a440fa7c4dbd20739107bcf30df"
    ],
    [
      "Minkai Xu",
      "2257420991",
      "Energy-Based Diffusion Language Models for Text Generation",
      "https://www.semanticscholar.org/paper/c3f18504c770549771b29c222db41b343dd1f36d"
    ],
    [
      "Samyadeep Basu",
      "2114710333",
      "Localizing and Editing Knowledge In Text-to-Image Generative Models",
      "https://iclr.cc/virtual/2024/poster/18667"
    ],
    [
      "Tong Wu",
      "2175455570",
      "AR-Diffusion: Auto-Regressive Diffusion Model for Text Generation",
      "https://www.semanticscholar.org/paper/54b6e5dcef733c151adef0ac06430f63cb301a36"
    ],
    [
      "Tong Wu",
      "2175455570",
      "AR-Diffusion: Auto-Regressive Diffusion Model for Text Generation",
      "https://neurips.cc/virtual/2023/poster/73068"
    ],
    [
      "Weicai Ye",
      "2282542307",
      "DiffPano: Scalable and Consistent Text to Panorama Generation with Spherical Epipolar-Aware Diffusion",
      "https://www.semanticscholar.org/paper/8dcd93275c8ba51e6452586b712ddbbf175b0cc9"
    ],
    [
      "Cai Zhou",
      "2363525958",
      "Next Semantic Scale Prediction via Hierarchical Diffusion Language Models",
      "https://www.semanticscholar.org/paper/c687879fc027cae89911cb84543b8093570c2f49"
    ],
    [
      "Weicai Ye",
      "2282542307",
      "DiffPano: Scalable and Consistent Text to Panorama Generation with Spherical Epipolar-Aware Diffusion",
      "https://proceedings.neurips.cc/paper_files/paper/2024/hash/02c1d1d33dbfbaf03b3971bb542e72e2-Abstract-Conference.html"
    ]
  ],
  "selected_urls_set": [
    "https://dl.acm.org/doi/10.5555/3618408.3619275",
    "https://arxiv.org/abs/2401.11708",
    "https://papers.cool/venue/2024.naacl-long.2@ACL",
    "https://dl.acm.org/doi/10.24963/ijcai.2023/750",
    "https://arxiv.org/abs/2410.21357",
    "https://aclanthology.org/2024.naacl-long.261",
    "https://acl2024-text-generation-tutorial.github.io",
    "https://arxiv.org/html/2410.13428v3",
    "https://aclanthology.org/2024.naacl-long.2",
    "https://neurips.cc/virtual/2024/105743",
    "https://neurips.cc/virtual/2025/poster/116380",
    "https://icml.cc/virtual/2024/poster/34686",
    "https://proceedings.neurips.cc/paper_files/paper/2024/file/860c1c657deafe09f64c013c2888bd7b-Paper-Conference.pdf",
    "https://iclr.cc/virtual/2024/poster/17865",
    "https://research.nvidia.com/labs/lpr/publication/xu2024eddm",
    "https://arxiv.org/html/2410.21357v2",
    "https://pubmed.ncbi.nlm.nih.gov/38435628",
    "https://aclanthology.org/2025.findings-naacl.352",
    "https://arxiv.org/abs/2408.04220",
    "https://aclanthology.org/2024.findings-acl.887",
    "https://pmc.ncbi.nlm.nih.gov/articles/PMC10909201",
    "https://proceedings.neurips.cc/paper_files/paper/2024/hash/91d193b65d0b120d29503590827de1ea-Abstract-Conference.html",
    "https://neurips.cc/virtual/2023/poster/73068",
    "https://www.microsoft.com/en-us/research/articles/msra-neurips-2024-papers",
    "https://iclr.cc/virtual/2024/poster/18667",
    "https://arxiv.org/abs/2505.22165",
    "https://aclanthology.org/2024.findings-acl.54",
    "https://iclr.cc/virtual/2024/poster/18202",
    "https://proceedings.neurips.cc/paper_files/paper/2024/hash/02c1d1d33dbfbaf03b3971bb542e72e2-Abstract-Conference.html",
    "https://openreview.net/forum?id=3s9IrEsjLyk"
  ],
  "selected_serp_url_set": [
    "https://dl.acm.org/doi/10.5555/3618408.3619275",
    "https://arxiv.org/abs/2401.11708",
    "https://papers.cool/venue/2024.naacl-long.2@ACL",
    "https://dl.acm.org/doi/10.24963/ijcai.2023/750",
    "https://arxiv.org/abs/2410.21357",
    "https://arxiv.org/html/2410.13428v3",
    "https://neurips.cc/virtual/2024/105743",
    "https://neurips.cc/virtual/2025/poster/116380",
    "https://icml.cc/virtual/2024/poster/34686",
    "https://proceedings.neurips.cc/paper_files/paper/2024/file/860c1c657deafe09f64c013c2888bd7b-Paper-Conference.pdf",
    "https://iclr.cc/virtual/2024/poster/17865",
    "https://arxiv.org/html/2410.21357v2",
    "https://arxiv.org/abs/2408.04220",
    "https://proceedings.neurips.cc/paper_files/paper/2024/hash/91d193b65d0b120d29503590827de1ea-Abstract-Conference.html",
    "https://neurips.cc/virtual/2023/poster/73068",
    "https://iclr.cc/virtual/2024/poster/18667",
    "https://arxiv.org/abs/2505.22165",
    "https://iclr.cc/virtual/2024/poster/18202",
    "https://proceedings.neurips.cc/paper_files/paper/2024/hash/02c1d1d33dbfbaf03b3971bb542e72e2-Abstract-Conference.html",
    "https://openreview.net/forum?id=3s9IrEsjLyk"
  ],
  "created_at": 1760988456.7508068,
  "updated_at": 1760988456.7508068
}