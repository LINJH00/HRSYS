{
  "task_id": "task_20251020_194126_43ef793e",
  "spec": {
    "top_n": 6,
    "years": [
      2025,
      2024,
      2026
    ],
    "venues": [
      "ICLR",
      "ICML",
      "NeurIPS",
      "ACL",
      "EMNLP",
      "NAACL"
    ],
    "keywords": [
      "text-generation",
      "diffusion model"
    ],
    "research_field": "Natural Language Processing",
    "must_be_current_student": true,
    "degree_levels": [
      "PhD"
    ],
    "author_priority": [
      "first",
      "last"
    ],
    "extra_constraints": []
  },
  "pos": 16,
  "terms": [
    "text generation, diffusion model ICLR 2026",
    "text generation, diffusion model ICML 2026",
    "text generation, diffusion model NeurIPS 2026",
    "text generation, diffusion model ACL 2026",
    "text generation, diffusion model EMNLP 2026",
    "text generation, diffusion model NAACL 2026",
    "text generation, diffusion model ICLR 2025",
    "text generation, diffusion model ICML 2025",
    "text generation, diffusion model NeurIPS 2025",
    "text generation, diffusion model ACL 2025",
    "text generation, diffusion model EMNLP 2025",
    "text generation, diffusion model NAACL 2025",
    "text generation, diffusion model ICLR 2024",
    "text generation, diffusion model ICML 2024",
    "text generation, diffusion model NeurIPS 2024",
    "text generation, diffusion model ACL 2024",
    "text generation, diffusion model EMNLP 2024",
    "text generation, diffusion model NAACL 2024"
  ],
  "rounds_completed": 2,
  "candidates_accum": {
    "Tong Wu": {
      "Name": "Tong Wu",
      "Email": "****@gmail.com",
      "Current Role & Affiliation": "PhD student at Rutgers University, New Brunswick",
      "Current Status": "",
      "Research Keywords": [],
      "Research Focus": [],
      "Profiles": {
        "OpenReview": "https://openreview.net/profile?id=~Tong_Wu6"
      },
      "Publication Overview": [
        "AR-Diffusion: Auto-Regressive Diffusion Model for Text Generation",
        "Text Generation with Diffusion Language Models: A Pre-training Approach with Continuous Paragraph Denoise",
        "Human-Robot Commensality: Bite Timing Prediction for Robot-Assisted Feeding in Groups"
      ],
      "Top-tier Hits (Last 24 Months)": [
        "Neural Information Processing Systems 2023",
        "International Conference on Machine Learning 2022",
        "Conference on Robot Learning 2022"
      ],
      "Honors/Grants": [],
      "Academic Service / Invited Talks": [],
      "Open-source / Datasets / Projects": [
        ""
      ],
      "Representative Papers": [
        {
          "Title": "AR-Diffusion: Auto-Regressive Diffusion Model for Text Generation",
          "Venue": "Neural Information Processing Systems",
          "Year": 2023,
          "Type": "Conference Paper",
          "Links": "https://www.semanticscholar.org/paper/54b6e5dcef733c151adef0ac06430f63cb301a36"
        },
        {
          "Title": "Text Generation with Diffusion Language Models: A Pre-training Approach with Continuous Paragraph Denoise",
          "Venue": "International Conference on Machine Learning",
          "Year": 2022,
          "Type": "Conference Paper",
          "Links": "https://www.semanticscholar.org/paper/cb648d482dbd1e6ad0b0f4da43aca71c06538d4f"
        },
        {
          "Title": "Human-Robot Commensality: Bite Timing Prediction for Robot-Assisted Feeding in Groups",
          "Venue": "Conference on Robot Learning",
          "Year": 2022,
          "Type": "Conference Paper",
          "Links": "https://www.semanticscholar.org/paper/aaca127cfd0cc4ad7f27b4cdf95e39aac36716e9"
        }
      ],
      "Trigger Paper Title": "AR-DIFFUSION: Auto-Regressive Diffusion Model for Text Generation",
      "Trigger Paper URL": "https://www.semanticscholar.org/paper/54b6e5dcef733c151adef0ac06430f63cb301a36",
      "Highlights": [],
      "Radar": {
        "Academic Background": 4,
        "Research Output": 5,
        "Research Alignment": 4,
        "Technical Skills": 5,
        "Recognition & Impact": 4,
        "Communication & Collaboration": 3,
        "Initiative & Independence": 4
      },
      "Total Score": 28,
      "Detailed Scores": {
        "Academic Background": "4/5 - Tong Wu is a PhD student at Rutgers University, a reputable institution, indicating a strong academic foundation. His work spans multiple areas of machine learning and robotics, suggesting a well-rounded education.",
        "Research Output": "5/5 - Tong Wu has published high-quality research in top-tier venues such as NeurIPS and ICML, with significant citations, demonstrating consistent and impactful research output.",
        "Research Alignment": "4/5 - His research focuses on diffusion models, text generation, and human-robot interaction, which aligns well with current trends in AI and machine learning, showing clear focus and relevance.",
        "Technical Skills": "5/5 - The technical depth of his publications, particularly in diffusion models and language modeling, reflects strong computational and algorithmic skills.",
        "Recognition & Impact": "4/5 - His work has received notable citations, particularly in the case of AR-Diffusion, indicating recognition within the research community and a measurable impact on the field.",
        "Communication & Collaboration": "3/5 - While there is no explicit information about collaboration or communication skills, the lack of details on social impact or team-based projects limits the assessment in this area.",
        "Initiative & Independence": "4/5 - Tong Wu has authored multiple papers in different areas, including both language models and human-robot interaction, suggesting initiative and the ability to independently pursue diverse research directions."
      }
    },
    "Hongyi Yuan": {
      "Name": "Hongyi Yuan",
      "Email": "****@mails.tsinghua.edu.cn",
      "Current Role & Affiliation": "PhD student at Department of Biomedical Informatics, Harvard Medical School, Harvard University",
      "Current Status": "",
      "Research Keywords": [
        "medical informatics",
        "natural language processing",
        "reinforcement learning",
        "energy planning"
      ],
      "Research Focus": [
        "medical informatics",
        "natural language processing",
        "reinforcement learning",
        "energy planning"
      ],
      "Profiles": {
        "OpenReview": "https://openreview.net/profile?id=~Hongyi_Yuan1"
      },
      "Publication Overview": [
        "Qwen Technical Report",
        "RRHF: Rank Responses to Align Language Models with Human Feedback without tears",
        "Scaling Relationship on Learning Mathematical Reasoning with Large Language Models"
      ],
      "Top-tier Hits (Last 24 Months)": [
        "arXiv.org 2023",
        "Neural Information Processing Systems 2023",
        "arXiv.org 2023"
      ],
      "Honors/Grants": [],
      "Academic Service / Invited Talks": [],
      "Open-source / Datasets / Projects": [
        ""
      ],
      "Representative Papers": [
        {
          "Title": "Qwen Technical Report",
          "Venue": "arXiv.org",
          "Year": 2023,
          "Type": "Conference Paper",
          "Links": "https://www.semanticscholar.org/paper/5fc1a3a49e8f1d106118b69d1d6be3b6caa23da0"
        },
        {
          "Title": "RRHF: Rank Responses to Align Language Models with Human Feedback without tears",
          "Venue": "Neural Information Processing Systems",
          "Year": 2023,
          "Type": "Conference Paper",
          "Links": "https://www.semanticscholar.org/paper/748698bd4387afd08594e0dc8150c2afa210d9ae"
        },
        {
          "Title": "Scaling Relationship on Learning Mathematical Reasoning with Large Language Models",
          "Venue": "arXiv.org",
          "Year": 2023,
          "Type": "Conference Paper",
          "Links": "https://www.semanticscholar.org/paper/91206346edbe28abb606d7b3425cd455d4019d4f"
        }
      ],
      "Trigger Paper Title": "Text Diffusion Model with Encoder-Decoder Transformers for Sequence-to-Sequence Generation",
      "Trigger Paper URL": "https://www.semanticscholar.org/paper/47e40c4c8d291f6d326ccef7313aba7d98dbf283",
      "Highlights": [],
      "Radar": {
        "Academic Background": 5,
        "Research Output": 5,
        "Research Alignment": 5,
        "Technical Skills": 5,
        "Recognition & Impact": 5,
        "Communication & Collaboration": 4,
        "Initiative & Independence": 5
      },
      "Total Score": 34,
      "Detailed Scores": {
        "Academic Background": "5/5 - Hongyi Yuan is a PhD student at Harvard Medical School, a prestigious institution known for its strong biomedical informatics program, indicating a solid academic foundation.",
        "Research Output": "5/5 - The candidate has published multiple high-impact papers in top venues such as arXiv and NeurIPS, with significant citations, demonstrating prolific and impactful research output.",
        "Research Alignment": "5/5 - Hongyi Yuan's research interests in medical informatics, NLP, and reinforcement learning align well with cutting-edge trends in AI and healthcare, showing clear focus and relevance.",
        "Technical Skills": "5/5 - The candidate's work spans advanced areas like language model alignment, mathematical reasoning, and energy planning, indicating strong technical proficiency across multiple domains.",
        "Recognition & Impact": "5/5 - Several of Hongyi Yuan's publications have received over 200 citations, reflecting recognition and influence within the research community.",
        "Communication & Collaboration": "4/5 - While specific details about collaboration are not provided, the candidate's presence on platforms like OpenReview suggests engagement with the academic community.",
        "Initiative & Independence": "5/5 - The candidate has authored multiple independently driven research projects, including contributions to large language models and human feedback alignment, showing strong initiative and independence."
      }
    },
    "Zecheng Tang": {
      "Name": "Zecheng Tang",
      "Email": "zecheng.tang@foxmail.com",
      "Current Role & Affiliation": "Phd Student @ OpenNLG Group 🎓 SCST, Soochow University",
      "Current Status": "",
      "Research Keywords": [
        "Qwen-Image",
        "Qwen-Image-Edit",
        "LCM-Lab",
        "LongRM",
        "LOOM-Scope",
        "LOGO",
        "L-CiteEval",
        "OpenBA",
        "StrokeNUWA",
        "LayoutNUWA"
      ],
      "Research Focus": [
        "Long Context Modeling",
        "Generative-AI",
        "Model Architectures",
        "Alignment",
        "Efficient Inference"
      ],
      "Profiles": {
        "Homepage": "https://zetangforward.github.io/",
        "Google Scholar": "https://scholar.google.com/citations?user=HUDkBMUAAAAJ&hl=zh-CN",
        "X (Twitter)": "https://x.com/clown_ck2000",
        "GitHub": "https://github.com/ZetangForward"
      },
      "Publication Overview": [
        "Visual ChatGPT: Talking, Drawing and Editing with Visual Foundation Models",
        "Open-ended Long Text Generation via Masked Language Modeling",
        "Rethinking Negative Instances for Generative Named Entity Recognition"
      ],
      "Top-tier Hits (Last 24 Months)": [
        "arXiv.org 2023",
        "Annual Meeting of the Association for Computational Linguistics 2023",
        "Annual Meeting of the Association for Computational Linguistics 2024"
      ],
      "Honors/Grants": [
        "National Scholarship , Soochow University, Oct 2025.",
        "Star of Tomorrow , MSRA, Feb 2024.",
        "Outstanding Graduate and Honorary Graduate , Soochow University, Jun 2022. Rank 1/380, School of Computer Science and Technology",
        "Huawei Scholarship , Huawei, Jun 2022. Top 5% Undergraduate"
      ],
      "Academic Service / Invited Talks": [
        "Reviewer @ ACL (2023-2025)",
        "Reviewer @ EMNLP (2023-2025)",
        "Reviewer @ ARR (2023-2025)",
        "Reviewer @ ICML (2024,2025)",
        "Reviewer @ NeurIPS (2024,2025)",
        "Reviewer @ ICLR (2024,2025)",
        "Reviewer @ CVPR (2025)",
        "Reviewer @ AAAI (2023,2024)",
        "Leveraging Large Language Models for Tool Invocation — OPPO Seminar (2023)",
        "Long Context Modeling in LLMs: Advances and Challenges — NLPCC2024 Tutorial (2024)"
      ],
      "Open-source / Datasets / Projects": [
        "Qwen-Image (project) - Project: Qwen-Image, Qwen-Image-Edit",
        "LCM-Lab (project) - I also lead a research team LCM-Lab, focus on: contextual faithfulness (evaluation & enhancement), efficient long-context and inference scaling, long-context system infrastructure (data & training & evaluation), and long-context reinforcement learning.",
        "L-CiteEval (dataset) - L-CiteEval: Do Long-Context Models Truly Leverage Context for Responding?",
        "LOGO (project) - LOGO -- Long cOntext aliGnment via efficient preference Optimization",
        "OpenBA (dataset) - OpenBA: An Open-sourced 15B Bilingual Asymmetric seq2seq Model Pre-trained from Scratch",
        "LOOM-Scope (project) - LOOM-Scope: a comprehensive and efficient LOng-cOntext Model evaluation framework"
      ],
      "Representative Papers": [
        {
          "Title": "L-CiteEval: Do Long-Context Models Truly Leverage Context for Responding?",
          "Venue": "ACL",
          "Year": 2025,
          "Type": "Conference Paper",
          "Links": ""
        },
        {
          "Title": "LOGO -- Long cOntext aliGnment via efficient preference Optimization",
          "Venue": "ICML",
          "Year": 2025,
          "Type": "Conference Paper",
          "Links": ""
        },
        {
          "Title": "Revisiting Long-Context Modeling from a Context Denoising Perspective",
          "Venue": "arXiv",
          "Year": 2025,
          "Type": "Preprint",
          "Links": ""
        }
      ],
      "Trigger Paper Title": "Can Diffusion Model Achieve Better Performance in Text Generation? Bridging the Gap between Training and Inference",
      "Trigger Paper URL": "https://aclanthology.org/2023.findings-acl.721.pdf",
      "Highlights": [
        "PhD student at Soochow University, OpenNLG Group",
        "Research intern at Tongyi-Qwen, Alibaba Cloud",
        "Lead LCM-Lab focusing on long-context modeling",
        "Published in ACL 2025, ICML 2025, EMNLP 2024",
        "Developed L-CiteEval, LOGO, OpenBA, CMD",
        "Proposed Context Denoising Training for long-context modeling",
        "Released LongRM: Revealing context boundary of reward models",
        "Qwen-Image technical report released",
        "LOOM-Scope: Long-context model evaluation framework",
        "National Scholarship, Soochow University, 2025",
        "Star of Tomorrow, MSRA, 2024",
        "Outstanding Graduate, Soochow University, 2022",
        "Invited talk at NLPCC2024 on long-context modeling",
        "Reviewer for ACL, EMNLP, ICML, NeurIPS, CVPR"
      ],
      "Radar": {
        "Academic Background": 5,
        "Research Output": 5,
        "Research Alignment": 5,
        "Technical Skills": 5,
        "Recognition & Impact": 5,
        "Communication & Collaboration": 4,
        "Initiative & Independence": 5
      },
      "Total Score": 34,
      "Detailed Scores": {
        "Academic Background": "5/5 - Zecheng Tang is a Ph.D. student at Soochow University, affiliated with the OpenNLG Group, and has received multiple prestigious scholarships including the National Scholarship and Huawei Scholarship. His academic achievements reflect strong foundational training and high potential for advanced research.",
        "Research Output": "5/5 - Zecheng Tang has published in top-tier venues such as ACL and arXiv, with notable work on long-context modeling and generation. His publications have received significant citations, demonstrating impactful and high-quality research contributions.",
        "Research Alignment": "5/5 - His research interests align closely with cutting-edge topics in long-context modeling, contextual faithfulness, and foundation models, which are highly relevant to current trends in natural language generation and AI research.",
        "Technical Skills": "5/5 - Zecheng Tang's work spans multiple technical areas including long-context system infrastructure, reinforcement learning, and efficient model scaling, indicating strong technical proficiency and versatility in NLP and AI.",
        "Recognition & Impact": "5/5 - He has received multiple recognitions such as Star of Tomorrow from MSRA and being ranked 1st among 380 students at Soochow University, reflecting both academic excellence and recognition from leading institutions.",
        "Communication & Collaboration": "4/5 - While there is no explicit information about his communication or collaboration experience, his active presence on platforms like GitHub and academic profiles suggests some level of engagement and collaboration in the research community.",
        "Initiative & Independence": "5/5 - Zecheng Tang has independently contributed to impactful research projects such as Visual ChatGPT and Open-ended Long Text Generation, showing strong initiative and the ability to drive research forward with original ideas."
      }
    },
    "Yong-Hyun Park": {
      "Name": "Yong-Hyun Park",
      "Email": "jojunghyo@snu.ac.kr",
      "Current Role & Affiliation": "Incoming Ph.D@UPenn-CIS. park19@seas.upenn.edu",
      "Current Status": "Incoming Ph.D. at UPenn-CIS",
      "Research Keywords": [
        "generative models",
        "diffusion models",
        "safe text-to-image generation",
        "unlearning",
        "Bayesian modeling",
        "Explainable AI"
      ],
      "Research Focus": [
        "diffusion models",
        "generative models",
        "safe text-to-image",
        "unlearning",
        "goal-conditioned behavior"
      ],
      "Profiles": {
        "Homepage": "https://enkeejunior1.github.io/",
        "Google Scholar": "https://scholar.google.com/citations?user=H8N2tHgAAAAJ",
        "GitHub": "https://github.com/alshedivat/al-folio"
      },
      "Publication Overview": [
        "Understanding the Latent Space of Diffusion Models through the Lens of Riemannian Geometry",
        "Direct Unlearning Optimization for Robust and Safe Text-to-Image Models",
        "Unsupervised Discovery of Semantic Latent Directions in Diffusion Models"
      ],
      "Top-tier Hits (Last 24 Months)": [
        "Neural Information Processing Systems 2023",
        "Neural Information Processing Systems 2024",
        "arXiv.org 2023"
      ],
      "Honors/Grants": [
        "NeurIPS 2023 Scholar Award"
      ],
      "Academic Service / Invited Talks": [],
      "Open-source / Datasets / Projects": [
        "Jump Your Steps: Optimizing Sampling Schedule of Discrete Diffusion Models (project) - Optimizing sampling schedule of discrete diffusion models for improved generative model performance.",
        "Direct Unlearning Optimization for Robust and Safe Text-to-Image Models (project) - Enhancing the safety of text-to-image models through unlearning algorithms.",
        "Upsample Guidance: Scale Up Diffusion Models without Training (project) - A method to scale up diffusion models without retraining.",
        "Geometric Remove-and-Retrain (GOAR): Coordinate-Invariant eXplainable AI Assessment (project) - A coordinate-invariant explainable AI assessment method for model evaluation.",
        "Understanding the Latent Space of Diffusion Models through the Lens of Riemannian Geometry (project) - Analyzing the latent space of diffusion models using Riemannian geometry.",
        "Diffusion Model Safety and Unlearning (project) - Research on enhancing safety of generative models using unlearning algorithms and robust training."
      ],
      "Representative Papers": [
        {
          "Title": "Jump Your Steps: Optimizing Sampling Schedule of Discrete Diffusion Models",
          "Venue": "ICLR",
          "Year": 2025,
          "Type": "Conference Paper",
          "Links": ""
        },
        {
          "Title": "Direct Unlearning Optimization for Robust and Safe Text-to-Image Models",
          "Venue": "NeurIPS",
          "Year": 2024,
          "Type": "Conference Paper",
          "Links": ""
        },
        {
          "Title": "Understanding the Latent Space of Diffusion Models through the Lens of Riemannian Geometry",
          "Venue": "NeurIPS",
          "Year": 2023,
          "Type": "Conference Paper",
          "Links": ""
        }
      ],
      "Trigger Paper Title": "Diffusion Unlearning Optimization for Robust and Safe Text-to-Image Models",
      "Trigger Paper URL": "https://blog.genlaw.org/pdfs/genlaw_icml2024/42.pdf",
      "Highlights": [
        "Paper 'Jump Your Steps' accepted at ICLR 2025",
        "Joining UPenn CIS as Ph.D. student (Fall 2025)",
        "Published 'Direct Unlearning Optimization' at NeurIPS 2024",
        "Published 'Understanding the Latent Space' at NeurIPS 2023",
        "Research on improving diffusion model safety and unlearning",
        "Worked on optimizing sampling schedules for diffusion models",
        "Awarded NeurIPS 2023 Scholar Award",
        "Published 'Upsample Guidance' (arXiv, 2024)"
      ],
      "Radar": {
        "Academic Background": 5,
        "Research Output": 5,
        "Research Alignment": 5,
        "Technical Skills": 5,
        "Recognition & Impact": 5,
        "Communication & Collaboration": 4,
        "Initiative & Independence": 5
      },
      "Total Score": 34,
      "Detailed Scores": {
        "Academic Background": "5/5 - Yong-Hyun Park is an incoming Ph.D. student at UPenn-CIS, a top-tier institution known for its strong computer science and AI research programs. This indicates a solid academic foundation and preparation for advanced research in AI.",
        "Research Output": "5/5 - The candidate has published high-impact papers in top venues such as NeurIPS, with multiple citations, demonstrating consistent and impactful research output in areas like diffusion models and safe AI.",
        "Research Alignment": "5/5 - The candidate's research interests align closely with cutting-edge topics in AI, including generative models, safe text-to-image generation, and explainable AI, which are highly relevant to current research trends and societal needs.",
        "Technical Skills": "5/5 - The candidate's work on diffusion models, Bayesian modeling, and unlearning demonstrates strong technical expertise across a range of advanced AI techniques and methodologies.",
        "Recognition & Impact": "5/5 - The candidate received the NeurIPS 2023 Scholar Award, and their work has already garnered significant attention with over 90 citations for a paper published in 2023, indicating early recognition and influence in the field.",
        "Communication & Collaboration": "4/5 - While specific details about communication and collaboration are not provided, the candidate's active presence on platforms like Google Scholar and GitHub suggests some level of engagement and sharing of research, though more evidence would be helpful.",
        "Initiative & Independence": "5/5 - The candidate has independently contributed to impactful research, as evidenced by their publications in top venues and involvement in projects related to safe and explainable AI, showing a strong ability to take initiative and work independently."
      }
    },
    "Sebastian Ochs": {
      "Name": "Sebastian Ochs",
      "Email": "****@gmail.com",
      "Current Role & Affiliation": "PhD student at Technische Universität Darmstadt",
      "Current Status": "",
      "Research Keywords": [
        "conditional text generation",
        "privacy in NLP",
        "text anonymization and de-identification",
        "differential privacy"
      ],
      "Research Focus": [
        "conditional text generation",
        "privacy in NLP",
        "text anonymization and de-identification",
        "differential privacy"
      ],
      "Profiles": {
        "OpenReview": "https://openreview.net/profile?id=~Sebastian_Ochs1"
      },
      "Publication Overview": [
        "Private Synthetic Text Generation with Diffusion Models",
        "Your Answer is Incorrect... Would you like to know why? Introducing a Bilingual Short Answer Feedback Dataset",
        "Cheating Automatic Short Answer Grading with the Adversarial Usage of Adjectives and Adverbs"
      ],
      "Top-tier Hits (Last 24 Months)": [
        "North American Chapter of the Association for Computational Linguistics 2024",
        "Annual Meeting of the Association for Computational Linguistics 2022",
        "International Journal of Artificial Intelligence in Education 2022"
      ],
      "Honors/Grants": [],
      "Academic Service / Invited Talks": [],
      "Open-source / Datasets / Projects": [
        ""
      ],
      "Representative Papers": [
        {
          "Title": "Private Synthetic Text Generation with Diffusion Models",
          "Venue": "North American Chapter of the Association for Computational Linguistics",
          "Year": 2024,
          "Type": "Conference Paper",
          "Links": "https://www.semanticscholar.org/paper/5cc9014843fb585fd2436bae2eb9fd2c87e369ae"
        },
        {
          "Title": "Your Answer is Incorrect... Would you like to know why? Introducing a Bilingual Short Answer Feedback Dataset",
          "Venue": "Annual Meeting of the Association for Computational Linguistics",
          "Year": 2022,
          "Type": "Conference Paper",
          "Links": "https://www.semanticscholar.org/paper/910fb3b376f5e9cc3087a62eedf4895aa47f7688"
        },
        {
          "Title": "Cheating Automatic Short Answer Grading with the Adversarial Usage of Adjectives and Adverbs",
          "Venue": "International Journal of Artificial Intelligence in Education",
          "Year": 2022,
          "Type": "Conference Paper",
          "Links": "https://www.semanticscholar.org/paper/c570c93650c4521f5731381f002a70b4268aa7b6"
        }
      ],
      "Trigger Paper Title": "Private Synthetic Text Generation with Diffusion Models",
      "Trigger Paper URL": "https://aclanthology.org/2025.naacl-long.532.pdf",
      "Highlights": [],
      "Radar": {
        "Academic Background": 5,
        "Research Output": 4,
        "Research Alignment": 5,
        "Technical Skills": 4,
        "Recognition & Impact": 3,
        "Communication & Collaboration": 3,
        "Initiative & Independence": 4
      },
      "Total Score": 29,
      "Detailed Scores": {
        "Academic Background": "5/5 - Sebastian Ochs is a PhD student at Technische Universität Darmstadt, a reputable institution known for its strong research in computer science and artificial intelligence, indicating a solid academic foundation.",
        "Research Output": "4/5 - He has published in top-tier venues such as ACL and the International Journal of Artificial Intelligence in Education, with several papers showing moderate citation counts, reflecting meaningful contributions to his field.",
        "Research Alignment": "5/5 - His research interests in conditional text generation, privacy in NLP, and differential privacy align closely with current trends and challenges in natural language processing, particularly in ethical and secure AI.",
        "Technical Skills": "4/5 - His work on diffusion models for private synthetic text generation and adversarial techniques in short answer grading suggests strong technical expertise in both NLP and machine learning.",
        "Recognition & Impact": "3/5 - While his work has been cited multiple times, the citations are not exceptionally high, suggesting that his impact is growing but not yet widely recognized across the broader research community.",
        "Communication & Collaboration": "3/5 - There is limited information provided about his collaborative efforts or communication skills, though his publications suggest some level of engagement with the academic community.",
        "Initiative & Independence": "4/5 - He has pursued independent research in niche areas like text anonymization and differential privacy, as evidenced by his publications, indicating a proactive and self-directed approach to his work."
      }
    }
  },
  "all_serp": [
    {
      "url": "https://arxiv.org/abs/2303.06574",
      "title": "Diffusion Models for Non-autoregressive Text Generation: A Survey",
      "snippet": "",
      "engine": "azure_ai_agent",
      "category": {
        "title": "Diffusion Models for Non-autoregressive Text Generation: A Survey",
        "authors": [
          "Yifan Li",
          "Kun Zhou",
          "Wayne Xin Zhao",
          "Ji-Rong Wen"
        ],
        "arxiv_link": "https://arxiv.org/abs/2303.06574"
      },
      "term": "text generation, diffusion model ICLR 2026",
      "parsed_url": "https://arxiv.org/abs/2303.06574",
      "engines": [
        "azure_aiagent"
      ],
      "positions": [
        1
      ]
    },
    {
      "url": "https://arxiv.org/abs/2205.14217",
      "title": "Diffusion-LM Improves Controllable Text Generation",
      "snippet": "",
      "engine": "azure_ai_agent",
      "category": {
        "title": "Diffusion-LM Improves Controllable Text Generation",
        "authors": [
          "Xiang Lisa Li",
          "John Thickstun",
          "Ishaan Gulrajani",
          "Percy Liang",
          "Tatsunori Hashimoto"
        ],
        "arxiv_link": "https://arxiv.org/abs/2205.14217"
      },
      "term": "text generation, diffusion model ICLR 2026",
      "parsed_url": "https://arxiv.org/abs/2205.14217",
      "engines": [
        "azure_aiagent"
      ],
      "positions": [
        2
      ]
    },
    {
      "url": "https://aclanthology.org/2024.naacl-long.261.pdf",
      "title": "Empowering Diffusion Models on the Embedding Space for Text Generation",
      "snippet": "",
      "engine": "azure_ai_agent",
      "category": {
        "title": "Empowering Diffusion Models on the Embedding Space for Text Generation",
        "authors": [
          "Zhujin Gao",
          "Junliang Guo",
          "Xu Tan",
          "Yongxin Zhu",
          "Fang Zhang",
          "Jiang Bian",
          "Linli Xu"
        ],
        "arxiv_link": "https://aclanthology.org/2024.naacl-long.261.pdf"
      },
      "term": "text generation, diffusion model ICLR 2026",
      "parsed_url": "https://aclanthology.org/2024.naacl-long.261.pdf",
      "engines": [
        "azure_aiagent"
      ],
      "positions": [
        3
      ]
    },
    {
      "url": "https://arxiv.org/abs/2410.21357",
      "title": "Energy-Based Diffusion Language Models for Text Generation",
      "snippet": "",
      "engine": "azure_ai_agent",
      "category": {
        "title": "Energy-Based Diffusion Language Models for Text Generation",
        "authors": [
          "Minkai Xu",
          "Tomas Geffner",
          "Karsten Kreis",
          "Weili Nie",
          "Yilun Xu",
          "Jure Leskovec",
          "Stefano Ermon",
          "Arash Vahdat"
        ],
        "arxiv_link": "https://arxiv.org/abs/2410.21357"
      },
      "term": "text generation, diffusion model ICLR 2026",
      "parsed_url": "https://arxiv.org/abs/2410.21357",
      "engines": [
        "azure_aiagent"
      ],
      "positions": [
        4
      ]
    },
    {
      "url": "https://arxiv.org/abs/2210.08933",
      "title": "DiffuSeq: Sequence to Sequence Text Generation with Diffusion Models",
      "snippet": "",
      "engine": "azure_ai_agent",
      "category": {
        "title": "DiffuSeq: Sequence to Sequence Text Generation with Diffusion Models",
        "authors": [
          "Shansan Gong",
          "Mukai Li",
          "Jiangtao Feng",
          "Zhiyong Wu",
          "Lingpeng Kong"
        ],
        "arxiv_link": "https://arxiv.org/abs/2210.08933"
      },
      "term": "text generation, diffusion model ICLR 2026",
      "parsed_url": "https://arxiv.org/abs/2210.08933",
      "engines": [
        "azure_aiagent"
      ],
      "positions": [
        5
      ]
    },
    {
      "url": "https://arxiv.org/abs/2212.11685",
      "title": "Text Generation with Diffusion Language Models: A Pre-training Approach with Continuous Paragraph Denoise",
      "snippet": "",
      "engine": "azure_ai_agent",
      "category": {
        "title": "Text Generation with Diffusion Language Models: A Pre-training Approach with Continuous Paragraph Denoise",
        "authors": [
          "Zhenghao Lin",
          "Zhihao Zhu",
          "Zheng Yuan",
          "Yiran Wang",
          "Maosong Sun"
        ],
        "arxiv_link": "https://arxiv.org/abs/2212.11685"
      },
      "term": "text generation, diffusion model ICLR 2026",
      "parsed_url": "https://arxiv.org/abs/2212.11685",
      "engines": [
        "azure_aiagent"
      ],
      "positions": [
        6
      ]
    },
    {
      "url": "https://arxiv.org/abs/2306.10351",
      "title": "A Reparameterized Discrete Diffusion Model for Text Generation",
      "snippet": "",
      "engine": "azure_ai_agent",
      "category": {
        "title": "A Reparameterized Discrete Diffusion Model for Text Generation",
        "authors": [
          "Zheng Lin",
          "Xiaoyu Shen",
          "Guoping Hu"
        ],
        "arxiv_link": "https://arxiv.org/abs/2306.10351"
      },
      "term": "text generation, diffusion model ICLR 2026",
      "parsed_url": "https://arxiv.org/abs/2306.10351",
      "engines": [
        "azure_aiagent"
      ],
      "positions": [
        7
      ]
    },
    {
      "url": "https://arxiv.org/abs/2209.07261",
      "title": "Self-conditioned Embedding Diffusion for Text Generation",
      "snippet": "",
      "engine": "azure_ai_agent",
      "category": {
        "title": "Self-conditioned Embedding Diffusion for Text Generation",
        "authors": [
          "Robin Strudel",
          "Jean-Baptiste Cordonnier",
          "Thomas Scialom",
          "Edouard Grave"
        ],
        "arxiv_link": "https://arxiv.org/abs/2209.07261"
      },
      "term": "text generation, diffusion model ICLR 2026",
      "parsed_url": "https://arxiv.org/abs/2209.07261",
      "engines": [
        "azure_aiagent"
      ],
      "positions": [
        8
      ]
    },
    {
      "url": "https://arxiv.org/abs/2107.03006",
      "title": "Structured Denoising Diffusion Models in Discrete State-Spaces",
      "snippet": "",
      "engine": "azure_ai_agent",
      "category": {
        "title": "Structured Denoising Diffusion Models in Discrete State-Spaces",
        "authors": [
          "Jacob Austin",
          "Luyu Wang",
          "Kathy Qian",
          "Mike Lewis",
          "Xiang Lisa Li",
          "Yuhuai Wu",
          "Percy Liang"
        ],
        "arxiv_link": "https://arxiv.org/abs/2107.03006"
      },
      "term": "text generation, diffusion model ICLR 2026",
      "parsed_url": "https://arxiv.org/abs/2107.03006",
      "engines": [
        "azure_aiagent"
      ],
      "positions": [
        9
      ]
    },
    {
      "url": "https://arxiv.org/abs/2203.15546",
      "title": "Latent Diffusion for Language Generation",
      "snippet": "",
      "engine": "azure_ai_agent",
      "category": {
        "title": "Latent Diffusion for Language Generation",
        "authors": [
          "Justin Lovelace",
          "James Glass"
        ],
        "arxiv_link": "https://arxiv.org/abs/2203.15546"
      },
      "term": "text generation, diffusion model ICLR 2026",
      "parsed_url": "https://arxiv.org/abs/2203.15546",
      "engines": [
        "azure_aiagent"
      ],
      "positions": [
        10
      ]
    },
    {
      "url": "https://aclanthology.org/2024.naacl-long.261/",
      "title": "Empowering Diffusion Models on the Embedding Space for Text Generation",
      "snippet": "",
      "engine": "azure_ai_agent",
      "category": {
        "title": "Empowering Diffusion Models on the Embedding Space for Text Generation",
        "authors": [
          "Zhujin Gao",
          "Junliang Guo",
          "Xu Tan",
          "Yongxin Zhu",
          "Fang Zhang",
          "Jiang Bian",
          "Linli Xu"
        ],
        "arxiv_link": "https://aclanthology.org/2024.naacl-long.261/"
      },
      "term": "text generation, diffusion model ICML 2026",
      "parsed_url": "https://aclanthology.org/2024.naacl-long.261/",
      "engines": [
        "azure_aiagent"
      ],
      "positions": [
        12
      ]
    },
    {
      "url": "https://proceedings.neurips.cc/paper_files/paper/2023/file/7d866abba506e5a56335e4644ebe18f9-Paper-Conference.pdf",
      "title": "AR-D: Auto-Regressive Diffusion Model for Text Generation",
      "snippet": "",
      "engine": "azure_ai_agent",
      "category": {
        "title": "AR-D: Auto-Regressive Diffusion Model for Text Generation",
        "authors": [
          "Hang Jiang",
          "Guoyong Cai",
          "Xiaolu Tang"
        ],
        "arxiv_link": "https://proceedings.neurips.cc/paper_files/paper/2023/file/7d866abba506e5a56335e4644ebe18f9-Paper-Conference.pdf"
      },
      "term": "text generation, diffusion model ICML 2026",
      "parsed_url": "https://proceedings.neurips.cc/paper_files/paper/2023/file/7d866abba506e5a56335e4644ebe18f9-Paper-Conference.pdf",
      "engines": [
        "azure_aiagent"
      ],
      "positions": [
        16
      ]
    },
    {
      "url": "https://aclanthology.org/2023.findings-acl.721.pdf",
      "title": "Can Diffusion Model Achieve Better Performance in Text Generation?",
      "snippet": "",
      "engine": "azure_ai_agent",
      "category": {
        "title": "Can Diffusion Model Achieve Better Performance in Text Generation?",
        "authors": [
          "Hang Jiang",
          "Guoyong Cai",
          "Xiaolu Tang"
        ],
        "arxiv_link": "https://aclanthology.org/2023.findings-acl.721.pdf"
      },
      "term": "text generation, diffusion model ICML 2026",
      "parsed_url": "https://aclanthology.org/2023.findings-acl.721.pdf",
      "engines": [
        "azure_aiagent"
      ],
      "positions": [
        18
      ]
    },
    {
      "url": "https://arxiv.org/abs/2305.16390",
      "title": "A Reparameterized Discrete Diffusion Model for Text Generation",
      "snippet": "",
      "engine": "azure_ai_agent",
      "category": {
        "title": "A Reparameterized Discrete Diffusion Model for Text Generation",
        "authors": [
          "Lin Zheng",
          "Wenpeng Chen",
          "Zhiyong Wu",
          "Dong Wang",
          "Xunying Liu"
        ],
        "arxiv_link": "https://arxiv.org/abs/2305.16390"
      },
      "term": "text generation, diffusion model ICML 2026",
      "parsed_url": "https://arxiv.org/abs/2305.16390",
      "engines": [
        "azure_aiagent"
      ],
      "positions": [
        20
      ]
    },
    {
      "url": "https://arxiv.org/pdf/2212.11685",
      "title": "Text Generation with Diffusion Language Models: A Pre-training Approach (GENIE)",
      "snippet": "",
      "engine": "azure_ai_agent",
      "category": {
        "title": "Text Generation with Diffusion Language Models: A Pre-training Approach (GENIE)",
        "authors": [
          "Qing Liu",
          "Yequan Wang",
          "Xiang Li",
          "Jie Fu",
          "Jian Wang",
          "Dongyan Zhao",
          "Rui Yan"
        ],
        "arxiv_link": "https://arxiv.org/pdf/2212.11685"
      },
      "term": "text generation, diffusion model NeurIPS 2026",
      "parsed_url": "https://arxiv.org/pdf/2212.11685",
      "engines": [
        "azure_aiagent"
      ],
      "positions": [
        24
      ]
    },
    {
      "url": "https://www.ijcai.org/proceedings/2023/0750.pdf",
      "title": "Diffusion Models for Non-autoregressive Text Generation: A Survey",
      "snippet": "",
      "engine": "azure_ai_agent",
      "category": {
        "title": "Diffusion Models for Non-autoregressive Text Generation: A Survey",
        "authors": [
          "Xiang Li",
          "Xiang Lisa Li",
          "Tatsunori Hashimoto",
          "Percy Liang"
        ],
        "arxiv_link": "https://www.ijcai.org/proceedings/2023/0750.pdf"
      },
      "term": "text generation, diffusion model NeurIPS 2026",
      "parsed_url": "https://www.ijcai.org/proceedings/2023/0750.pdf",
      "engines": [
        "azure_aiagent"
      ],
      "positions": [
        26
      ]
    },
    {
      "url": "https://github.com/bansky-cl/Diffusion-LM-Papers",
      "title": "Scaling Diffusion Language Models via Adaptation from Autoregressive Models",
      "snippet": "",
      "engine": "azure_ai_agent",
      "category": {
        "title": "Scaling Diffusion Language Models via Adaptation from Autoregressive Models",
        "authors": [
          "Ziqing Gong",
          "Others (See link for full list)"
        ],
        "arxiv_link": "https://github.com/bansky-cl/Diffusion-LM-Papers"
      },
      "term": "text generation, diffusion model NeurIPS 2026",
      "parsed_url": "https://github.com/bansky-cl/Diffusion-LM-Papers",
      "engines": [
        "azure_aiagent"
      ],
      "positions": [
        27
      ]
    },
    {
      "url": "https://arxiv.org/abs/2402.03223",
      "title": "Non-Markovian Discrete Diffusion with Causal Language Models",
      "snippet": "",
      "engine": "azure_ai_agent",
      "category": {
        "title": "Non-Markovian Discrete Diffusion with Causal Language Models",
        "authors": [
          "Feiran Zhang",
          "Haotian Liu",
          "Tao Ge",
          "Xu Tan",
          "Bing Qin",
          "Tie-Yan Liu"
        ],
        "arxiv_link": "https://arxiv.org/abs/2402.03223"
      },
      "term": "text generation, diffusion model ACL 2026",
      "parsed_url": "https://arxiv.org/abs/2402.03223",
      "engines": [
        "azure_aiagent"
      ],
      "positions": [
        37
      ]
    },
    {
      "url": "https://arxiv.org/abs/2410.20004",
      "title": "Scaling up Masked Diffusion Models on Text",
      "snippet": "",
      "engine": "azure_ai_agent",
      "category": {
        "title": "Scaling up Masked Diffusion Models on Text",
        "authors": [
          "Weili Nie",
          "Haixu Wu",
          "Karsten Kreis",
          "Arash Vahdat"
        ],
        "arxiv_link": "https://arxiv.org/abs/2410.20004"
      },
      "term": "text generation, diffusion model ACL 2026",
      "parsed_url": "https://arxiv.org/abs/2410.20004",
      "engines": [
        "azure_aiagent"
      ],
      "positions": [
        38
      ]
    },
    {
      "url": "https://arxiv.org/abs/2410.20809",
      "title": "Beyond Autoregression: Discrete Diffusion for Complex Reasoning and Planning",
      "snippet": "",
      "engine": "azure_ai_agent",
      "category": {
        "title": "Beyond Autoregression: Discrete Diffusion for Complex Reasoning and Planning",
        "authors": [
          "Qianqian Ye",
          "Minkai Xu",
          "Arash Vahdat",
          "Stefano Ermon"
        ],
        "arxiv_link": "https://arxiv.org/abs/2410.20809"
      },
      "term": "text generation, diffusion model ACL 2026",
      "parsed_url": "https://arxiv.org/abs/2410.20809",
      "engines": [
        "azure_aiagent"
      ],
      "positions": [
        39
      ]
    },
    {
      "url": "https://arxiv.org/abs/2410.21541",
      "title": "Scaling Diffusion Language Models via Adaptation from Autoregressive Models",
      "snippet": "",
      "engine": "azure_ai_agent",
      "category": {
        "title": "Scaling Diffusion Language Models via Adaptation from Autoregressive Models",
        "authors": [
          "Jiawei Gong",
          "Haoxian Zhang",
          "Zhiyi Ouyang",
          "Yilun Xu",
          "Stefano Ermon"
        ],
        "arxiv_link": "https://arxiv.org/abs/2410.21541"
      },
      "term": "text generation, diffusion model ACL 2026",
      "parsed_url": "https://arxiv.org/abs/2410.21541",
      "engines": [
        "azure_aiagent"
      ],
      "positions": [
        40
      ]
    },
    {
      "url": "https://aclanthology.org/2024.naacl-long.2.pdf",
      "title": "Text Diffusion Model with Encoder-Decoder Transformers for Sequence-to-Sequence Generation",
      "snippet": "",
      "engine": "azure_ai_agent",
      "category": {
        "title": "Text Diffusion Model with Encoder-Decoder Transformers for Sequence-to-Sequence Generation",
        "authors": [
          "Hongyi Yuan",
          "Zheng Yuan",
          "Chuanqi Tan",
          "Fei Huang",
          "Songfang Huang"
        ],
        "arxiv_link": "https://aclanthology.org/2024.naacl-long.2.pdf"
      },
      "term": "text generation, diffusion model EMNLP 2026",
      "parsed_url": "https://aclanthology.org/2024.naacl-long.2.pdf",
      "engines": [
        "azure_aiagent"
      ],
      "positions": [
        42
      ]
    },
    {
      "url": "https://arxiv.org/abs/2212.09412",
      "title": "Empowering Diffusion Models on the Embedding Space for Text Generation",
      "snippet": "",
      "engine": "azure_ai_agent",
      "category": {
        "title": "Empowering Diffusion Models on the Embedding Space for Text Generation",
        "authors": [
          "Zhujin Gao",
          "Junliang Guo",
          "Xu Tan",
          "Yongxin Zhu",
          "Fang Zhang",
          "Jiang Bian",
          "Linli Xu"
        ],
        "arxiv_link": "https://arxiv.org/abs/2212.09412"
      },
      "term": "text generation, diffusion model EMNLP 2026",
      "parsed_url": "https://arxiv.org/abs/2212.09412",
      "engines": [
        "azure_aiagent"
      ],
      "positions": [
        44
      ]
    },
    {
      "url": "https://arxiv.org/abs/2110.12979",
      "title": "Structured Denoising Diffusion Models in Discrete State-Spaces",
      "snippet": "",
      "engine": "azure_ai_agent",
      "category": {
        "title": "Structured Denoising Diffusion Models in Discrete State-Spaces",
        "authors": [
          "Jacob Austin",
          "Katherine Lee",
          "Jann Karpicke",
          "Alexander Poms",
          "David M. Blei"
        ],
        "arxiv_link": "https://arxiv.org/abs/2110.12979"
      },
      "term": "text generation, diffusion model EMNLP 2026",
      "parsed_url": "https://arxiv.org/abs/2110.12979",
      "engines": [
        "azure_aiagent"
      ],
      "positions": [
        48
      ]
    },
    {
      "url": "https://arxiv.org/abs/2211.11275",
      "title": "Self-conditioned Embedding Diffusion for Text Generation",
      "snippet": "",
      "engine": "azure_ai_agent",
      "category": {
        "title": "Self-conditioned Embedding Diffusion for Text Generation",
        "authors": [
          "Robin Strudel",
          "Jacob Austin",
          "Katherine Lee",
          "Ruiqi Gao",
          "David M. Blei"
        ],
        "arxiv_link": "https://arxiv.org/abs/2211.11275"
      },
      "term": "text generation, diffusion model EMNLP 2026",
      "parsed_url": "https://arxiv.org/abs/2211.11275",
      "engines": [
        "azure_aiagent"
      ],
      "positions": [
        49
      ]
    },
    {
      "url": "https://arxiv.org/abs/2307.04835",
      "title": "A Reparameterized Discrete Diffusion Model for Text Generation",
      "snippet": "",
      "engine": "azure_ai_agent",
      "category": {
        "title": "A Reparameterized Discrete Diffusion Model for Text Generation",
        "authors": [
          "Lin Zheng",
          "Haoyu Zhang",
          "Wei Liu",
          "An Yang"
        ],
        "arxiv_link": "https://arxiv.org/abs/2307.04835"
      },
      "term": "text generation, diffusion model EMNLP 2026",
      "parsed_url": "https://arxiv.org/abs/2307.04835",
      "engines": [
        "azure_aiagent"
      ],
      "positions": [
        50
      ]
    },
    {
      "url": "https://arxiv.org/abs/2306.08236",
      "title": "Text Diffusion Model with Encoder-Decoder Transformers for Sequence-to-Sequence Generation",
      "snippet": "",
      "engine": "azure_ai_agent",
      "category": {
        "title": "Text Diffusion Model with Encoder-Decoder Transformers for Sequence-to-Sequence Generation",
        "authors": [
          "Hongyi Yuan",
          "Zheng Yuan",
          "Chuanqi Tan",
          "Fei Huang",
          "Songfang Huang"
        ],
        "arxiv_link": "https://arxiv.org/abs/2306.08236"
      },
      "term": "text generation, diffusion model NAACL 2026",
      "parsed_url": "https://arxiv.org/abs/2306.08236",
      "engines": [
        "azure_aiagent"
      ],
      "positions": [
        51
      ]
    },
    {
      "url": "https://arxiv.org/abs/2302.05324",
      "title": "Empowering Diffusion Models on the Embedding Space for Text Generation",
      "snippet": "",
      "engine": "azure_ai_agent",
      "category": {
        "title": "Empowering Diffusion Models on the Embedding Space for Text Generation",
        "authors": [
          "Zhujin Gao",
          "Junliang Guo",
          "Xu Tan",
          "Yongxin Zhu",
          "Fang Zhang",
          "Jiang Bian",
          "Linli Xu"
        ],
        "arxiv_link": "https://arxiv.org/abs/2302.05324"
      },
      "term": "text generation, diffusion model NAACL 2026",
      "parsed_url": "https://arxiv.org/abs/2302.05324",
      "engines": [
        "azure_aiagent"
      ],
      "positions": [
        52
      ]
    },
    {
      "url": "https://arxiv.org/abs/2311.01986",
      "title": "Diffusion-NAT: Self-Prompting Discrete Diffusion for Non-Autoregressive Text Generation",
      "snippet": "",
      "engine": "azure_ai_agent",
      "category": {
        "title": "Diffusion-NAT: Self-Prompting Discrete Diffusion for Non-Autoregressive Text Generation",
        "authors": [
          "Kun Zhou",
          "Yifan Li",
          "Xin Zhao",
          "Ji-Rong Wen"
        ],
        "arxiv_link": "https://arxiv.org/abs/2311.01986"
      },
      "term": "text generation, diffusion model NAACL 2026",
      "parsed_url": "https://arxiv.org/abs/2311.01986",
      "engines": [
        "azure_aiagent"
      ],
      "positions": [
        55
      ]
    },
    {
      "url": "https://arxiv.org/abs/2306.00575",
      "title": "A Reparameterized Discrete Diffusion Model for Text Generation",
      "snippet": "",
      "engine": "azure_ai_agent",
      "category": {
        "title": "A Reparameterized Discrete Diffusion Model for Text Generation",
        "authors": [
          "Lin Zheng",
          "Jie Fu",
          "Qinghua Liu",
          "Lu Zhang",
          "Youzheng Wu",
          "Jian Sun"
        ],
        "arxiv_link": "https://arxiv.org/abs/2306.00575"
      },
      "term": "text generation, diffusion model NAACL 2026",
      "parsed_url": "https://arxiv.org/abs/2306.00575",
      "engines": [
        "azure_aiagent"
      ],
      "positions": [
        58
      ]
    },
    {
      "url": "https://arxiv.org/abs/2208.12252",
      "title": "Self-conditioned Embedding Diffusion for Text Generation",
      "snippet": "",
      "engine": "azure_ai_agent",
      "category": {
        "title": "Self-conditioned Embedding Diffusion for Text Generation",
        "authors": [
          "Robin Strudel",
          "Jean-Baptiste Cordonnier",
          "Thomas Scialom",
          "Edouard Grave"
        ],
        "arxiv_link": "https://arxiv.org/abs/2208.12252"
      },
      "term": "text generation, diffusion model NAACL 2026",
      "parsed_url": "https://arxiv.org/abs/2208.12252",
      "engines": [
        "azure_aiagent"
      ],
      "positions": [
        60
      ]
    },
    {
      "url": "https://arxiv.org/abs/2503.00522",
      "title": "Periodic Materials Generation using Text-Guided Joint Diffusion Model",
      "snippet": "",
      "engine": "azure_ai_agent",
      "category": {
        "title": "Periodic Materials Generation using Text-Guided Joint Diffusion Model",
        "authors": [
          "Kishalay Das",
          "Subhojyoti Khastagir",
          "Pawan Goyal",
          "Seung-Cheol Lee",
          "Satadeep Bhattacharjee",
          "Niloy Ganguly"
        ],
        "arxiv_link": "https://arxiv.org/abs/2503.00522"
      },
      "term": "text generation, diffusion model ICLR 2025",
      "parsed_url": "https://arxiv.org/abs/2503.00522",
      "engines": [
        "azure_aiagent"
      ],
      "positions": [
        62
      ]
    },
    {
      "url": "https://openreview.net/forum?id=l2zFn6TIQi",
      "title": "Controlling Language and Diffusion Models by Transporting Activations",
      "snippet": "",
      "engine": "azure_ai_agent",
      "category": {
        "title": "Controlling Language and Diffusion Models by Transporting Activations",
        "authors": [
          "Pau Rodriguez",
          "Arno Blaas",
          "Michal Klein",
          "Luca Zappella",
          "Nicholas Apostoloff",
          "Marco Cuturi",
          "Xavier Suau"
        ],
        "arxiv_link": "https://openreview.net/forum?id=l2zFn6TIQi"
      },
      "term": "text generation, diffusion model ICLR 2025",
      "parsed_url": "https://openreview.net/forum?id=l2zFn6TIQi",
      "engines": [
        "azure_aiagent"
      ],
      "positions": [
        63
      ]
    },
    {
      "url": "https://openreview.net/forum?id=tyEyYT267x",
      "title": "Block Diffusion: Interpolating Between Autoregressive and Diffusion Language Models",
      "snippet": "",
      "engine": "azure_ai_agent",
      "category": {
        "title": "Block Diffusion: Interpolating Between Autoregressive and Diffusion Language Models",
        "authors": [
          "Marianne Arriola",
          "Aaron Gokaslan",
          "Justin T. Chiu",
          "Zhihan Yang",
          "Zhixuan Qi",
          "Jiaqi Han",
          "Subham Sekhar Sahoo",
          "Volodymyr Kuleshov"
        ],
        "arxiv_link": "https://openreview.net/forum?id=tyEyYT267x"
      },
      "term": "text generation, diffusion model ICLR 2025",
      "parsed_url": "https://openreview.net/forum?id=tyEyYT267x",
      "engines": [
        "azure_aiagent"
      ],
      "positions": [
        64
      ]
    },
    {
      "url": "https://arxiv.org/abs/2502.09992",
      "title": "Large Language Diffusion Models",
      "snippet": "",
      "engine": "azure_ai_agent",
      "category": {
        "title": "Large Language Diffusion Models",
        "authors": [
          "Zeming Li",
          "Xiaoyu Li",
          "Xiang Yu",
          "Kaixiang Mo",
          "Xiaotao Gu",
          "Chen Henry Wu",
          "Yiming Yang",
          "Jian Tang"
        ],
        "arxiv_link": "https://arxiv.org/abs/2502.09992"
      },
      "term": "text generation, diffusion model ICLR 2025",
      "parsed_url": "https://arxiv.org/abs/2502.09992",
      "engines": [
        "azure_aiagent"
      ],
      "positions": [
        65
      ]
    },
    {
      "url": "https://arxiv.org/abs/2505.21467",
      "title": "Accelerating Diffusion Language Model Inference via Efficient KV Cache Sharing",
      "snippet": "",
      "engine": "azure_ai_agent",
      "category": {
        "title": "Accelerating Diffusion Language Model Inference via Efficient KV Cache Sharing",
        "authors": [
          "Wei Li",
          "Jiahui Geng",
          "Shizheng Li",
          "Zirui Wang",
          "Hangfeng He"
        ],
        "arxiv_link": "https://arxiv.org/abs/2505.21467"
      },
      "term": "text generation, diffusion model ICLR 2025",
      "parsed_url": "https://arxiv.org/abs/2505.21467",
      "engines": [
        "azure_aiagent"
      ],
      "positions": [
        66
      ]
    },
    {
      "url": "https://iclr.cc/virtual/2025/poster/29366",
      "title": "Scaling up Masked Diffusion Models on Text",
      "snippet": "",
      "engine": "azure_ai_agent",
      "category": {
        "title": "Scaling up Masked Diffusion Models on Text",
        "authors": [
          "Wenhao Huang",
          "Jingjing Xu",
          "Xu Tan",
          "Lei He",
          "Yu Wu",
          "Jiang Bian",
          "Shi Han",
          "Jian-Yun Nie"
        ],
        "arxiv_link": "https://iclr.cc/virtual/2025/poster/29366"
      },
      "term": "text generation, diffusion model ICLR 2025",
      "parsed_url": "https://iclr.cc/virtual/2025/poster/29366",
      "engines": [
        "azure_aiagent"
      ],
      "positions": [
        68
      ]
    },
    {
      "url": "https://iclr.cc/virtual/2025/poster/29614",
      "title": "Toward Understanding Text Hallucination of Diffusion Language Models",
      "snippet": "",
      "engine": "azure_ai_agent",
      "category": {
        "title": "Toward Understanding Text Hallucination of Diffusion Language Models",
        "authors": [
          "Xinyi Dang",
          "Wenhao Huang",
          "Jingjing Xu",
          "Lei He",
          "Yu Wu"
        ],
        "arxiv_link": "https://iclr.cc/virtual/2025/poster/29614"
      },
      "term": "text generation, diffusion model ICLR 2025",
      "parsed_url": "https://iclr.cc/virtual/2025/poster/29614",
      "engines": [
        "azure_aiagent"
      ],
      "positions": [
        69
      ]
    },
    {
      "url": "https://ai-ml.cn/iclr25",
      "title": "DreamDistribution: Prompt Distribution Learning for Text-to-Image Diffusion Models",
      "snippet": "",
      "engine": "azure_ai_agent",
      "category": {
        "title": "DreamDistribution: Prompt Distribution Learning for Text-to-Image Diffusion Models",
        "authors": [
          "Brian Nlong Zhao",
          "Yuhang Xiao",
          "Jiashu Xu",
          "Xinyang Jiang",
          "Yifan Yang",
          "Dongsheng Li",
          "Laurent Itti",
          "Vibhav Vineet",
          "Yunhao Ge"
        ],
        "arxiv_link": "https://ai-ml.cn/iclr25"
      },
      "term": "text generation, diffusion model ICLR 2025",
      "parsed_url": "https://ai-ml.cn/iclr25",
      "engines": [
        "azure_aiagent"
      ],
      "positions": [
        70
      ]
    },
    {
      "url": "https://arxiv.org/abs/2505.13740",
      "title": "Improving Compositional Generation with Diffusion Models Using Lift Scores",
      "snippet": "",
      "engine": "azure_ai_agent",
      "category": {
        "title": "Improving Compositional Generation with Diffusion Models Using Lift Scores",
        "authors": [
          "Chenning Yu",
          "Sicun Gao"
        ],
        "arxiv_link": "https://arxiv.org/abs/2505.13740"
      },
      "term": "text generation, diffusion model ICML 2025",
      "parsed_url": "https://arxiv.org/abs/2505.13740",
      "engines": [
        "azure_aiagent"
      ],
      "positions": [
        71
      ]
    },
    {
      "url": "https://arxiv.org/abs/2506.10892",
      "title": "The Diffusion Duality",
      "snippet": "",
      "engine": "azure_ai_agent",
      "category": {
        "title": "The Diffusion Duality",
        "authors": [
          "Subham Sekhar Sahoo",
          "Justin Deschenaux",
          "Aaron Gokaslan",
          "Guanghan Wang",
          "Justin Chiu",
          "Volodymyr Kuleshov"
        ],
        "arxiv_link": "https://arxiv.org/abs/2506.10892"
      },
      "term": "text generation, diffusion model ICML 2025",
      "parsed_url": "https://arxiv.org/abs/2506.10892",
      "engines": [
        "azure_aiagent"
      ],
      "positions": [
        72
      ]
    },
    {
      "url": "https://arxiv.org/abs/2503.03595",
      "title": "Towards Understanding Text Hallucination of Diffusion Models via Local Generation Bias",
      "snippet": "",
      "engine": "azure_ai_agent",
      "category": {
        "title": "Towards Understanding Text Hallucination of Diffusion Models via Local Generation Bias",
        "authors": [
          "Rui Lu",
          "Runzhe Wang",
          "Kaifeng Lyu",
          "Xitai Jiang",
          "Gao Huang",
          "Mengdi Wang"
        ],
        "arxiv_link": "https://arxiv.org/abs/2503.03595"
      },
      "term": "text generation, diffusion model ICML 2025",
      "parsed_url": "https://arxiv.org/abs/2503.03595",
      "engines": [
        "azure_aiagent"
      ],
      "positions": [
        73
      ]
    },
    {
      "url": "https://arxiv.org/abs/2506.07903",
      "title": "Diffuse Everything: Multimodal Diffusion Models on Arbitrary State Spaces",
      "snippet": "",
      "engine": "azure_ai_agent",
      "category": {
        "title": "Diffuse Everything: Multimodal Diffusion Models on Arbitrary State Spaces",
        "authors": [
          "Kevin Rojas",
          "Yuchen Zhu",
          "Sichen Zhu",
          "Felix X.-F. Ye",
          "Molei Tao"
        ],
        "arxiv_link": "https://arxiv.org/abs/2506.07903"
      },
      "term": "text generation, diffusion model ICML 2025",
      "parsed_url": "https://arxiv.org/abs/2506.07903",
      "engines": [
        "azure_aiagent"
      ],
      "positions": [
        74
      ]
    },
    {
      "url": "https://arxiv.org/abs/2508.03256",
      "title": "Beyond Isolated Words: Diffusion Brush for Handwritten Text-Line Generation",
      "snippet": "",
      "engine": "azure_ai_agent",
      "category": {
        "title": "Beyond Isolated Words: Diffusion Brush for Handwritten Text-Line Generation",
        "authors": [
          "Gang Dai",
          "Yifan Zhang",
          "Yutao Qin",
          "Qiangya Guo",
          "Shuangping Huang",
          "Shuicheng Yan"
        ],
        "arxiv_link": "https://arxiv.org/abs/2508.03256"
      },
      "term": "text generation, diffusion model ICML 2025",
      "parsed_url": "https://arxiv.org/abs/2508.03256",
      "engines": [
        "azure_aiagent"
      ],
      "positions": [
        75
      ]
    },
    {
      "url": "https://arxiv.org/abs/2505.22165",
      "title": "Unifying Continuous and Discrete Text Diffusion with Non-simultaneous Diffusion Processes",
      "snippet": "",
      "engine": "azure_ai_agent",
      "category": {
        "title": "Unifying Continuous and Discrete Text Diffusion with Non-simultaneous Diffusion Processes",
        "authors": [
          "Bocheng Li",
          "Zhujin Gao",
          "Linli Xu"
        ],
        "arxiv_link": "https://arxiv.org/abs/2505.22165"
      },
      "term": "text generation, diffusion model ICML 2025",
      "parsed_url": "https://arxiv.org/abs/2505.22165",
      "engines": [
        "azure_aiagent"
      ],
      "positions": [
        76
      ]
    },
    {
      "url": "https://arxiv.org/abs/2505.23606",
      "title": "Muddit: Liberating Generation Beyond Text-to-Image with a Unified Discrete Diffusion Transformer",
      "snippet": "",
      "engine": "azure_ai_agent",
      "category": {
        "title": "Muddit: Liberating Generation Beyond Text-to-Image with a Unified Discrete Diffusion Transformer",
        "authors": [
          "Yuchen Zhu",
          "Kevin Rojas",
          "Felix X.-F. Ye",
          "Molei Tao"
        ],
        "arxiv_link": "https://arxiv.org/abs/2505.23606"
      },
      "term": "text generation, diffusion model ICML 2025",
      "parsed_url": "https://arxiv.org/abs/2505.23606",
      "engines": [
        "azure_aiagent"
      ],
      "positions": [
        77
      ]
    },
    {
      "url": "https://arxiv.org/abs/2409.13031",
      "title": "How Discrete and Continuous Diffusion Meet: Comprehensive Analysis of Discrete Diffusion Models via a Stochastic Integral Framework",
      "snippet": "",
      "engine": "azure_ai_agent",
      "category": {
        "title": "How Discrete and Continuous Diffusion Meet: Comprehensive Analysis of Discrete Diffusion Models via a Stochastic Integral Framework",
        "authors": [
          "Hao Ren",
          "Ming Ding",
          "Jing Liu",
          "Yichi Zhang",
          "Wenqi Wang",
          "Yuan Yao",
          "Zhilin Yang"
        ],
        "arxiv_link": "https://arxiv.org/abs/2409.13031"
      },
      "term": "text generation, diffusion model ICML 2025",
      "parsed_url": "https://arxiv.org/abs/2409.13031",
      "engines": [
        "azure_aiagent"
      ],
      "positions": [
        78
      ]
    },
    {
      "url": "https://arxiv.org/abs/2408.17923",
      "title": "Scaling up Masked Diffusion Models on Text",
      "snippet": "",
      "engine": "azure_ai_agent",
      "category": {
        "title": "Scaling up Masked Diffusion Models on Text",
        "authors": [
          "Zihan Nie",
          "Ming Ding",
          "Yichi Zhang",
          "Wenqi Wang",
          "Zhilin Yang"
        ],
        "arxiv_link": "https://arxiv.org/abs/2408.17923"
      },
      "term": "text generation, diffusion model ICML 2025",
      "parsed_url": "https://arxiv.org/abs/2408.17923",
      "engines": [
        "azure_aiagent"
      ],
      "positions": [
        79
      ]
    },
    {
      "url": "https://proceedings.mlr.press/v202/lin23d/lin23d.pdf",
      "title": "Text Generation with Diffusion Language Models: A Pre-training Approach (GENIE)",
      "snippet": "",
      "engine": "azure_ai_agent",
      "category": {
        "title": "Text Generation with Diffusion Language Models: A Pre-training Approach (GENIE)",
        "authors": [
          "Yong Lin",
          "Tianyu Liu",
          "Zheng Wang",
          "Weining Song",
          "Yiran Yang",
          "Xipeng Qiu"
        ],
        "arxiv_link": "https://proceedings.mlr.press/v202/lin23d/lin23d.pdf"
      },
      "term": "text generation, diffusion model ICML 2025",
      "parsed_url": "https://proceedings.mlr.press/v202/lin23d/lin23d.pdf",
      "engines": [
        "azure_aiagent"
      ],
      "positions": [
        80
      ]
    },
    {
      "url": "https://arxiv.org/abs/2410.21357",
      "title": "Energy-Based Diffusion Language Models for Text Generation",
      "snippet": "",
      "engine": "azure_ai_agent",
      "category": {
        "title": "Energy-Based Diffusion Language Models for Text Generation",
        "authors": [
          "Minkai Xu",
          "Tomas Geffner",
          "Karsten Kreis",
          "Weili Nie",
          "Yilun Xu",
          "Jure Leskovec",
          "Stefano Ermon",
          "Arash Vahdat"
        ],
        "arxiv_link": "https://arxiv.org/abs/2410.21357"
      },
      "term": "text generation, diffusion model NeurIPS 2025",
      "parsed_url": "https://arxiv.org/abs/2410.21357",
      "engines": [
        "azure_aiagent"
      ],
      "positions": [
        1
      ]
    },
    {
      "url": "https://aclanthology.org/2025.naacl-long.532.pdf",
      "title": "Private Synthetic Text Generation with Diffusion Models",
      "snippet": "",
      "engine": "azure_ai_agent",
      "category": {
        "title": "Private Synthetic Text Generation with Diffusion Models",
        "authors": [
          "Renshu Yang",
          "Samuel Hinchliffe",
          "Daniel Li",
          "Xuchao Zhang",
          "Daming Xu",
          "Aaron Cheung",
          "Phong Le",
          "Richard Evans"
        ],
        "arxiv_link": "https://aclanthology.org/2025.naacl-long.532.pdf"
      },
      "term": "text generation, diffusion model NeurIPS 2025",
      "parsed_url": "https://aclanthology.org/2025.naacl-long.532.pdf",
      "engines": [
        "azure_aiagent"
      ],
      "positions": [
        2
      ]
    },
    {
      "url": "https://aclanthology.org/2025.findings-naacl.352/",
      "title": "Discrete Diffusion Language Model for Efficient Text Summarization",
      "snippet": "",
      "engine": "azure_ai_agent",
      "category": {
        "title": "Discrete Diffusion Language Model for Efficient Text Summarization",
        "authors": [
          "Do Huu Dat",
          "Duc Anh Do",
          "Anh Tuan Luu",
          "Wray Buntine"
        ],
        "arxiv_link": "https://aclanthology.org/2025.findings-naacl.352/"
      },
      "term": "text generation, diffusion model NeurIPS 2025",
      "parsed_url": "https://aclanthology.org/2025.findings-naacl.352/",
      "engines": [
        "azure_aiagent"
      ],
      "positions": [
        3
      ]
    },
    {
      "url": "https://arxiv.org/abs/2305.09515",
      "title": "AR-Diffusion: Auto-Regressive Diffusion Model for Text Generation",
      "snippet": "",
      "engine": "azure_ai_agent",
      "category": {
        "title": "AR-Diffusion: Auto-Regressive Diffusion Model for Text Generation",
        "authors": [
          "Tong Wu",
          "Zhihao Fan",
          "Xiao Liu",
          "Yeyun Gong",
          "Yelong Shen",
          "Jian Jiao",
          "Hai-Tao Zheng",
          "Juntao Li",
          "Zhongyu Wei",
          "Jian Guo",
          "Nan Duan",
          "Weizhu Chen"
        ],
        "arxiv_link": "https://arxiv.org/abs/2305.09515"
      },
      "term": "text generation, diffusion model NeurIPS 2025",
      "parsed_url": "https://arxiv.org/abs/2305.09515",
      "engines": [
        "azure_aiagent"
      ],
      "positions": [
        4
      ]
    },
    {
      "url": "https://arxiv.org/abs/2303.06574",
      "title": "Diffusion Models for Non-autoregressive Text Generation: A Survey",
      "snippet": "",
      "engine": "azure_ai_agent",
      "category": {
        "title": "Diffusion Models for Non-autoregressive Text Generation: A Survey",
        "authors": [
          "Yifan Li",
          "Kun Zhou",
          "Wayne Xin Zhao",
          "Ji-Rong Wen"
        ],
        "arxiv_link": "https://arxiv.org/abs/2303.06574"
      },
      "term": "text generation, diffusion model NeurIPS 2025",
      "parsed_url": "https://arxiv.org/abs/2303.06574",
      "engines": [
        "azure_aiagent"
      ],
      "positions": [
        5
      ]
    },
    {
      "url": "https://arxiv.org/pdf/2212.11685",
      "title": "Text Generation with Diffusion Language Models: A Pre-training Approach (GENIE)",
      "snippet": "",
      "engine": "azure_ai_agent",
      "category": {
        "title": "Text Generation with Diffusion Language Models: A Pre-training Approach (GENIE)",
        "authors": [
          "Han Li",
          "Fan Yang",
          "Yingqi Qu",
          "Pengjun Xie",
          "Qingyu Zhang",
          "Zhengyan Zhang",
          "Deng Cai",
          "Yan Song",
          "Zhiyuan Liu",
          "Tat-Seng Chua",
          "Maosong Sun"
        ],
        "arxiv_link": "https://arxiv.org/pdf/2212.11685"
      },
      "term": "text generation, diffusion model NeurIPS 2025",
      "parsed_url": "https://arxiv.org/pdf/2212.11685",
      "engines": [
        "azure_aiagent"
      ],
      "positions": [
        6
      ]
    },
    {
      "url": "https://arxiv.org/abs/2205.14217",
      "title": "Diffusion-LM Improves Controllable Text Generation",
      "snippet": "",
      "engine": "azure_ai_agent",
      "category": {
        "title": "Diffusion-LM Improves Controllable Text Generation",
        "authors": [
          "Yizhou Zhou",
          "Yichong Xu",
          "Jingjing Gong",
          "Wenhao Liu",
          "Hongyu Zhang",
          "Jianfeng Gao",
          "Jian Peng",
          "Zhiguang Wang"
        ],
        "arxiv_link": "https://arxiv.org/abs/2205.14217"
      },
      "term": "text generation, diffusion model NeurIPS 2025",
      "parsed_url": "https://arxiv.org/abs/2205.14217",
      "engines": [
        "azure_aiagent"
      ],
      "positions": [
        7
      ]
    },
    {
      "url": "https://aclanthology.org/2025.acl-long.210.pdf",
      "title": "Segment-Level Diffusion: A Framework for Controllable Long-Form Generation with Diffusion Language Models",
      "snippet": "",
      "engine": "azure_ai_agent",
      "category": {
        "title": "Segment-Level Diffusion: A Framework for Controllable Long-Form Generation with Diffusion Language Models",
        "authors": [
          "Yipeng Zhu",
          "Mengnan Zhao",
          "Haoran Wei",
          "Jing Sun",
          "Xipeng Qiu"
        ],
        "arxiv_link": "https://aclanthology.org/2025.acl-long.210.pdf"
      },
      "term": "text generation, diffusion model NeurIPS 2025",
      "parsed_url": "https://aclanthology.org/2025.acl-long.210.pdf",
      "engines": [
        "azure_aiagent"
      ],
      "positions": [
        8
      ]
    },
    {
      "url": "https://iclr.cc/virtual/2025/poster/29614",
      "title": "Towards Understanding Text Hallucination of Diffusion Models via Local Generation Bias",
      "snippet": "",
      "engine": "azure_ai_agent",
      "category": {
        "title": "Towards Understanding Text Hallucination of Diffusion Models via Local Generation Bias",
        "authors": [
          "Rui Lu",
          "Runzhe Wang",
          "Kaifeng Lyu",
          "Xitai Jiang",
          "Gao Huang",
          "Mengdi Wang"
        ],
        "arxiv_link": "https://iclr.cc/virtual/2025/poster/29614"
      },
      "term": "text generation, diffusion model NeurIPS 2025",
      "parsed_url": "https://iclr.cc/virtual/2025/poster/29614",
      "engines": [
        "azure_aiagent"
      ],
      "positions": [
        9
      ]
    },
    {
      "url": "https://github.com/bansky-cl/Diffusion-LM-Papers",
      "title": "Beyond Autoregression: Discrete Diffusion for Complex Reasoning and Planning",
      "snippet": "",
      "engine": "azure_ai_agent",
      "category": {
        "title": "Beyond Autoregression: Discrete Diffusion for Complex Reasoning and Planning",
        "authors": [
          "Zichao Ye",
          "Haoran Li",
          "Chenyan Jia",
          "Hongyu Ren",
          "Yu Cheng",
          "Kaiming He",
          "Xiang Ren"
        ],
        "arxiv_link": "https://github.com/bansky-cl/Diffusion-LM-Papers"
      },
      "term": "text generation, diffusion model NeurIPS 2025",
      "parsed_url": "https://github.com/bansky-cl/Diffusion-LM-Papers",
      "engines": [
        "azure_aiagent"
      ],
      "positions": [
        10
      ]
    },
    {
      "url": "https://arxiv.org/abs/2503.03595",
      "title": "Towards Understanding Text Hallucination of Diffusion Models via Local Generation Bias",
      "snippet": "",
      "engine": "azure_ai_agent",
      "category": {
        "title": "Towards Understanding Text Hallucination of Diffusion Models via Local Generation Bias",
        "authors": [
          "Rui Lu",
          "Runzhe Wang",
          "Kaifeng Lyu",
          "Xitai Jiang",
          "Gao Huang",
          "Mengdi Wang"
        ],
        "arxiv_link": "https://arxiv.org/abs/2503.03595"
      },
      "term": "text generation, diffusion model ACL 2025",
      "parsed_url": "https://arxiv.org/abs/2503.03595",
      "engines": [
        "azure_aiagent"
      ],
      "positions": [
        11
      ]
    },
    {
      "url": "https://arxiv.org/abs/2410.06014",
      "title": "How Discrete and Continuous Diffusion Meet: Comprehensive Analysis of Discrete Diffusion Models via a Stochastic Integral Framework",
      "snippet": "",
      "engine": "azure_ai_agent",
      "category": {
        "title": "How Discrete and Continuous Diffusion Meet: Comprehensive Analysis of Discrete Diffusion Models via a Stochastic Integral Framework",
        "authors": [
          "Xiang Ren",
          "Haoran Wei",
          "Pengcheng Zou"
        ],
        "arxiv_link": "https://arxiv.org/abs/2410.06014"
      },
      "term": "text generation, diffusion model ACL 2025",
      "parsed_url": "https://arxiv.org/abs/2410.06014",
      "engines": [
        "azure_aiagent"
      ],
      "positions": [
        16
      ]
    },
    {
      "url": "https://arxiv.org/abs/2410.09923",
      "title": "Scaling Diffusion Language Models via Adaptation from Autoregressive Models",
      "snippet": "",
      "engine": "azure_ai_agent",
      "category": {
        "title": "Scaling Diffusion Language Models via Adaptation from Autoregressive Models",
        "authors": [
          "Zeye Gong",
          "Zeming Nie",
          "Yilun Xu",
          "Arash Vahdat",
          "Stefano Ermon"
        ],
        "arxiv_link": "https://arxiv.org/abs/2410.09923"
      },
      "term": "text generation, diffusion model ACL 2025",
      "parsed_url": "https://arxiv.org/abs/2410.09923",
      "engines": [
        "azure_aiagent"
      ],
      "positions": [
        17
      ]
    },
    {
      "url": "https://arxiv.org/abs/2410.17678",
      "title": "Beyond Autoregression: Discrete Diffusion for Complex Reasoning and Planning",
      "snippet": "",
      "engine": "azure_ai_agent",
      "category": {
        "title": "Beyond Autoregression: Discrete Diffusion for Complex Reasoning and Planning",
        "authors": [
          "Shengyao Ye",
          "Yuankai Qi",
          "Zhiwei Xu",
          "Tomas Geffner",
          "Minkai Xu"
        ],
        "arxiv_link": "https://arxiv.org/abs/2410.17678"
      },
      "term": "text generation, diffusion model ACL 2025",
      "parsed_url": "https://arxiv.org/abs/2410.17678",
      "engines": [
        "azure_aiagent"
      ],
      "positions": [
        18
      ]
    },
    {
      "url": "https://arxiv.org/abs/2410.16563",
      "title": "Controllable Synthetic Data Generation via Diffusion Language Models",
      "snippet": "",
      "engine": "azure_ai_agent",
      "category": {
        "title": "Controllable Synthetic Data Generation via Diffusion Language Models",
        "authors": [
          "Jiawei Zhou",
          "Siyuan Dong",
          "Lu Wang",
          "Weili Nie",
          "Arash Vahdat",
          "Stefano Ermon"
        ],
        "arxiv_link": "https://arxiv.org/abs/2410.16563"
      },
      "term": "text generation, diffusion model ACL 2025",
      "parsed_url": "https://arxiv.org/abs/2410.16563",
      "engines": [
        "azure_aiagent"
      ],
      "positions": [
        19
      ]
    },
    {
      "url": "https://arxiv.org/abs/2410.15637",
      "title": "EdiText: Controllable Coarse-to-Fine Text Editing with Diffusion Language Models",
      "snippet": "",
      "engine": "azure_ai_agent",
      "category": {
        "title": "EdiText: Controllable Coarse-to-Fine Text Editing with Diffusion Language Models",
        "authors": [
          "Jooyoung Lee",
          "Xinkai Feng",
          "Haoran Wei",
          "Xiang Ren"
        ],
        "arxiv_link": "https://arxiv.org/abs/2410.15637"
      },
      "term": "text generation, diffusion model ACL 2025",
      "parsed_url": "https://arxiv.org/abs/2410.15637",
      "engines": [
        "azure_aiagent"
      ],
      "positions": [
        20
      ]
    },
    {
      "url": "https://arxiv.org/abs/2505.22165",
      "title": "Unifying Continuous and Discrete Text Diffusion with Non-simultaneous Diffusion Processes",
      "snippet": "",
      "engine": "azure_ai_agent",
      "category": {
        "title": "Unifying Continuous and Discrete Text Diffusion with Non-simultaneous Diffusion Processes",
        "authors": [
          "Bocheng Li",
          "Zhujin Gao",
          "Linli Xu"
        ],
        "arxiv_link": "https://arxiv.org/abs/2505.22165"
      },
      "term": "text generation, diffusion model EMNLP 2025",
      "parsed_url": "https://arxiv.org/abs/2505.22165",
      "engines": [
        "azure_aiagent"
      ],
      "positions": [
        22
      ]
    },
    {
      "url": "https://arxiv.org/abs/2210.08933",
      "title": "DiffuSeq: Sequence to Sequence Text Generation with Diffusion Models",
      "snippet": "",
      "engine": "azure_ai_agent",
      "category": {
        "title": "DiffuSeq: Sequence to Sequence Text Generation with Diffusion Models",
        "authors": [
          "Shansan Gong",
          "Mukai Li",
          "Jiangtao Feng",
          "Zhiyong Wu",
          "Lingpeng Kong"
        ],
        "arxiv_link": "https://arxiv.org/abs/2210.08933"
      },
      "term": "text generation, diffusion model EMNLP 2025",
      "parsed_url": "https://arxiv.org/abs/2210.08933",
      "engines": [
        "azure_aiagent"
      ],
      "positions": [
        23
      ]
    },
    {
      "url": "https://arxiv.org/abs/2212.11685",
      "title": "Text Generation with Diffusion Language Models: A Pre-training Approach with Continuous Paragraph Denoise",
      "snippet": "",
      "engine": "azure_ai_agent",
      "category": {
        "title": "Text Generation with Diffusion Language Models: A Pre-training Approach with Continuous Paragraph Denoise",
        "authors": [
          "Zhenghao Lin",
          "Yeyun Gong",
          "Yelong Shen",
          "Tong Wu",
          "Zhihao Fan",
          "Chen Lin",
          "Nan Duan",
          "Weizhu Chen"
        ],
        "arxiv_link": "https://arxiv.org/abs/2212.11685"
      },
      "term": "text generation, diffusion model EMNLP 2025",
      "parsed_url": "https://arxiv.org/abs/2212.11685",
      "engines": [
        "azure_aiagent"
      ],
      "positions": [
        24
      ]
    },
    {
      "url": "https://arxiv.org/abs/2410.15119",
      "title": "How Discrete and Continuous Diffusion Meet: Comprehensive Analysis of Discrete Diffusion Models via a Stochastic Integral Framework",
      "snippet": "",
      "engine": "azure_ai_agent",
      "category": {
        "title": "How Discrete and Continuous Diffusion Meet: Comprehensive Analysis of Discrete Diffusion Models via a Stochastic Integral Framework",
        "authors": [
          "Jiacheng Ren",
          "Jinchao Zhang",
          "Wei Wang"
        ],
        "arxiv_link": "https://arxiv.org/abs/2410.15119"
      },
      "term": "text generation, diffusion model EMNLP 2025",
      "parsed_url": "https://arxiv.org/abs/2410.15119",
      "engines": [
        "azure_aiagent"
      ],
      "positions": [
        25
      ]
    },
    {
      "url": "https://arxiv.org/abs/2410.14638",
      "title": "Beyond Autoregression: Discrete Diffusion for Complex Reasoning and Planning",
      "snippet": "",
      "engine": "azure_ai_agent",
      "category": {
        "title": "Beyond Autoregression: Discrete Diffusion for Complex Reasoning and Planning",
        "authors": [
          "Jianhuan Ye",
          "Fanghan Yang",
          "Hang Su"
        ],
        "arxiv_link": "https://arxiv.org/abs/2410.14638"
      },
      "term": "text generation, diffusion model EMNLP 2025",
      "parsed_url": "https://arxiv.org/abs/2410.14638",
      "engines": [
        "azure_aiagent"
      ],
      "positions": [
        26
      ]
    },
    {
      "url": "https://arxiv.org/abs/2410.10177",
      "title": "Scaling up Masked Diffusion Models on Text",
      "snippet": "",
      "engine": "azure_ai_agent",
      "category": {
        "title": "Scaling up Masked Diffusion Models on Text",
        "authors": [
          "Weili Nie",
          "Hanwen Zha",
          "Jonathan Ho",
          "Mingzheng Li",
          "Anima Anandkumar"
        ],
        "arxiv_link": "https://arxiv.org/abs/2410.10177"
      },
      "term": "text generation, diffusion model EMNLP 2025",
      "parsed_url": "https://arxiv.org/abs/2410.10177",
      "engines": [
        "azure_aiagent"
      ],
      "positions": [
        27
      ]
    },
    {
      "url": "https://arxiv.org/abs/2402.09798",
      "title": "Train for the Worst, Plan for the Best: Understanding Token Ordering in Masked Diffusions",
      "snippet": "",
      "engine": "azure_ai_agent",
      "category": {
        "title": "Train for the Worst, Plan for the Best: Understanding Token Ordering in Masked Diffusions",
        "authors": [
          "Yoonsung Kim",
          "Wookjeong Jeong",
          "Minjoon Seo"
        ],
        "arxiv_link": "https://arxiv.org/abs/2402.09798"
      },
      "term": "text generation, diffusion model EMNLP 2025",
      "parsed_url": "https://arxiv.org/abs/2402.09798",
      "engines": [
        "azure_aiagent"
      ],
      "positions": [
        28
      ]
    },
    {
      "url": "https://arxiv.org/abs/2410.12749",
      "title": "LLaDA 1.5: Variance-Reduced Preference Optimization for Large Language Diffusion Models",
      "snippet": "",
      "engine": "azure_ai_agent",
      "category": {
        "title": "LLaDA 1.5: Variance-Reduced Preference Optimization for Large Language Diffusion Models",
        "authors": [
          "Weili Nie",
          "Mingzheng Li",
          "Hanwen Zha",
          "Minkai Xu",
          "Anima Anandkumar"
        ],
        "arxiv_link": "https://arxiv.org/abs/2410.12749"
      },
      "term": "text generation, diffusion model EMNLP 2025",
      "parsed_url": "https://arxiv.org/abs/2410.12749",
      "engines": [
        "azure_aiagent"
      ],
      "positions": [
        29
      ]
    },
    {
      "url": "https://arxiv.org/abs/2410.16059",
      "title": "DiffLM: Controllable Synthetic Data Generation via Diffusion Language Models",
      "snippet": "",
      "engine": "azure_ai_agent",
      "category": {
        "title": "DiffLM: Controllable Synthetic Data Generation via Diffusion Language Models",
        "authors": [
          "Yichuan Zhou",
          "Leqing Liu",
          "Jingjing Xu",
          "Zhiyong Wu"
        ],
        "arxiv_link": "https://arxiv.org/abs/2410.16059"
      },
      "term": "text generation, diffusion model EMNLP 2025",
      "parsed_url": "https://arxiv.org/abs/2410.16059",
      "engines": [
        "azure_aiagent"
      ],
      "positions": [
        30
      ]
    },
    {
      "url": "https://arxiv.org/abs/2505.23606",
      "title": "Muddit: Liberating Generation Beyond Text-to-Image with a Unified Discrete Diffusion Transformer",
      "snippet": "",
      "engine": "azure_ai_agent",
      "category": {
        "title": "Muddit: Liberating Generation Beyond Text-to-Image with a Unified Discrete Diffusion Transformer",
        "authors": [
          "Chengkun Chen",
          "Han Wu",
          "Jian Yang",
          "Yong Luo"
        ],
        "arxiv_link": "https://arxiv.org/abs/2505.23606"
      },
      "term": "text generation, diffusion model NAACL 2025",
      "parsed_url": "https://arxiv.org/abs/2505.23606",
      "engines": [
        "azure_aiagent"
      ],
      "positions": [
        33
      ]
    },
    {
      "url": "https://arxiv.org/abs/2402.04485",
      "title": "EDLM: Energy-Based Diffusion Language Models for Text Generation",
      "snippet": "",
      "engine": "azure_ai_agent",
      "category": {
        "title": "EDLM: Energy-Based Diffusion Language Models for Text Generation",
        "authors": [
          "Wei Xu",
          "et al."
        ],
        "arxiv_link": "https://arxiv.org/abs/2402.04485"
      },
      "term": "text generation, diffusion model NAACL 2025",
      "parsed_url": "https://arxiv.org/abs/2402.04485",
      "engines": [
        "azure_aiagent"
      ],
      "positions": [
        34
      ]
    },
    {
      "url": "https://arxiv.org/abs/2402.07095",
      "title": "Scaling up Masked Diffusion Models on Text",
      "snippet": "",
      "engine": "azure_ai_agent",
      "category": {
        "title": "Scaling up Masked Diffusion Models on Text",
        "authors": [
          "Zheng Nie",
          "et al."
        ],
        "arxiv_link": "https://arxiv.org/abs/2402.07095"
      },
      "term": "text generation, diffusion model NAACL 2025",
      "parsed_url": "https://arxiv.org/abs/2402.07095",
      "engines": [
        "azure_aiagent"
      ],
      "positions": [
        35
      ]
    },
    {
      "url": "https://arxiv.org/abs/2402.10397",
      "title": "How Discrete and Continuous Diffusion Meet: Comprehensive Analysis of Discrete Diffusion Models via a Stochastic Integral Framework",
      "snippet": "",
      "engine": "azure_ai_agent",
      "category": {
        "title": "How Discrete and Continuous Diffusion Meet: Comprehensive Analysis of Discrete Diffusion Models via a Stochastic Integral Framework",
        "authors": [
          "Renjie Pi",
          "et al."
        ],
        "arxiv_link": "https://arxiv.org/abs/2402.10397"
      },
      "term": "text generation, diffusion model NAACL 2025",
      "parsed_url": "https://arxiv.org/abs/2402.10397",
      "engines": [
        "azure_aiagent"
      ],
      "positions": [
        36
      ]
    },
    {
      "url": "https://arxiv.org/abs/2402.11882",
      "title": "Beyond Autoregression: Discrete Diffusion for Complex Reasoning and Planning",
      "snippet": "",
      "engine": "azure_ai_agent",
      "category": {
        "title": "Beyond Autoregression: Discrete Diffusion for Complex Reasoning and Planning",
        "authors": [
          "Hongxin Ye",
          "et al."
        ],
        "arxiv_link": "https://arxiv.org/abs/2402.11882"
      },
      "term": "text generation, diffusion model NAACL 2025",
      "parsed_url": "https://arxiv.org/abs/2402.11882",
      "engines": [
        "azure_aiagent"
      ],
      "positions": [
        37
      ]
    },
    {
      "url": "https://arxiv.org/abs/2401.11708",
      "title": "DiffLM: Controllable Synthetic Data Generation via Diffusion Language Models",
      "snippet": "",
      "engine": "azure_ai_agent",
      "category": {
        "title": "DiffLM: Controllable Synthetic Data Generation via Diffusion Language Models",
        "authors": [
          "Kun Zhou",
          "et al."
        ],
        "arxiv_link": "https://arxiv.org/abs/2401.11708"
      },
      "term": "text generation, diffusion model NAACL 2025",
      "parsed_url": "https://arxiv.org/abs/2401.11708",
      "engines": [
        "azure_aiagent"
      ],
      "positions": [
        39
      ]
    },
    {
      "url": "https://arxiv.org/abs/2311.07604",
      "title": "Finetuning Text-to-Image Diffusion Models for Fairness",
      "snippet": "",
      "engine": "azure_ai_agent",
      "category": {
        "title": "Finetuning Text-to-Image Diffusion Models for Fairness",
        "authors": [
          "Xudong Shen",
          "Chao Du",
          "Tianyu Pang",
          "Min Lin",
          "Yongkang Wong",
          "Mohan Kankanhalli"
        ],
        "arxiv_link": "https://arxiv.org/abs/2311.07604"
      },
      "term": "text generation, diffusion model ICLR 2024",
      "parsed_url": "https://arxiv.org/abs/2311.07604",
      "engines": [
        "azure_aiagent"
      ],
      "positions": [
        42
      ]
    },
    {
      "url": "https://arxiv.org/abs/2310.05737",
      "title": "Language Model Beats Diffusion -- Tokenizer is Key to Visual Generation",
      "snippet": "",
      "engine": "azure_ai_agent",
      "category": {
        "title": "Language Model Beats Diffusion -- Tokenizer is Key to Visual Generation",
        "authors": [
          "Lijun Yu",
          "José Lezama",
          "Nitesh B. Gundavarapu",
          "Luca Versari",
          "Kihyuk Sohn",
          "David Minnen",
          "Yong Cheng",
          "Vighnesh Birodkar",
          "Agrim Gupta",
          "Xiuye Gu",
          "Alexander G. Hauptmann",
          "Boqing Gong",
          "Ming-Hsuan Yang",
          "Irfan Essa",
          "David A. Ross",
          "Lu Jiang"
        ],
        "arxiv_link": "https://arxiv.org/abs/2310.05737"
      },
      "term": "text generation, diffusion model ICLR 2024",
      "parsed_url": "https://arxiv.org/abs/2310.05737",
      "engines": [
        "azure_aiagent"
      ],
      "positions": [
        46
      ]
    },
    {
      "url": "https://arxiv.org/abs/2309.16779",
      "title": "Cross-Modal Contextualized Diffusion Models for Text-Guided Visual Generation and Editing (ContextDiff)",
      "snippet": "",
      "engine": "azure_ai_agent",
      "category": {
        "title": "Cross-Modal Contextualized Diffusion Models for Text-Guided Visual Generation and Editing (ContextDiff)",
        "authors": [
          "Ling Yang",
          "Zhilong Zhang",
          "Zhaochen Yu",
          "Jingwei Liu",
          "Minkai Xu",
          "Stefano Ermon",
          "Bin Cui"
        ],
        "arxiv_link": "https://arxiv.org/abs/2309.16779"
      },
      "term": "text generation, diffusion model ICLR 2024",
      "parsed_url": "https://arxiv.org/abs/2309.16779",
      "engines": [
        "azure_aiagent"
      ],
      "positions": [
        47
      ]
    },
    {
      "url": "https://proceedings.iclr.cc/paper_files/paper/2024/file/081b08068e4733ae3e7ad019fe8d172f-Paper-Conference.pdf",
      "title": "SDXL: I Latent Diffusion Models for High-Resolution Image Synthesis",
      "snippet": "",
      "engine": "azure_ai_agent",
      "category": {
        "title": "SDXL: I Latent Diffusion Models for High-Resolution Image Synthesis",
        "authors": [
          "Robin Rombach",
          "Andreas Blattmann",
          "Dominik Lorenz",
          "Patrick Esser",
          "Björn Ommer",
          "others"
        ],
        "arxiv_link": "https://proceedings.iclr.cc/paper_files/paper/2024/file/081b08068e4733ae3e7ad019fe8d172f-Paper-Conference.pdf"
      },
      "term": "text generation, diffusion model ICLR 2024",
      "parsed_url": "https://proceedings.iclr.cc/paper_files/paper/2024/file/081b08068e4733ae3e7ad019fe8d172f-Paper-Conference.pdf",
      "engines": [
        "azure_aiagent"
      ],
      "positions": [
        48
      ]
    },
    {
      "url": "https://arxiv.org/abs/2403.07128",
      "title": "GeoDiffusion: Text-Prompted Geometric Control for Object Detection Data Generation",
      "snippet": "",
      "engine": "azure_ai_agent",
      "category": {
        "title": "GeoDiffusion: Text-Prompted Geometric Control for Object Detection Data Generation",
        "authors": [
          "Kai Chen",
          "Enze Xie",
          "Zhe Chen",
          "Yibo Wang",
          "Lanqing Hong",
          "Zhenguo Li",
          "Dit-Yan Yeung"
        ],
        "arxiv_link": "https://arxiv.org/abs/2403.07128"
      },
      "term": "text generation, diffusion model ICLR 2024",
      "parsed_url": "https://arxiv.org/abs/2403.07128",
      "engines": [
        "azure_aiagent"
      ],
      "positions": [
        49
      ]
    },
    {
      "url": "https://arxiv.org/abs/2507.07079",
      "title": "Evaluating Attribute Confusion in Fashion Text-to-Image Generation",
      "snippet": "",
      "engine": "azure_ai_agent",
      "category": {
        "title": "Evaluating Attribute Confusion in Fashion Text-to-Image Generation",
        "authors": [
          "Jinghe Zhang",
          "Nipun Batra",
          "Varun Chandola"
        ],
        "arxiv_link": "https://arxiv.org/abs/2507.07079"
      },
      "term": "text generation, diffusion model ICLR 2024",
      "parsed_url": "https://arxiv.org/abs/2507.07079",
      "engines": [
        "azure_aiagent"
      ],
      "positions": [
        50
      ]
    },
    {
      "url": "https://arxiv.org/abs/2310.16834",
      "title": "Discrete Diffusion Modeling by Estimating the Ratios of the Data Distribution",
      "snippet": "",
      "engine": "azure_ai_agent",
      "category": {
        "title": "Discrete Diffusion Modeling by Estimating the Ratios of the Data Distribution",
        "authors": [
          "Aaron Lou",
          "Chenlin Meng",
          "Stefano Ermon"
        ],
        "arxiv_link": "https://arxiv.org/abs/2310.16834"
      },
      "term": "text generation, diffusion model ICML 2024",
      "parsed_url": "https://arxiv.org/abs/2310.16834",
      "engines": [
        "azure_aiagent"
      ],
      "positions": [
        52
      ]
    },
    {
      "url": "https://aclanthology.org/2024.naacl-long.261/",
      "title": "Empowering Diffusion Models on the Embedding Space for Text Generation",
      "snippet": "",
      "engine": "azure_ai_agent",
      "category": {
        "title": "Empowering Diffusion Models on the Embedding Space for Text Generation",
        "authors": [
          "Zhujin Gao",
          "Junliang Guo",
          "Xu Tan",
          "Yongxin Zhu",
          "Fang Zhang",
          "Jiang Bian",
          "Linli Xu"
        ],
        "arxiv_link": "https://aclanthology.org/2024.naacl-long.261/"
      },
      "term": "text generation, diffusion model ICML 2024",
      "parsed_url": "https://aclanthology.org/2024.naacl-long.261/",
      "engines": [
        "azure_aiagent"
      ],
      "positions": [
        53
      ]
    },
    {
      "url": "https://arxiv.org/abs/2408.04220",
      "title": "Diffusion Guided Language Modeling",
      "snippet": "",
      "engine": "azure_ai_agent",
      "category": {
        "title": "Diffusion Guided Language Modeling",
        "authors": [
          "Justin Lovelace",
          "Varsha Kishore",
          "Yiwei Chen",
          "Kilian Q. Weinberger"
        ],
        "arxiv_link": "https://arxiv.org/abs/2408.04220"
      },
      "term": "text generation, diffusion model ICML 2024",
      "parsed_url": "https://arxiv.org/abs/2408.04220",
      "engines": [
        "azure_aiagent"
      ],
      "positions": [
        54
      ]
    },
    {
      "url": "https://arxiv.org/abs/2405.08246",
      "title": "Compositional Text-to-Image Generation with Dense Blob Representations",
      "snippet": "",
      "engine": "azure_ai_agent",
      "category": {
        "title": "Compositional Text-to-Image Generation with Dense Blob Representations",
        "authors": [
          "Weili Nie",
          "Sifei Liu",
          "Morteza Mardani",
          "Chao Liu",
          "Benjamin Eckart",
          "Arash Vahdat"
        ],
        "arxiv_link": "https://arxiv.org/abs/2405.08246"
      },
      "term": "text generation, diffusion model ICML 2024",
      "parsed_url": "https://arxiv.org/abs/2405.08246",
      "engines": [
        "azure_aiagent"
      ],
      "positions": [
        55
      ]
    },
    {
      "url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Li_On_the_Scalability_of_Diffusion-based_Text-to-Image_Generation_CVPR_2024_paper.pdf",
      "title": "On the Scalability of Diffusion-based Text-to-Image Generation",
      "snippet": "",
      "engine": "azure_ai_agent",
      "category": {
        "title": "On the Scalability of Diffusion-based Text-to-Image Generation",
        "authors": [
          "Ao Li",
          "Kaihua Tang",
          "Jing Liao"
        ],
        "arxiv_link": "https://openaccess.thecvf.com/content/CVPR2024/papers/Li_On_the_Scalability_of_Diffusion-based_Text-to-Image_Generation_CVPR_2024_paper.pdf"
      },
      "term": "text generation, diffusion model ICML 2024",
      "parsed_url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Li_On_the_Scalability_of_Diffusion-based_Text-to-Image_Generation_CVPR_2024_paper.pdf",
      "engines": [
        "azure_aiagent"
      ],
      "positions": [
        56
      ]
    },
    {
      "url": "https://peerj.com/articles/cs-1905.pdf",
      "title": "Diffusion models in text generation: a survey",
      "snippet": "",
      "engine": "azure_ai_agent",
      "category": {
        "title": "Diffusion models in text generation: a survey",
        "authors": [
          "Bohan Li",
          "Yi Li",
          "Youzheng Wu",
          "Wenjing Yang",
          "Qun Liu"
        ],
        "arxiv_link": "https://peerj.com/articles/cs-1905.pdf"
      },
      "term": "text generation, diffusion model ICML 2024",
      "parsed_url": "https://peerj.com/articles/cs-1905.pdf",
      "engines": [
        "azure_aiagent"
      ],
      "positions": [
        57
      ]
    },
    {
      "url": "https://blog.genlaw.org/pdfs/genlaw_icml2024/42.pdf",
      "title": "Diffusion Unlearning Optimization for Robust and Safe Text-to-Image Models",
      "snippet": "",
      "engine": "azure_ai_agent",
      "category": {
        "title": "Diffusion Unlearning Optimization for Robust and Safe Text-to-Image Models",
        "authors": [
          "Anonymous (ICML 2024 authors, see ICML GenLaw Workshop)"
        ],
        "arxiv_link": "https://blog.genlaw.org/pdfs/genlaw_icml2024/42.pdf"
      },
      "term": "text generation, diffusion model ICML 2024",
      "parsed_url": "https://blog.genlaw.org/pdfs/genlaw_icml2024/42.pdf",
      "engines": [
        "azure_aiagent"
      ],
      "positions": [
        58
      ]
    },
    {
      "url": "https://arxiv.org/pdf/2403.04279",
      "title": "Controllable Generation with Text-to-Image Diffusion Models: A Survey",
      "snippet": "",
      "engine": "azure_ai_agent",
      "category": {
        "title": "Controllable Generation with Text-to-Image Diffusion Models: A Survey",
        "authors": [
          "Hongjin Zhang",
          "Junfeng Wu",
          "Yinghao Xu",
          "Chuanxia Zheng",
          "Xiaoguang Han",
          "Yizhou Yu"
        ],
        "arxiv_link": "https://arxiv.org/pdf/2403.04279"
      },
      "term": "text generation, diffusion model ICML 2024",
      "parsed_url": "https://arxiv.org/pdf/2403.04279",
      "engines": [
        "azure_aiagent"
      ],
      "positions": [
        59
      ]
    },
    {
      "url": "https://arxiv.org/abs/2402.13040",
      "title": "Text-Guided Molecule Generation with Diffusion Language Model",
      "snippet": "",
      "engine": "azure_ai_agent",
      "category": {
        "title": "Text-Guided Molecule Generation with Diffusion Language Model",
        "authors": [
          "Yuxuan Zhang",
          "Zeming Lin",
          "Ziang Liu",
          "Hongyu Chen",
          "Linfeng Zhang",
          "Weinan Zhang",
          "Yong Yu"
        ],
        "arxiv_link": "https://arxiv.org/abs/2402.13040"
      },
      "term": "text generation, diffusion model ICML 2024",
      "parsed_url": "https://arxiv.org/abs/2402.13040",
      "engines": [
        "azure_aiagent"
      ],
      "positions": [
        60
      ]
    },
    {
      "url": "https://arxiv.org/abs/2406.10951",
      "title": "Text-DiFuse: An Interactive Multi-Modal Image Fusion Framework based on Text-modulated Diffusion Model",
      "snippet": "",
      "engine": "azure_ai_agent",
      "category": {
        "title": "Text-DiFuse: An Interactive Multi-Modal Image Fusion Framework based on Text-modulated Diffusion Model",
        "authors": [
          "Hao Zhang",
          "Lei Cao",
          "Jiayi Ma"
        ],
        "arxiv_link": "https://arxiv.org/abs/2406.10951"
      },
      "term": "text generation, diffusion model NeurIPS 2024",
      "parsed_url": "https://arxiv.org/abs/2406.10951",
      "engines": [
        "azure_aiagent"
      ],
      "positions": [
        61
      ]
    },
    {
      "url": "https://arxiv.org/abs/2404.11373",
      "title": "Empowering Diffusion Models on the Embedding Space for Text Generation",
      "snippet": "",
      "engine": "azure_ai_agent",
      "category": {
        "title": "Empowering Diffusion Models on the Embedding Space for Text Generation",
        "authors": [
          "Zhujin Gao",
          "Junliang Guo",
          "Xu Tan",
          "Yongxin Zhu",
          "Fang Zhang",
          "Jiang Bian",
          "Linli Xu"
        ],
        "arxiv_link": "https://arxiv.org/abs/2404.11373"
      },
      "term": "text generation, diffusion model NeurIPS 2024",
      "parsed_url": "https://arxiv.org/abs/2404.11373",
      "engines": [
        "azure_aiagent"
      ],
      "positions": [
        63
      ]
    },
    {
      "url": "https://arxiv.org/abs/2412.09491",
      "title": "Meta-DiffuB: A Contextualized Sequence-to-Sequence Text Diffusion Model with Meta-Exploration",
      "snippet": "",
      "engine": "azure_ai_agent",
      "category": {
        "title": "Meta-DiffuB: A Contextualized Sequence-to-Sequence Text Diffusion Model with Meta-Exploration",
        "authors": [
          "Hao Chuang",
          "et al."
        ],
        "arxiv_link": "https://arxiv.org/abs/2412.09491"
      },
      "term": "text generation, diffusion model NeurIPS 2024",
      "parsed_url": "https://arxiv.org/abs/2412.09491",
      "engines": [
        "azure_aiagent"
      ],
      "positions": [
        65
      ]
    },
    {
      "url": "https://arxiv.org/abs/2403.12473",
      "title": "Diffuse, Retrieve or Adapt: A Comparative Study of Pre-trained Models for Diffusion-based Text Generation",
      "snippet": "",
      "engine": "azure_ai_agent",
      "category": {
        "title": "Diffuse, Retrieve or Adapt: A Comparative Study of Pre-trained Models for Diffusion-based Text Generation",
        "authors": [
          "Yiming Zhang",
          "et al."
        ],
        "arxiv_link": "https://arxiv.org/abs/2403.12473"
      },
      "term": "text generation, diffusion model NeurIPS 2024",
      "parsed_url": "https://arxiv.org/abs/2403.12473",
      "engines": [
        "azure_aiagent"
      ],
      "positions": [
        66
      ]
    },
    {
      "url": "https://arxiv.org/abs/2410.10244",
      "title": "Scaling Diffusion Language Models via Adaptation from Autoregressive Models",
      "snippet": "",
      "engine": "azure_ai_agent",
      "category": {
        "title": "Scaling Diffusion Language Models via Adaptation from Autoregressive Models",
        "authors": [
          "Haoyu Gong",
          "et al."
        ],
        "arxiv_link": "https://arxiv.org/abs/2410.10244"
      },
      "term": "text generation, diffusion model NeurIPS 2024",
      "parsed_url": "https://arxiv.org/abs/2410.10244",
      "engines": [
        "azure_aiagent"
      ],
      "positions": [
        67
      ]
    },
    {
      "url": "https://arxiv.org/abs/2410.05523",
      "title": "How Discrete and Continuous Diffusion Meet: Comprehensive Analysis of Discrete Diffusion Models via a Stochastic Integral Framework",
      "snippet": "",
      "engine": "azure_ai_agent",
      "category": {
        "title": "How Discrete and Continuous Diffusion Meet: Comprehensive Analysis of Discrete Diffusion Models via a Stochastic Integral Framework",
        "authors": [
          "Ruizhi Ren",
          "et al."
        ],
        "arxiv_link": "https://arxiv.org/abs/2410.05523"
      },
      "term": "text generation, diffusion model NeurIPS 2024",
      "parsed_url": "https://arxiv.org/abs/2410.05523",
      "engines": [
        "azure_aiagent"
      ],
      "positions": [
        68
      ]
    },
    {
      "url": "https://arxiv.org/abs/2410.07482",
      "title": "Beyond Autoregression: Discrete Diffusion for Complex Reasoning and Planning",
      "snippet": "",
      "engine": "azure_ai_agent",
      "category": {
        "title": "Beyond Autoregression: Discrete Diffusion for Complex Reasoning and Planning",
        "authors": [
          "Minye Ye",
          "et al."
        ],
        "arxiv_link": "https://arxiv.org/abs/2410.07482"
      },
      "term": "text generation, diffusion model NeurIPS 2024",
      "parsed_url": "https://arxiv.org/abs/2410.07482",
      "engines": [
        "azure_aiagent"
      ],
      "positions": [
        69
      ]
    },
    {
      "url": "https://aclanthology.org/2024.naacl-long.261.pdf",
      "title": "Empowering Diffusion Models on the Embedding Space for Text Generation",
      "snippet": "",
      "engine": "azure_ai_agent",
      "category": {
        "title": "Empowering Diffusion Models on the Embedding Space for Text Generation",
        "authors": [
          "Zhujin Gao",
          "Junliang Guo",
          "Xu Tan",
          "Yongxin Zhu",
          "Fang Zhang",
          "Jiang Bian",
          "Linli Xu"
        ],
        "arxiv_link": "https://aclanthology.org/2024.naacl-long.261.pdf"
      },
      "term": "text generation, diffusion model ACL 2024",
      "parsed_url": "https://aclanthology.org/2024.naacl-long.261.pdf",
      "engines": [
        "azure_aiagent"
      ],
      "positions": [
        72
      ]
    },
    {
      "url": "https://aclanthology.org/2024.eacl-long.86.pdf",
      "title": "Diffusion-NAT: Self-Prompting Discrete Diffusion for Non-Autoregressive Text Generation",
      "snippet": "",
      "engine": "azure_ai_agent",
      "category": {
        "title": "Diffusion-NAT: Self-Prompting Discrete Diffusion for Non-Autoregressive Text Generation",
        "authors": [
          "Kun Zhou",
          "Yifan Li",
          "Wayne Xin Zhao",
          "Ji-Rong Wen"
        ],
        "arxiv_link": "https://aclanthology.org/2024.eacl-long.86.pdf"
      },
      "term": "text generation, diffusion model ACL 2024",
      "parsed_url": "https://aclanthology.org/2024.eacl-long.86.pdf",
      "engines": [
        "azure_aiagent"
      ],
      "positions": [
        74
      ]
    },
    {
      "url": "https://proceedings.neurips.cc/paper_files/paper/2023/file/7d866abba506e5a56335e4644ebe18f9-Paper-Conference.pdf",
      "title": "AR-D : Auto-Regressive Diffusion Model for Text Generation",
      "snippet": "",
      "engine": "azure_ai_agent",
      "category": {
        "title": "AR-D : Auto-Regressive Diffusion Model for Text Generation",
        "authors": [
          "Baoyuan Wu",
          "Ziliang Che",
          "Wayne Xin Zhao",
          "Ji-Rong Wen"
        ],
        "arxiv_link": "https://proceedings.neurips.cc/paper_files/paper/2023/file/7d866abba506e5a56335e4644ebe18f9-Paper-Conference.pdf"
      },
      "term": "text generation, diffusion model ACL 2024",
      "parsed_url": "https://proceedings.neurips.cc/paper_files/paper/2023/file/7d866abba506e5a56335e4644ebe18f9-Paper-Conference.pdf",
      "engines": [
        "azure_aiagent"
      ],
      "positions": [
        76
      ]
    },
    {
      "url": "https://aclanthology.org/2023.findings-acl.721.pdf",
      "title": "Can Diffusion Model Achieve Better Performance in Text Generation?",
      "snippet": "",
      "engine": "azure_ai_agent",
      "category": {
        "title": "Can Diffusion Model Achieve Better Performance in Text Generation?",
        "authors": [
          "Zhujin Gao",
          "Junliang Guo",
          "Xu Tan",
          "Yongxin Zhu",
          "Fang Zhang",
          "Jiang Bian",
          "Linli Xu"
        ],
        "arxiv_link": "https://aclanthology.org/2023.findings-acl.721.pdf"
      },
      "term": "text generation, diffusion model ACL 2024",
      "parsed_url": "https://aclanthology.org/2023.findings-acl.721.pdf",
      "engines": [
        "azure_aiagent"
      ],
      "positions": [
        77
      ]
    },
    {
      "url": "https://arxiv.org/abs/2305.10855",
      "title": "TextDiffuser: Diffusion Models as Text Painters",
      "snippet": "",
      "engine": "azure_ai_agent",
      "category": {
        "title": "TextDiffuser: Diffusion Models as Text Painters",
        "authors": [
          "Jun Tian",
          "Zhaofeng Li",
          "Chenglu Zhu",
          "Feng Gao",
          "Hang Su",
          "Jun Zhu"
        ],
        "arxiv_link": "https://arxiv.org/abs/2305.10855"
      },
      "term": "text generation, diffusion model ACL 2024",
      "parsed_url": "https://arxiv.org/abs/2305.10855",
      "engines": [
        "azure_aiagent"
      ],
      "positions": [
        78
      ]
    },
    {
      "url": "https://arxiv.org/abs/2209.12033",
      "title": "Self-conditioned Embedding Diffusion for Text Generation",
      "snippet": "",
      "engine": "azure_ai_agent",
      "category": {
        "title": "Self-conditioned Embedding Diffusion for Text Generation",
        "authors": [
          "Robin Strudel",
          "Olivier Delalleau",
          "Karim Beguir"
        ],
        "arxiv_link": "https://arxiv.org/abs/2209.12033"
      },
      "term": "text generation, diffusion model ACL 2024",
      "parsed_url": "https://arxiv.org/abs/2209.12033",
      "engines": [
        "azure_aiagent"
      ],
      "positions": [
        80
      ]
    }
  ],
  "sources": {
    "https://iclr.cc/virtual/2025/poster/29366": "SNIPPET: Masked diffusion models (MDMs) have shown promise in language modeling, yet their scalability and effectiveness in core language tasks, such as text generation and language understanding, remain underexplored. This paper establishes the first scaling law for MDMs, demonstrating a scaling rate comparable to autoregressive models (ARMs) and a relatively small compute gap. Motivated by their ...\n\nTITLE: Main Navigation\n\nBODY:\nPoster\nScaling up Masked Diffusion Models on Text\nShen Nie · Fengqi Zhu · Chao Du · Tianyu Pang · Qian Liu · Guangtao Zeng · Min Lin · Chongxuan Li\nMasked diffusion models (MDMs) have shown promise in language modeling, yet their scalability and effectiveness in core language tasks, such as text generation and language understanding, remain underexplored. This paper establishes the first scaling law for MDMs, demonstrating a scaling rate comparable to autoregressive models (ARMs) and a relatively small compute gap. Motivated by their scalability, we train a family of MDMs with up to 1.1 billion (B) parameters to systematically evaluate their performance against ARMs of comparable or larger sizes. Fully leveraging the probabilistic formulation of MDMs, we propose a simple yet effective unsupervised classifier-free guidance that effectively exploits large-scale unpaired data, boosting performance for conditional inference. In language understanding, the 1.1B MDM outperforms the 1.1B TinyLlama model trained on the same data across four of eight zero-shot benchmarks. Notably, it achieves competitive math reasoning ability with the 7B Llama-2 model on the GSM8K dataset. In text generation, MDMs with 16 times more pre-training time offer a flexible trade-off against ARMs with the accelerated sampling technique KV-Cache: MDMs match ARMs in performance while being 1.4 times faster during sampling.Moreover, MDMs address challenging tasks for ARMs by effectively handling bidirectional reasoning and adapting to temporal shifts in data. Notably, a 1.1B MDM breaks the reverse curse encountered by much larger ARMs with significantly more data and computation, such as 13B Llama-2 and 175B GPT-3. Our code is available at https://github.com/ML-GSAI/SMDM.\n\nSOURCE: https://iclr.cc/virtual/2025/poster/29366",
    "https://iclr.cc/virtual/2025/poster/29614": "SNIPPET: Score-based diffusion models have achieved incredible performance in generating realistic images, audio, and video data. While these models produce high-quality samples with impressive details, they often introduce unrealistic artifacts, such as distorted fingers or hallucinated texts with no meaning. This paper focuses on textual hallucinations, where diffusion models correctly generate ...\n\nTITLE: Main Navigation\n\nBODY:\nPoster\nTowards Understanding Text Hallucination of Diffusion Models via Local Generation Bias\nRui Lu · Runzhe Wang · Kaifeng Lyu · Xitai Jiang · Gao Huang · Mengdi Wang\nScore-based diffusion models have achieved incredible performance in generating realistic images, audio, and video data. While these models produce high-quality samples with impressive details, they often introduce unrealistic artifacts, such as distorted fingers or hallucinated texts with no meaning. This paper focuses on textual hallucinations, where diffusion models correctly generate individual symbols but assemble them in a nonsensical manner. Through experimental probing, we consistently observe that such phenomenon is attributed it to the network's local generation bias. Denoising networks tend to produce outputs that rely heavily on highly correlated local regions, particularly when different dimensions of the data distribution are nearly pairwise independent. This behavior leads to a generation process that decomposes the global distribution into separate, independent distributions for each symbol, ultimately failing to capture the global structure, including underlying grammar. Intriguingly, this bias persists across various denoising network architectures including MLP and transformers which have the structure to model global dependency. These findings also provide insights into understanding other types of hallucinations, extending beyond text, as a result of implicit biases in the denoising models. Additionally, we theoretically analyze the training dynamics for a specific case involving a two-layer MLP learning parity points on a hypercube, offering an explanation of its underlying mechanism.\n\nSOURCE: https://iclr.cc/virtual/2025/poster/29614",
    "https://arxiv.org/abs/2503.03595": "SNIPPET: Mar 5, 2025 ... arXiv:2503.03595 [cs.LG]. (or arXiv:2503.03595v1 [cs.LG] for this version). https://doi.org/10.48550/arXiv.2503.03595. Focus to learn more.\n\nTITLE: Towards Understanding Text Hallucination of Diffusion Models via Local Generation Bias\n\nBODY:\nComputer Science > Machine Learning\n[Submitted on 5 Mar 2025]\nTitle:Towards Understanding Text Hallucination of Diffusion Models via Local Generation Bias\nView PDF HTML (experimental)Abstract:Score-based diffusion models have achieved incredible performance in generating realistic images, audio, and video data. While these models produce high-quality samples with impressive details, they often introduce unrealistic artifacts, such as distorted fingers or hallucinated texts with no meaning. This paper focuses on textual hallucinations, where diffusion models correctly generate individual symbols but assemble them in a nonsensical manner. Through experimental probing, we consistently observe that such phenomenon is attributed it to the network's local generation bias. Denoising networks tend to produce outputs that rely heavily on highly correlated local regions, particularly when different dimensions of the data distribution are nearly pairwise independent. This behavior leads to a generation process that decomposes the global distribution into separate, independent distributions for each symbol, ultimately failing to capture the global structure, including underlying grammar. Intriguingly, this bias persists across various denoising network architectures including MLP and transformers which have the structure to model global dependency. These findings also provide insights into understanding other types of hallucinations, extending beyond text, as a result of implicit biases in the denoising models. Additionally, we theoretically analyze the training dynamics for a specific case involving a two-layer MLP learning parity points on a hypercube, offering an explanation of its underlying mechanism.\nReferences & Citations\nexport BibTeX citation\nLoading...\nBibliographic and Citation Tools\nBibliographic Explorer (What is the Explorer?)\nConnected Papers (What is Connected Papers?)\nLitmaps (What is Litmaps?)\nscite Smart Citations (What are Smart Citations?)\nCode, Data and Media Associated with this Article\nalphaXiv (What is alphaXiv?)\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\nDagsHub (What is DagsHub?)\nGotit.pub (What is GotitPub?)\nHugging Face (What is Huggingface?)\nPapers with Code (What is Papers with Code?)\nScienceCast (What is ScienceCast?)\nDemos\nRecommenders and Search Tools\nInfluence Flower (What are Influence Flowers?)\nCORE Recommender (What is CORE?)\nIArxiv Recommender\n(What is IArxiv?)\narXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.\n\nSOURCE: https://arxiv.org/abs/2503.03595",
    "https://openreview.net/forum?id=l2zFn6TIQi": "SNIPPET: OpenReview.net · Login · back arrow Go to ICLR 2025 Conference homepage ... Submission Guidelines: I certify that this submission complies with the submission ...\n\nTITLE: Controlling Language and Diffusion Models by Transporting Activations\n\nBODY:\nKeywords: controllability, generative models, toxicity, diffusion\nTL;DR: We propose an inference-time intervention framework based on Optimal Transport that generalizes previous methods and allows interpretable control of both LLMs and Diffusion models.\nAbstract: The increasing capabilities of large generative models and their ever more widespread deployment have raised concerns about their reliability, safety, and potential misuse. To address these issues, recent works have proposed to control model generation by steering model activations in order to effectively induce or prevent the emergence of concepts or behaviors in the generated output.\nIn this paper we introduce Activation Transport (AcT), a general framework to steer activations guided by optimal transport theory that generalizes many previous activation-steering works. AcT is modality-agnostic and provides fine-grained control over the model behavior with negligible computational overhead, while minimally impacting model abilities. We experimentally show the effectiveness and versatility of our approach by addressing key challenges in large language models (LLMs) and text-to-image diffusion models (T2Is). For LLMs, we show that AcT can effectively mitigate toxicity, induce arbitrary concepts, and increase their truthfulness. In T2Is, we show how AcT enables fine-grained style control and concept negation.\nPrimary Area: interpretability and explainable AI\nCode Of Ethics: I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.\nSubmission Guidelines: I certify that this submission complies with the submission instructions as described on https://iclr.cc/Conferences/2025/AuthorGuide.\nAnonymous Url: I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.\nNo Acknowledgement Section: I certify that there is no acknowledgement section in this submission for double blind review.\nSubmission Number: 7460\nLoading\n\nSOURCE: https://openreview.net/forum?id=l2zFn6TIQi",
    "https://ai-ml.cn/iclr25": "SNIPPET: ... artificial intelligence, statistics and data science, as well as important application areas such as machine ... representation learning for computer vision ...\n\nTITLE: MSRA Shanghai AI/ML\n\nBODY:\nSeven papers accepted by ICLR 2025!\nIn this year’s ICLR conference, our team members have published seven papers in diffusion models, foundation models and LLM. Several team members will attend the conference in person and welcome everyone to stop by their posters for discussions.\nReference\n-\nHow Do Large Language Models Understand Graph Patterns? A Benchmark for Graph Pattern Comprehension\nXinnan Dai, Haohao Qu, Yifei Shen, Bohang Zhang, Qihao Wen, Wenqi Fan, Dongsheng Li, Jiliang Tang, Caihua Shan.\nThe Thirteenth International Conference on Learning Representations (ICLR). 2025\n-\nSharedContextBench: How Lossy are Long-context Methods in KV Cache Reuse\nYUCHENG LI, Huiqiang Jiang, Qianhui Wu, Xufang Luo, Surin Ahn, Chengruidong Zhang, Amir H. Abdi, Dongsheng Li, Jianfeng Gao, Yuqing Yang, Lili Qiu.\nThe Thirteenth International Conference on Learning Representations (ICLR). 2025\n-\nSeCom: On Memory Construction and Retrieval for Personalized Conversational Agents\nZhuoshi Pan, Qianhui Wu, Huiqiang Jiang, Xufang Luo, Hao Cheng, Dongsheng Li, Yuqing Yang, Chin-Yew Lin, H. Vicky Zhao, Lili Qiu, Jianfeng Gao.\nThe Thirteenth International Conference on Learning Representations (ICLR). 2025\n-\nNeuroLM: A Universal Multi-task Foundation Model for Bridging the Gap between Language and EEG Signals\nWeibang Jiang, Yansen Wang, Bao-liang Lu, Dongsheng Li.\nThe Thirteenth International Conference on Learning Representations (ICLR). 2025\n-\npMoE: Prompting Diverse Experts Together Wins More in Visual Adaptation\nShentong Mo, Xufang Luo, Dongsheng Li.\nThe Thirteenth International Conference on Learning Representations (ICLR). 2025\n-\nWhat Makes a Good Diffusion Planner for Decision Making?\nHaofei Lu, Dongqi Han, Yifei Shen, Dongsheng Li.\nThe Thirteenth International Conference on Learning Representations (ICLR). 2025\n-\nDreamDistribution: Prompt Distribution Learning for Text-to-Image Diffusion Models\nBrian Nlong Zhao, Yuhang Xiao, Jiashu Xu, XINYANG JIANG, Yifan Yang, Dongsheng Li, Laurent Itti, Vibhav Vineet, Yunhao Ge.\nThe Thirteenth International Conference on Learning Representations (ICLR). 2025\n\nSOURCE: https://ai-ml.cn/iclr25",
    "https://openreview.net/forum?id=tyEyYT267x": "SNIPPET: OpenReview.net · Login. ×. ×. BibTeX Record. Click anywhere on the box above to highlight complete record. Done. back arrow Go to ICLR 2025 Conference homepage ...\n\nTITLE: Block Diffusion: Interpolating Between Autoregressive and Diffusion...\n\nBODY:\nKeywords: Diffusion Models, Text Diffusion, Generative Models\nAbstract: Diffusion language models offer unique benefits over autoregressive models due to their potential for parallelized generation and controllability, yet they lag in likelihood modeling and are limited to fixed-length generation. In this work, we introduce a class of block diffusion language models that interpolate between discrete denoising diffusion and autoregressive models. Block diffusion overcomes key limitations of both approaches by supporting flexible-length generation and improving inference efficiency with KV caching and parallel token sampling. We propose a recipe for building effective block diffusion models that includes an efficient training algorithm, estimators of gradient variance, and data-driven noise schedules to minimize the variance. Block diffusion sets a new state-of-the-art performance among diffusion models on language modeling benchmarks and enables generation of arbitrary-length sequences. We provide the code, along with the model weights and blog post on the project page: https://m-arriola.com/bd3lms/\nPrimary Area: generative models\nCode Of Ethics: I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.\nSubmission Guidelines: I certify that this submission complies with the submission instructions as described on https://iclr.cc/Conferences/2025/AuthorGuide.\nAnonymous Url: I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.\nNo Acknowledgement Section: I certify that there is no acknowledgement section in this submission for double blind review.\nSubmission Number: 12566\nLoading\n\nSOURCE: https://openreview.net/forum?id=tyEyYT267x",
    "https://arxiv.org/abs/2303.06574": "SNIPPET: Mar 12, 2023 ... arXiv:2303.06574 [cs.CL]. (or arXiv:2303.06574v2 [cs.CL] for this version). https://doi.org/10.48550/arXiv.2303.06574. Focus to learn more.\n\nTITLE: Diffusion Models for Non-autoregressive Text Generation: A Survey\n\nBODY:\nComputer Science > Computation and Language\n[Submitted on 12 Mar 2023 (v1), last revised 13 May 2023 (this version, v2)]\nTitle:Diffusion Models for Non-autoregressive Text Generation: A Survey\nView PDFAbstract:Non-autoregressive (NAR) text generation has attracted much attention in the field of natural language processing, which greatly reduces the inference latency but has to sacrifice the generation accuracy. Recently, diffusion models, a class of latent variable generative models, have been introduced into NAR text generation, showing an improved text generation quality. In this survey, we review the recent progress in diffusion models for NAR text generation. As the background, we first present the general definition of diffusion models and the text diffusion models, and then discuss their merits for NAR generation. As the core content, we further introduce two mainstream diffusion models in existing work of text diffusion, and review the key designs of the diffusion process. Moreover, we discuss the utilization of pre-trained language models (PLMs) for text diffusion models and introduce optimization techniques for text data. Finally, we discuss several promising directions and conclude this paper. Our survey aims to provide researchers with a systematic reference of related research on text diffusion models for NAR generation. We present our collection of text diffusion models at this https URL.\nSubmission history\nFrom: Yifan Li [view email][v1] Sun, 12 Mar 2023 05:11:09 UTC (356 KB)\n[v2] Sat, 13 May 2023 12:42:49 UTC (539 KB)\nReferences & Citations\nexport BibTeX citation\nLoading...\nBibliographic and Citation Tools\nBibliographic Explorer (What is the Explorer?)\nConnected Papers (What is Connected Papers?)\nLitmaps (What is Litmaps?)\nscite Smart Citations (What are Smart Citations?)\nCode, Data and Media Associated with this Article\nalphaXiv (What is alphaXiv?)\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\nDagsHub (What is DagsHub?)\nGotit.pub (What is GotitPub?)\nHugging Face (What is Huggingface?)\nPapers with Code (What is Papers with Code?)\nScienceCast (What is ScienceCast?)\nDemos\nRecommenders and Search Tools\nInfluence Flower (What are Influence Flowers?)\nCORE Recommender (What is CORE?)\narXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.\n\nSOURCE: https://arxiv.org/abs/2303.06574",
    "https://arxiv.org/abs/2306.08236": "SNIPPET: Jun 14, 2023 ... Information sharing through fake screenshots can be highly responsible for misinformation and disinformation spread on social media. Our ...\n\nTITLE: Extracting Information from Twitter Screenshots\n\nBODY:\nComputer Science > Information Retrieval\n[Submitted on 14 Jun 2023]\nTitle:Extracting Information from Twitter Screenshots\nView PDFAbstract:Screenshots are prevalent on social media as a common approach for information sharing. Users rarely verify before sharing a screenshot whether the post it contains is fake or real. Information sharing through fake screenshots can be highly responsible for misinformation and disinformation spread on social media. Our ultimate goal is to develop a tool that could take a screenshot of a tweet and provide a probability that the tweet is real, using resources found on the live web and in web archives. This paper provides methods for extracting the tweet text, timestamp, and Twitter handle from a screenshot of a tweet.\nReferences & Citations\nexport BibTeX citation\nLoading...\nBibliographic and Citation Tools\nBibliographic Explorer (What is the Explorer?)\nConnected Papers (What is Connected Papers?)\nLitmaps (What is Litmaps?)\nscite Smart Citations (What are Smart Citations?)\nCode, Data and Media Associated with this Article\nalphaXiv (What is alphaXiv?)\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\nDagsHub (What is DagsHub?)\nGotit.pub (What is GotitPub?)\nHugging Face (What is Huggingface?)\nPapers with Code (What is Papers with Code?)\nScienceCast (What is ScienceCast?)\nDemos\nRecommenders and Search Tools\nInfluence Flower (What are Influence Flowers?)\nCORE Recommender (What is CORE?)\narXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.\n\nSOURCE: https://arxiv.org/abs/2306.08236",
    "https://arxiv.org/abs/2409.13031": "SNIPPET: Sep 19, 2024 ... arXiv:2409.13031 [astro-ph.IM]. (or arXiv:2409.13031v1 [astro-ph.IM] for this version). https://doi.org/10.48550/arXiv.2409.13031. Focus to ...\n\nTITLE: Combining statistical learning with deep learning for improved exoplanet detection and characterization\n\nBODY:\nAstrophysics > Instrumentation and Methods for Astrophysics\n[Submitted on 19 Sep 2024]\nTitle:Combining statistical learning with deep learning for improved exoplanet detection and characterization\nView PDF HTML (experimental)Abstract:In direct imaging at high contrast, the bright glare produced by the host star makes the detection and the characterization of sub-stellar companions particularly challenging. In spite of the use of an extreme adaptive optics system combined with a coronagraphic mask to strongly attenuate the starlight contamination, dedicated post-processing methods combining several images recorded with the pupil tracking mode of the telescope are needed to reach the required contrast. In that context, we recently proposed to combine the statistics-based model of PACO with a deep learning approach in a three-step algorithm. First, the data are centered and whitened locally using the PACO framework to improve the stationarity and the contrast in a preprocessing step. Second, a convolutional neural network (CNN) is trained in a supervised fashion to detect the signature of synthetic sources in the preprocessed science data. Finally, the trained network is applied to the preprocessed observations and delivers a detection map. A second network is trained to infer locally the photometry of detected sources. Both deep models are trained from scratch with a custom data augmentation strategy allowing to generate a large training set from a single spatio-temporo-spectral dataset. This strategy can be applied to process jointly the images of observations conducted with angular, and eventually spectral, differential imaging (A(S)DI). In this proceeding, we present in a unified framework the key ingredients of the deep PACO algorithm both for ADI and ASDI. We apply our method on several datasets from the the IRDIS imager of the VLT/SPHERE instrument. Our method reaches, in average, a better trade-off between precision and recall than the comparative algorithms.\nSubmission history\nFrom: Olivier Flasseur [view email][v1] Thu, 19 Sep 2024 18:04:02 UTC (37,621 KB)\nCurrent browse context:\nastro-ph.IM\nChange to browse by:\nReferences & Citations\nexport BibTeX citation\nLoading...\nBibliographic and Citation Tools\nBibliographic Explorer (What is the Explorer?)\nConnected Papers (What is Connected Papers?)\nLitmaps (What is Litmaps?)\nscite Smart Citations (What are Smart Citations?)\nCode, Data and Media Associated with this Article\nalphaXiv (What is alphaXiv?)\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\nDagsHub (What is DagsHub?)\nGotit.pub (What is GotitPub?)\nHugging Face (What is Huggingface?)\nPapers with Code (What is Papers with Code?)\nScienceCast (What is ScienceCast?)\nDemos\nRecommenders and Search Tools\nInfluence Flower (What are Influence Flowers?)\nCORE Recommender (What is CORE?)\nIArxiv Recommender\n(What is IArxiv?)\narXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.\n\nSOURCE: https://arxiv.org/abs/2409.13031",
    "https://www.ijcai.org/proceedings/2023/0750.pdf": "SNIPPET: Proceedings of the Thirty-Second International Joint Conference on Artificial Intelligence (IJCAI-23). Survey Track. 6692. Page 2. Model. NAR Diffusion space ...\n\nTITLE: (from PDF)\n\nBODY:\nDiffusion Models for Non-autoregressive Text Generation: A Survey\n\nYifan Li1 , Kun Zhou2,3 , Wayne Xin Zhao 1,3 ∗ and Ji-Rong Wen1,2,3\n1Gaoling School of Artificial Intelligence, Renmin University of China\n2School of Information, Renmin University of China\n3Beijing Key Laboratory of Big Data Management and Analysis Methods\n{liyifan0925, batmanfly}@gmail.com, francis kun zhou@163.com, jrwen@ruc.edu.cn\n\nAbstract\n\nNon-autoregressive (NAR) text generation has at-\ntracted much attention in the field of natural lan-\nguage processing, which greatly reduces the infer-\nence latency but has to sacrifice the generation ac-\ncuracy. Recently, diffusion models, a class of la-\ntent variable generative models, have been intro-\nduced into NAR text generation, showing an im-\nproved text generation quality. In this survey, we\nreview the recent progress in diffusion models for\nNAR text generation. As the background, we first\npresent the general definition of diffusion mod-\nels and the text diffusion models, and then dis-\ncuss their merits for NAR generation. As the core\ncontent, we further introduce two mainstream dif-\nfusion models in existing work of text diffusion,\nand review the key designs of the diffusion pro-\ncess. Moreover, we discuss the utilization of pre-\ntrained language models (PLMs) for text diffusion\nmodels and introduce optimization techniques for\ntext data. Finally, we discuss several promising\ndirections and conclude this paper. Our survey\naims to provide researchers with a systematic refer-\nence of related research on text diffusion models for\nNAR generation. We present our collection of text\ndiffusion models at https://github.com/RUCAIBox/\nAwesome-Text-Diffusion-Models.\n\nIntroduction\n\n1\nText generation [Gatt and Krahmer, 2018] (a.k.a., natural lan-\nguage generation) aims to generate human-like text (i.e., a\nsequence of word tokens) given the input data (e.g., sentence\nor keywords), enabling a wide range of real-world applica-\ntions such as machine translation [Bahdanau et al., 2015]\nand text summarization [Nallapati et al., 2017]. Due to the\nexcellent sequence modeling capacity, deep learning has be-\ncome the mainstream approach to developing the backbone\nfor text generation models, exemplified by RNN [Cho et\nal., 2014] and transformer [Vaswani et al., 2017]. More re-\ncently, pre-trained language models (PLMs) [Li et al., 2021;\n\n∗Corresponding Author.\n\nZhao et al., 2023] further raise the performance bar of text\ngeneration. After being pre-trained on the large-scale general\ncorpus, PLMs can be effectively fine-tuned for downstream\ntasks, leveraging the pre-learned rich knowledge to improve\ntask performance. Generally, existing text generation meth-\nods mostly adopt the autoregressive way (AR) that generates\nthe output tokens one by one. Such a way is able to cap-\nture the sequential dependency relations among tokens, but\nwould be time-consuming when generating long texts. Thus,\nnon-autoregressive (NAR) generation methods, which gen-\nerate all tokens in parallel and greatly reduce the inference\nlatency, have been proposed [Gu et al., 2018].\n\nHowever, NAR models generally underperform AR ones\non text generation accuracy, since the token dependency re-\nlations cannot be well captured by the parallel generation.\nTo narrow the performance gap, previous works have pro-\nposed various improvement techniques for NAR methods,\ne.g., knowledge distillation [Zhou et al., 2020] and large-scale\npre-training [Qi et al., 2021]. More recently, diffusion mod-\nels [Sohl-Dickstein et al., 2015; Ho et al., 2020], a class of\ngenerative models that have shown superiority in image gen-\neration, are introduced into NAR text generation. In essence,\ndiffusion models perform a multi-step denoising process to\nprogressively convert a random noise into a data sample. To\nadapt to NAR text generation tasks, diffusion models itera-\ntively refine the intermediate generated results conditioned\non the input data, which are shown to be potentially more\ncapable of handling complex control conditions in produc-\ning high-quality text [Li et al., 2022]. Further, by design-\ning proper sampling acceleration methods [Song et al., 2021],\ndiffusion models can well balance the inference latency and\ngeneration quality, leading to an improved generation ability.\nExisting studies have explored two types of representative\ndiffusion processes from image generation into text genera-\ntion, i.e., continuous diffusion [Li et al., 2022] and discrete\ndiffusion [Hoogeboom et al., 2021; Austin et al., 2021] that\nperform the diffusion process in continuous latent represen-\ntations and discrete text tokens, respectively. We provide a\ndetailed illustration of these studies and the major features in\nTable 1. However, due to the discrete essence and complex\nsemantics of texts, it is not easy to effectively adapt the above\ndiffusion models to NAR text generation tasks. Prior stud-\nies have introduced or devised specific strategies to improve\nthe original settings of diffusion models for NAR text genera-\n\nProceedingsoftheThirty-SecondInternationalJointConferenceonArtiﬁcialIntelligence(IJCAI-23)SurveyTrack6692\fNAR Diffusion space Noise schedule\n\nModel\nD3PM [2021]\nDiffusion-LM [2022]\nDiffuseq [2022]\nSED [2022]\nSSD-LM [2022]\nDiffusionBERT [2022]\nCDCD [2022]\nDifformer [2022]\nLD4LG [2022]\nSeqDiffuSeq [2022]\nDiff-Glat [2022]\nGENIE [2022]\nDINOISER [2023]\nGlyphDiffusion [2023]\nDiffusion-NAT [2023]\n\n✓\n✓\n✓\n✓\n✓\n✓\n✓\n✓\n✗\n✓\n✓\n✓\n✓\n✓\n✓\n\nDiscrete\nContinuous\nContinuous\nContinuous\nContinuous\nDiscrete\nContinuous\nContinuous\nContinuous\nContinuous\nDiscrete\nContinuous\nContinuous\nContinuous\nDiscrete\n\nTasks\nMutual information UCG\nA2T\nSqrt\nT2T\nSqrt\nUCG, A2T\nCosine\nUCG, A2T\nCosine\nUCG\nSpindle\nT2T\n-\nT2T\nLinear\nUCG, A2T\nLinear\nT2T\nAdaptive\nT2T\n-\nT2T\n-\nT2T\nLinear\nT2T\n-\nT2T\nLinear\n\nx0-param PLMs Clamping\n✗\n✗\n✗\n✗\n✓\n✓\n✗\n✓\n✓\n✗\n✗\n✗\n✗\n✗\n✓\n\n✗\n✓\n✓\n✗\n✗\n✗\n✗\n✗\n✗\n✗\n✗\n✓\n✗\n✗\n✗\n\n✓\n✓\n✓\n✓\n✓\n✓\n✓\n✓\n✓\n✓\n✓\n✗\n✓\n✗\n✓\n\nTable 1: Comparison of existing text diffusion models. x0-param, PLMs and Clamping denote using the x0-parameterized loss, PLMs, and\nthe clamping trick, respectively. UCG, A2T, T2T refers to UnConditional Generation, Attribute-To-Text and Text-To-Text, respectively.\n\ntion, including revising the training objective, adopting noise\nschedules tailored for text, and integrating PLMs. Despite\nthis progress, the field of diffusion models for text generation\nis still nascent, requiring a deep investigation in this line. For\nthis purpose, a comprehensive survey that summarizes the re-\ncent advances is highly needed. To the best of our knowledge,\nthis survey is the first literature review that concentrates on\nthe research of diffusion models for NAR text generation.\n\nTo start with, we first briefly review diffusion models, for-\nmulate text diffusion models and describe the connections be-\ntween text diffusion models and NAR models in Section 2.\nThen, we introduce two mainstream diffusion models used\nfor NAR text generation in Section 3, and review four key\ndesigns of the diffusion process in Section 4, i.e., denoising\nnetwork, noise schedule, objective function and condition-\ning strategy. Next, we discuss the utilization of pre-trained\nlanguage models (PLMs) for text diffusion in Section 5 and\npresent other optimizations of diffusion models for text data\nin Section 6. Finally, we prospect the future directions and\nconclude this survey in Section 7.\n\n2 Overview of Text Diffusion Models\nDiffusion models have made remarkable progress in generat-\ning continuous data, e.g., image [Dhariwal and Nichol, 2021]\nand audio [Kong et al., 2021]. Recently, their applications\nin discrete text data, referred to as text diffusion models, are\ngaining growing attention.\nIn this section, we first present\na brief overview of the typical diffusion model [Ho et al.,\n2020], then give a formulated definition of text diffusion mod-\nels, and finally compare them with traditional NAR models\nfor text generation.\n\nforward process q(xt|xt−1) gradually corrupts the data sam-\nple x0 using random noise. The reverse process pθ(xt−1|xt)\nrelies on a denoising network fθ to progressively recover a\nrandom noise into the desired data sample.\n\nTo be more specific, given a data sample x0 ∼ q(x),\nthe forward process generates a sequence of latent variables\nx1, ..., xT by sampling from\n\nq(xt|xt−1) = N (xt; p1 − βtxt−1, βtI),\nwhere βt ∈ (0, 1) is the noise scale. Following a pre-defined\nnoise schedule, βt increases as the timestep grows and even-\ntually corrupts x0 into a random noise. Then, based on the\nreparameterization trick, arbitrary intermediate latent vari-\nable xt can be sampled from x0 in a closed form:\n\n(1)\n\nq(xt|x0) = N (xt;\n\n¯αtx0,\n\n1 − ¯αtI),\n\n(2)\n\n√\n\n√\n\nwhere αt = 1 − βt and ¯αt = Qt\ni=1 αi. The reverse process is\nthe approximation of the posterior q(xt−1|xt), which can be\nseen as a Gaussian when βt is small enough. In this way, the\nreverse process is also formulated as a Gaussian distribution:\n\npθ(xt−1|xt) = N (xt−1; µθ(xt, t), Σθ(xt, t)),\n\n(3)\n\nwhere µθ(xt, t) and Σθ(xt, t) are parameterized by a denois-\ning networks fθ like U-Net [Ronneberger et al., 2015] or\ntransformer [Vaswani et al., 2017]. During inference, the re-\nverse process begins with sampling noise from a Gaussian\ndistribution p(xT ) = N (xT ; 0, I) and iteratively denoise it\nby pθ(xt−1|xt) until obtaining x0. The learning objective of\ndiffusion models is derived from the variational lower bound\nof the negative log-likelihood of input x0, denoted as:\n\nLvlb =Eq[DKL(q(xt|x0)||pθ(xT ))\n}\n\n|\n\n{z\nLT\n\n] − log pθ(x0|x1)\n}\n\n|\n\n{z\nL0\n\n2.1 Diffusion Models\nDiffusion models are a class of latent variable models char-\nacterized by a forward and a reverse Markov process. The\n\n+ Eq[\n\nT\nX\n\nt=2\n\n].\nDKL(q(xt−1|xt, x0)||pθ(xt−1|xt))\n|\n}\n{z\nLt−1\n\n(4)\n\nProceedingsoftheThirty-SecondInternationalJointConferenceonArtiﬁcialIntelligence(IJCAI-23)SurveyTrack6693\fThe final training objective is derived from Lt−1. With ad-\nditional condition on x0, the posterior of the forward process\nq(xt−1|xt, x0) can be calculated using Bayes theorem, then\nthe simplified objective Lsimple can be expressed as:\n\nLsimple =\n\nT\nX\n\nt=1\n\nEq\n\n(cid:2)||µt(xt, x0) − µθ(xt, t)||2(cid:3),\n\n(5)\n\nwhere µt is the mean of posterior q(xt−1|xt, x0). Therefore,\nthe denoising network fθ is trained to predict µt given xt and\nt. Through different parameterization strategies, the predic-\ntion objective can also be the noise ϵt [Ho et al., 2020] or\noriginal data x0 [Li et al., 2022].\n\n2.2 Text Diffusion Models\nText diffusion models aim to gradually recover a random\nnoise to a desired text based on the given input data. The\nstarting noise can be discrete (e.g., [MASK] tokens) or con-\ntinuous (e.g., random Gaussian noise), corresponding to the\ndiscrete or continuous diffusion model (Section 3). The de-\nnoising process relies on a parameterized denoising network,\nwhich is generally implemented by the transformer archi-\ntecture [Vaswani et al., 2017]. During training, the denois-\ning network learns to recover the intermediate noised results\nbased on the settings of noise schedule, objective function and\nconditioning strategy (Section 4). During inference, starting\nfrom a random noise YT , the denoising network progressively\ndenoises it at each step, until producing the target text. Note\nthat at each step, following the NAR generation manner, text\ndiffusion models predict all the latent variables in parallel.\nThe above process can be formulated as:\n\np(Y|c) =\n\nT −1\nY\n\nn\nY\n\nt=0\n\ni=1\n\np(yi| ˆYt+1, c, t),\n\n(6)\n\nwhere Y is the target text consisting of a sequence of tokens\nyi, ˆYt+1 denotes the latent variables predicted at the t + 1\ntimestep, c is the input condition and t denotes the timestep.\nTo improve the performance, it is significant to incorpo-\nrate advanced NLP techniques with text diffusion models. As\nan important progress in the NLP area, pre-trained language\nmodels (PLMs) have been explored for integration with text\ndiffusion models (Section 5). Moreover, a variety of opti-\nmization strategies have been proposed in existing text dif-\nfusion models to better capture the unique characteristics of\ntext data (Section 6).\n\n2.3 Merits of Text Diffusion Models for NAR\n\nGeneration\n\nAs mentioned before, the parallel generation manner of NAR\nmethods would greatly reduce the inference latency, but is in-\ncapable of learning the dependency relations among tokens,\nleading to a decreased accuracy. While, text diffusion models\nhave several merits that can help improve the NAR generation\naccuracy. In this part, we show three major merits of text dif-\nfusion models, i.e., constrained iterative refinement, introduc-\ning intermediate control and trading off time-cost and quality.\n\nConstrained Iterative Refinement. Typical NAR models\ngenerate all the target tokens in parallel, hence the infer-\nence latency would be rather smaller than in AR meth-\nods. Therefore, existing works [Ghazvininejad et al., 2019;\nGu et al., 2019] also incorporate the iterative refinement strat-\negy to enhance the quality of the generated results. How-\never, with the increase of the iteration steps, it also raises the\nproblem of how to effectively control or supervise the inter-\nmediate refinement process on the discrete target tokens, re-\nstraining the improvement of the generation performance. As\na promising solution, text diffusion models provide a con-\nstrained iterative refinement process for gradually enhancing\nthe generation quality, where each step is constrained to de-\nnoise a random noise with a pre-defined variation.\nImposing Intermediate Control.\nIn the iteration process,\nit is also hard to directly control the intermediate results for\nexisting NAR methods, especially for injecting complex con-\ntrolled conditions (e.g., following a syntax parse tree). For\ntext diffusion models, existing works have extensively stud-\nied injecting the control conditions in the intermediate results,\nby adding extra classifiers [Li et al., 2022] or using classifier-\nfree controls [Strudel et al., 2022]. As theoretically and em-\npirically proved, these approaches can better steer the inter-\nmediate prediction steps toward the generation of the target\ntext that satisfies the control requirements.\nTrading off between Time Cost and Quality. During in-\nference, existing NAR methods seek to strike a balance be-\ntween time cost and quality. They mainly rely on tuning the\niterative turns to achieve the goal, where decreasing the num-\nber of iterations would increase the inference speed but po-\ntentially sacrifice the generation quality. To provide a more\nflexible trade-off between quality and inference time, text dif-\nfusion models can adopt inference acceleration techniques,\ne.g., DDIM [Song et al., 2021]. Empirical results have shown\nthat these methods can flexibly adjust the iteration steps with\na slight decrease in the\n...[truncated]",
    "https://proceedings.mlr.press/v202/lin23d/lin23d.pdf": "SNIPPET: Jul 3, 2023 ... ... Proceedings of Machine Learning ... https://proceedings.mlr.press/v202/lin23d.html. Copy to Clipboard Download. Related Material. Download PDF ...\n\nTITLE: (from PDF)\n\nBODY:\nText Generation with Diffusion Language Models: A Pre-training Approach\nwith Continuous Paragraph Denoise\n\nZhenghao Lin 1 2 3 Yeyun Gong 4 Yelong Shen 5 Tong Wu 6 2 Zhihao Fan 7 2\nChen Lin 1 3 Nan Duan 4 Weizhu Chen 5\n\nAbstract\n\nIn this paper, we introduce a novel dIffusion\nlanguage modEl pre-training framework for text\ngeneration, which we call GENIE. GENIE is a\nlarge-scale pre-trained diffusion language model\nthat consists of an encoder and a diffusion-based\ndecoder, which can generate text by gradually\ntransforming a random noise sequence into a\ncoherent text sequence. To pre-train GENIE\non a large-scale language corpus, we design a\nnew continuous paragraph denoise objective,\nwhich encourages the diffusion-decoder\nto\nreconstruct a clean text paragraph from a\ncorrupted version while preserving the semantic\nand syntactic coherence. We evaluate GENIE\non four downstream text generation benchmarks,\nnamely XSUM, CNN/DAILYMAIL, GIGA-\nWORD, and COMMONGEN. Our experimental\nresults show that GENIE achieves comparable\nperformance with the state-of-the-art autore-\ngressive models on these benchmarks, and\ngenerates more diverse text samples.\nThe\ncode and models of GENIE are available\nhttps://github.com/microsoft/\nat\nProphetNet/tree/master/GENIE.\n\n1. Introduction\n\nText generation is a crucial task in natural language process-\ning, which aims to produce ﬂuent and coherent texts for var-\nious applications. Previous text generation methods mainly\nrelied on recurrent neural networks (RNNs) (Pawade et al.,\n2018; Song et al., 2018; Gu et al., 2016a; Qi et al., 2021),\n\n1School of Informatics, Xiamen University 2This work was\ndone during an internship in MSRA 3Shanghai Artiﬁcial Intelli-\ngence Laboratory 4Microsoft Research Asia 5Microsoft 6Tsinghua\nUniversity 7Fudan University. Correspondence to: Chen Lin\n<chenlin@xmu.edu.cn>.\n\nProceedings of the 40 th International Conference on Machine\nLearning, Honolulu, Hawaii, USA. PMLR 202, 2023. Copyright\n2023 by the author(s).\n\n1\n\nwhich generate texts sequentially from left to right. How-\never, RNNs suffer from issues such as long-term dependency\nand exposure bias. Recently, Transformer (Vaswani et al.,\n2017b), a self-attention-based neural network, has emerged\nas the dominant paradigm for text generation, thanks to its\nability to capture global dependencies and leverage large-\nscale pre-trained language models (Qi et al., 2020; Lewis\net al., 2019; Raffel et al., 2020a). Transformer-based meth-\nods typically adopt an encoder-decoder architecture, where\nthe encoder maps the input text to a sequence of hidden vec-\ntors, and the decoder generates the output text either autore-\ngressively (AR) or non-autoregressively (NAR). Generally,\nAR decoding is more accurate but slower, as it predicts each\nword conditioned on the previous ones. NAR decoding is\nfaster but less precise, as it predicts all words simultaneously\nwithout modeling their dependencies.\n\nThis paper presents a new text generation approach, called\nGENIE, that integrates the diffusion model and Transformer-\nbased method. The diffusion model is a generative model\nthat reverses a stochastic process of adding noise to the data\nand has shown promising results in image (Ho et al., 2020;\nSong et al., 2020), molecule (Hoogeboom et al., 2022),\nvideo (Ho et al., 2022), and text (Li et al., 2022b; Gong\net al., 2022; Strudel et al., 2022; Reid et al., 2022) generation.\nGENIE follows the encoder-decoder architecture, where the\nencoder transforms the input text to hidden vectors, and\nthe diffusion model restores the output text from a random\nGaussian noise, guided by the encoder’s hidden vectors.\nThe diffusion model iterates over multiple time steps and\ngradually denoises the output text at each step.\n\nTo leverage the large-scale unlabeled text data, we also pro-\npose an end-to-end pre-training method for GENIE. Unlike\nthe existing pre-training tasks that involve masking or split-\nting tokens or texts (Qi et al., 2020; Lewis et al., 2019; Raffel\net al., 2020a), we design a novel pre-training task, called\ncontinuous paragraph denoise (CPD). CPD requires the\nmodel to predict the noise added to continuous paragraphs\nin the current time step, given the paragraph context and the\nnoisy paragraph information.\n\nWe evaluate GENIE on four popular text generation bench-\nmarks: XSum (Narayan et al., 2018), CNN/DailyMail (Her-\n\n\fText Generation with Diffusion Language Models: A Pre-training Approach with Continuous Paragraph Denoise\n\nmann et al., 2015), Gigaword (Rush et al., 2015), and Com-\nmonGen (Lin et al., 2019). The experimental results demon-\nstrate that GENIE achieves competitive performance with\nTransformer-based AR methods, and that the proposed pre-\ntraining method can effectively improve the performance.\nWe notice that GENIE has signiﬁcantly increased the diver-\nsity of the generated texts. To evaluate the multiple outputs\nof the generation model, we design an automatic annotation\nmethod based on the large language model. We also conduct\nablation studies to analyze the impact of the diffusion steps\nand pre-training steps.\n\nThe main contributions of this work are summarized as\nfollows:\n\n• We propose GENIE, the ﬁrst large-scale language\npre-trained model based on the diffusion framework,\nwhich can generate high-quality texts for sequence-to-\nsequence tasks.\n\n• We introduce a novel CPD loss as the pre-training\nobjective, which can enhance the model’s ability to\ndenoise noisy texts and capture paragraph-level coher-\nence.\n\n• We validate the effectiveness of the pre-trained diffu-\nsion model on downstream tasks, and design a new\nautomatic annotation method for the evaluation based\non a large language model. We also provide extensive\nanalyses of the model’s behavior and properties.\n\n2. Preliminary\n\n2.1. Task Deﬁnition\n\nIn the classical sequence-to-sequence task, given a source\ntext s = {ws\n2, . . . , ws\n1, ws\nn} with n tokens, it generates tar-\nget text sequence y = {wy\n1 , wy\nn}. A sequence gen-\neration model can achieve this by modeling the conditional\nprobability: p (y | s).\n\n2 , . . . , wy\n\n2.2. Diffusion model\n\nIn the diffusion model, the diffusion process can be regarded\nas a discrete-time Markov process. The diffusion process\nstarts with the initial state x0 at time step t = 0, where x0\nis the Gaussian distribution of the original data. It gradually\nadds Gaussian noises to x0 in the forward diffusion process\naccording to a variance schedule β1, ..., βT . At the time step\nt + 1, the latent variable xt+1 is only determined by the xt\nat time t, expressed as:\n\nq (xt+1 | xt) = N\n\n(cid:16)\n\nxt+1; (cid:112)1 − βt+1xt, βt+1I\n\n(cid:17)\n\n.\n\n(1)\n\nAs t increases, xt becomes closer to standard Gaussian\nnoise N (xT ; 0, I).\n\n2\n\nThe diffusion model learns to perform the inverse diffusion\nprocess during generation, which predicts the noise given\nthe current state xt at time step t. The previous state xt−1\ncan be reconstructed by subtracting the noise and re-scaling\nthe mean. Thus, the distribution of xt−1 given xt is a\nGaussian with mean µt−1\n\nand variance σt−1\n\n2\n\n:\n\nθ\n\nθ\n\np (xt−1 | xt) = N (cid:0)xt−1; µt−1\n\nθ\n\n, σt−1\nθ\n\n(cid:1) ,\n\n(2)\n\nµt−1\n\nθ =\n\n(cid:18)\n\nxt −\n\n1\n√\nαt\n\nβt√\n\n1 − ¯αt\n\n(cid:19)\n\nzθ (xt, t)\n\n,\n\n(3)\n\n2\n\nσt−1\nθ\n\n=\n\n1 − ¯αt−1\n1 − ¯αt\n\n· βt,\n\n(4)\n\nwhere αt = 1 − βt, ¯αt = (cid:81)t\ni=1 αi and zθ is predicted by\na neural network parameterized by θ. The diffusion model\nis trained by minimizing the mean squared error between\nµt−1\nand the true mean ˆµt−1, which is computed from the\nθ\nreverse conditional distribution q(xt−1|xt, x0):\n\nq (xt−1 | xt, x0) = N\n\n(cid:17)\n(cid:16)\nxt−1; ˆµt−1, ˆβt−1I\n\n,\n\n(5)\n\nˆµt−1\n\nθ =\n\n√\n\n¯αt−1βt\n1 − ¯αt\n\nx0 +\n\n√\n\nαt (1 − ¯αt−1)\n1 − ¯αt\n\nxt.\n\n(6)\n\nFollowing the variational lower bound (VLB) approach (Ho\net al., 2020), the diffusion model can be trained by minimiz-\ning the loss function:\n\nLdiff =\n\nT\n(cid:88)\n\nt=1\n\nE\nq(xt|x0)\n\n(cid:13)\n(cid:13)µt−1\n\nθ − ˆµt−1\n\n(cid:13)\n(cid:13)\n\n2\n\n.\n\n(7)\n\n3. Model\n\nGENIE is the proposed diffusion language model for pre-\ntraining, it adopts the sequence-to-sequence framework\nas illustrated in Figure 1. GENIE could generate a high-\nquality text sequence y given a source text s, such as pro-\nducing y : Messi’s performance from s : In the World\nCup 2022, [MASK] won people’s praise.. To achieve this,\nGENIE leverages two components: a bidirectional encoder\nmodel and a cross-attention diffusion model. The encoder\nmodel encodes the source text s into a set of hidden vec-\ntors Hs = Encoder(s), which indicates the distributed\nrepresentation of s. The diffusion model takes Hs and a\nGaussian noise as inputs, and iteratively reﬁnes the data by\napplying a sequence of denoising operations. In contrast\nto the traditional autoregressive text generation paradigm,\nwhich generates one token at a time, the diffusion model in\nGENIE outputs the sequence of embeddings in parallel at\neach denoising step, making GENIE a non-autoregressive\ngeneration (NAR) model.\n\n\fText Generation with Diffusion Language Models: A Pre-training Approach with Continuous Paragraph Denoise\n\nFigure 1. The framework of GENIE. We take the masked source sequence s as the input of the Encoder to obtain the hidden information\nHs, and interact with Language Diffusion Model through cross attention. The language Diffusion Model restores the randomly initial\nGaussian noise to the output text y through the iterative denoising and grounding process.\n\nEncoder The encoder in GENIE is a 6-layer transformer\nmodel which takes the source text s as input with bidi-\nrectional self-attention. Speciﬁcally, given a source text\nsequence s = {ws\nn} with n tokens, the encoder\nmodel computes the vector hi for each token wi. Thus,\nthe source text s can be represented as Hs by the encoder\nmodel:\n\n2, . . . , ws\n\n1, ws\n\nHs = {h1, h2, ..., hn} = Encoder(s).\n\n(8)\n\nLanguage Diffusion Model The diffusion model\nin\nGENIE is a 6-layer transformer with cross-attention on the\nsource text representation Hs. It learns to predict Gaussian\nnoise zθ (xt, t, Hs) conditioned on the current diffusion\nstep t and the state xt, where xt is the continuous latent\nrepresentation of the target text. We use an embedding func-\ntion and a clamping trick to ground the continuous state xt\nwith discrete target tokens, which will be elaborated in the\nfollowing section.\n\nInference Phase To generate text from the diffusion\nmodel, we start from the ﬁnal step t = T and sample a\nstate xT from a standard Gaussian distribution. Then we\niteratively generate the noise for the previous step using\nequations 3 and 4, and subtract it from the current state to\nobtain xt−1. After arriving at t = 0, we apply the clamp-\ning trick (Li et al., 2022b) to replace the values of x0 with\nits closest word embeddings, and then decode the discrete\ntokens from x0.\n\nTraining Phase To train the diffusion model for sequence-\nto-sequence tasks, we ﬁrst convert the target sequence\n\n3\n\n1 , wy\n\n2 , . . . , wy\n\ny = {wy\nn} into a continuous state x0 using\nthe embedding function with an additional Gaussian noise\npermutation, which can be expressed as:\n\nq(x0|y) = N (x0; Emb(y), β0I) ,\n\n(9)\n\nwhere Emb(·) is embedding function, β0 represents the\nscaling of variance at time step t = 0. Then we apply the\nforward diffusion process (equation 1) to obtain the state xt\nat any step t as a function of x0, as shown in equation:\n\nq(xt|x0) = N (cid:0)xt;\n\n√\n\n¯αtx0,\n\n√\n\n1 − ¯αtI(cid:1) ,\n\n(10)\n\nwhere ¯αt = (cid:81)t\ni=1 αi. In the training phase, we sample\na random step t to calculate xt, and then use the denois-\ning architecture to predict the noise for that step, based\non the cross-attention with the source representation Hs.\nThe mean and variance of the predicted noise are given by\nequations 11:\n\nµt−1\n\nθ =\n\n(cid:18)\n\nxt −\n\n1\n√\nαt\n\nβt√\n\n1 − ¯αt\n\n(cid:19)\n\nzθ (xt, t, Hs)\n\n,\n\n(11)\n\nwhere zθ is the output of the denoising architecture and θ\nare its parameters. The training objective is to minimize\nthe squared error between the predicted and true noise, as\nwell as the reconstruction error between x0 and the target\nembeddings, as expressed in equation 12:\n\nLs2s =\n\nE\nq(x0:T |y)\n\nT\n(cid:88)\n[\n\nt=1\n\n(cid:13)\n(cid:13)µt−1\n\nθ − ˆµt−1\n\n(cid:13)\n2\n(cid:13)\n\n(12)\n\n+ (cid:13)\n\n(cid:13)Emb(y) − µ0\nθ\n\n(cid:13)\n2\n(cid:13)\n\n− log pθ(y|x0)],\n\nEncoder𝑤(cid:2869)𝑤(cid:2870)𝑀𝑤(cid:2869)(cid:2868)𝑤(cid:2869)(cid:2869)𝑥(cid:3021)Initial Gaussian Noise𝑦(cid:2871)𝑦(cid:2872)𝑦(cid:2873)outputs𝑦(cid:2870)𝑦(cid:2869)𝑦(cid:2874)Language Diffusion ModelEncoder Inputsℎ(cid:2869)ℎ(cid:2870)ℎ(cid:2871)ℎ(cid:2872)ℎ(cid:2873)𝐻(cid:3046)Decoder/GroundingCross AttentionStatus 𝑥(cid:3047), time step 𝑡𝐻(cid:3046)Transformer blocks𝑥(cid:3047)(cid:2879)(cid:2869)denoise\fText Generation with Diffusion Language Models: A Pre-training Approach with Continuous Paragraph Denoise\n\nwhere pθ(y|x0) = (cid:81)n\ni=1 pθ(wy\ni |x0), represents mapping\nthe continuous latent variable x0 into the discrete space\ntoken wy\ni .\n\n3.1. Pre-training GENIE\n\nDiffusion models have great potential for natural language\ngeneration (NLG) due to their ability to produce diverse\noutputs. However, they have been largely overlooked in\nNLG because of their slow convergence and low quality\ncompared to autoregressive models. In this section, we ad-\ndress these challenges by pre-training a diffusion language\nmodel and introducing a novel, tailored pre-training task.\nThe novel pre-training task we propose is called continuous\nparagraph denoise (CPD). CPD aims to train the model to\npredict the noise added to a continuous paragraph in the cur-\nrent diffusion step, given the paragraph and its surrounding\ncontext.\n\n1, wd\n\n1, wp\n\n1 , wd(cid:48)\n\n2, . . . , wp\n\n2 , . . . , [MASK], . . . , wd(cid:48)\n\nSpeciﬁcally, given a document d = {wd\n2, . . . , wd\nl }\nwith l words, we randomly select a paragraph p =\n{wp\nm} from d, where m = (cid:98)γ ∗ l(cid:99) is\nthe paragraph length and γ is a predeﬁned ratio. We\nmask the paragraph in the document with a special to-\nken ([MASK]), and feed the masked document d(cid:48) =\n{wd(cid:48)\nl−m} to the GENIE en-\ncoder. We also apply the forward diffusion process to the\nparagraph p and obtain a noisy version xt at a random step\nt, and feed it to the GENIE denoising architecture. The\ndenoising architecture then uses the cross-attention with the\nsource representation Hs to predict the noise for the current\nstep, using equations 11. In summary, the pre-training ob-\njective of CPD is to minimize the same loss as in equation\n12, except that y is replaced by p and x0 is the embedded\nparagraph with noise.\n\nThrough this pre-training task, the diffusion model can en-\nhance its semantic understanding of the continuous text and\nits denoising ability at each diffusion step. Moreover, the\nCPD task is self-supervised and does not rely on external\nlabeled data sources, so it can fully exploit the information\nin the origina\n...[truncated]",
    "https://aclanthology.org/2024.naacl-long.261.pdf": "SNIPPET: Human Language Technologies (Volume 1: Long Papers), pages 4664–4683. June 16-21, 2024 ©2024 Association for Computational Linguistics.\n\nTITLE: (from PDF)\n\nBODY:\n4664\nProceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics:\nHuman Language Technologies (Volume 1: Long Papers), pages 4664–4683\nJune 16-21, 2024 ©2024 Association for Computational Linguistics\n\nEmpoweringDiffusionModelsontheEmbeddingSpaceforTextGenerationZhujinGao1,2*,JunliangGuo3*,XuTan3,YongxinZhu1,2,FangZhang1,2,JiangBian3,LinliXu1,2†1SchoolofComputerScienceandTechnology,UniversityofScienceandTechnologyofChina2StateKeyLaboratoryofCognitiveIntelligence3MicrosoftResearchAsiagaozhujin@mail.ustc.edu.cn,{junliangguo,xuta}@microsoft.com{zyx2016,fangzhang}@mail.ustc.edu.cn,jiabia@microsoft.comlinlixu@ustc.edu.cnAbstractDiffusionmodelshaveachievedstate-of-the-artsynthesisqualityonbothvisualandaudiotasks,andrecentworksfurtheradaptthemtotextualdatabydiffusingontheembeddingspace.Inthispaper,weconductsystematicstudiesoftheoptimizationchallengesencounteredwithboththeembeddingspaceandthedenoisingmodel,whichhavenotbeencarefullyexplored.Firstly,thedatadistributionislearnableforem-beddings,whichmayleadtothecollapseoftheembeddingspaceandunstabletraining.Toalle-viatethisproblem,weproposeanewobjectivecalledtheanchorlosswhichismoreefficientthanpreviousmethods.Secondly,wefindthenoiselevelsofconventionalschedulesareinsuf-ficientfortrainingadesirabledenoisingmodelwhileintroducingvaryingdegreesofdegenera-tioninconsequence.Toaddressthischallenge,weproposeanovelframeworkcallednoiserescaling.Basedontheaboveanalysis,wepro-poseDifformer,anembeddingdiffusionmodelbasedonTransformer.Experimentsonvari-etiesofseminaltextgenerationtasksshowtheeffectivenessoftheproposedmethodsandthesuperiorityofDifformeroverpreviousstate-of-the-artembeddingdiffusionbaselines.11IntroductionAwaveofdiffusionmodels(Sohl-Dicksteinetal.,2015;Hoetal.,2020;Songetal.,2020b)issweep-ingthegenerationtasks(e.g.,imageandaudiosyn-thesis)recently,showingtheirgreatcapacityforhigh-qualitydatageneration.Diffusionmodelsareafamilyofiterativegenerativemodels,whicharetrainedtorecovercorrupteddataandthengener-atedatabygraduallyrefiningsamplesfromthepurenoise.Thisprocedureenablesthemodeltomakesubtlerefinementsofoutputsamplesina*Equalcontribution.†Correspondingauthor.1Codeisavailableathttps://github.com/zhjgao/difformermulti-stepdenoisingprocess,andthusgeneratehigh-fidelityanddiversesamples(DhariwalandNichol,2021;NicholandDhariwal,2021;HoandSalimans,2021;Rombachetal.,2022;Chenetal.,2020;Kongetal.,2020).Theboomingachievementsinvisionandaudiodomainsinspireresearcherstodelveintotherealmoftextgeneration.Diffusionmodelsintroduceanovelnoisingparadigmandatrainingobjectiveotherthantokenprediction,establishinganalterna-tiveformoflanguagemodels,whichexhibitsthepotentialtofosteranenhancedcomprehensionoflanguagemodeling.Fromahigherperspective,thisinvestigationgeneralizesthediffusionmodelacrossmodalities,andfurthercontributestoaunifiedmul-timodalframework(Baoetal.,2023;Tangetal.,2024).Nonetheless,theexplorationisstillatanini-tialstage.Recentworks(Lietal.,2022;Gongetal.,2022;Strudeletal.,2022)basicallyconvertthediscretetokenstoembeddingsandthenutilizecon-tinuousdiffusionmodelstogeneratethem,whichcanbetermedembeddingdiffusionmodels.Thesepreliminaryattemptsfollowtheoriginalmodeltodealwiththeembeddings,withlittleconsiderationoftheuniquepropertiesandtheoptimizationchal-lengesoftheembeddingspaceandthedenoisingmodel.Inthispaper,weexploretheembeddingdiffu-sionmodelfromtwoperspectivesseparately,i.e.,theembeddingspaceandthedenoisingmodel,basedonwhichweconductathoroughstudyre-spectively.Firstly,fordiffusionmodelsonimageandaudiogeneration,thegroundtruthdataissta-tionaryduringtraining.Incontrast,itislearnableforthetextualdata(i.e.,embeddings),whichmaycausethecollapseoftheembeddingspaceandin-troduceinstabilitytothetrainingofthemodel.Toavoidthecollapsecausedbydynamicallyshiftingembeddingparameters,weproposeananchorlossfunctiontoattainwell-distributedembeddingsandstabilizethetrainingprocess.Thedetailedanalysis\f4665\n\nispresentedinSection3.1.Secondly,inSection3.2,wefindthatinthehighdimensionalembeddingspace,theinsuffi-cientnoiseresultsinasimpledenoisingtask,whichcausesthedegenerationofthemodel.Totacklethischallenge,weproposeanovelframeworknamednoiserescaling,whichisorthogonaltothechoiceofthenoisescheduleandapplicabletoanyexistingschedules.Specifically,wedefineanindextermeddegenerationscoreasameasurementofthedegreeofdegeneration.Guidedbythedegenerationscore,wecanapplyanoiserescalingproceduretopreventthemodelfromdegenerating.Basedontheabovediscussion,weproposeanin-tegratedframeworkofDifformer,adenoisingdiffu-sionTransformermodel.Weconductexperimentsonavarietyofimportanttextgenerationtasksin-cludingmachinetranslation,textsummarization,paraphrasing,textsimplification,andquestiongen-eration.Onthesebenchmarkdatasets,Difformeroutperformsdiffusion-basedanditeration-basednon-autoregressivebaselinesandachievesstate-of-the-artperformanceamongembeddingdiffusionmodels.Furtherexperimentsdemonstratethesupe-riorityofDifformeroverbaselinesincludingLLMsinquality,diversity,andefficiency,emphasizingthepotentialofdiffusionmodelsfortextgenerationintheeraofLLMs.2BackgroundDiffusionModelsDenoisingdiffusionproba-bilisticmodels(Sohl-Dicksteinetal.,2015;Hoetal.,2020)utilizeaforwardprocesstoperturbthedatawithGaussiannoise,andareversepro-cesstorestorethedatasymmetrically.Hoetal.(2020)developtheapproachbyspecificparam-eterizations,achievingcomparablesamplequal-itywithstate-of-the-artgenerativemodelssuchasGANs(GoodfellowIanetal.,2014).Afterthat,greatimprovementshavebeenmadebymanyfol-lowingworks(Songetal.,2020a;DhariwalandNichol,2021;NicholandDhariwal,2021;Rom-bachetal.,2022)bothinqualityandefficiency.Givenadatasamplez0∈Rd,thedenoisingdiffu-sionprobabilisticmodelgraduallyperturbsitintoapureGaussiannoisezT∼N(0,I)throughaseriesoflatentvariablesz1,···,zTintheforwardprocess:q(zt|z0)=N(cid:0)zt;√¯αtz0,¯βtI(cid:1),where¯αt,¯βtarehyper-parameterscontrollingthenoiseleveladdedattimestept,whichformthenoiseschedule.Usually,thesehyper-parametersaresettosatisfy¯αt:=Qti=0αi,αt+βt=1,and¯αt+¯βt=1.Thereverseprocessisparameterizedas:pθ(zt−1|zt)=N(zt−1;µθ(zt,t),Σθ(zt,t)),whereµθ(·)andΣθ(·)arethepredictedmeanandcovarianceofq(zt−1|zt),andθdenotesthemodelparameters.Afterparameterization,weutilizeasimplifiedvariationallower-boundastheobjectivefunctionLvlb=Ez0,zt,t(cid:2)∥ˆz0(zt,t)−z0∥2(cid:3),(1)whereˆz0(zt,t)isthemodelpredictionoftheorig-inaldataz0givenzt.ThedetailedderivationcanbefoundinAppendixB.DiffusionModelsforTextGenerationThebreakthroughofdiffusionmodelsoncontinuousdataencouragespeopletoexploretheirpotentialondiscretetextualdata.Thedefinitionofforwardandreverseprocessesisthekeyquestionfordif-fusionmodels.Recentworksmainlyfollowtwodirections.Firstly,discretediffusionmodelsoncategori-caldistributionsareproposed(Hoogeboometal.,2021;Austinetal.,2021;Savinovetal.,2021;Reidetal.,2022),bywhichsentencesarecorruptedandrefinedatthetokenlevel.However,thesekindsofcorruptionarecoarse-grained.Attemptshavebeenmadetoexploremodelingonsurrogaterepresen-tationsofdiscretedatasuchasanalogbits(Chenetal.,2022)andsimplex(Hanetal.,2023).Never-theless,theserepresentationscarrylittlesemanticinformationabouttokens,whichimpliesthatthedistancesinthisspacecannotaccuratelyreflectsemanticcorrelationsbetweentokens.Incontrast,embeddingdiffusionmodels(Lietal.,2022;Strudeletal.,2022;Gongetal.,2022;Yeetal.,2023)introduceanadditionalembed-dingstepandroundingstepintheforwardandreverseprocessesrespectively.Theembeddingstepconvertstokensintolearnableorpre-trainedem-beddings,whichcarrysemanticinformation,andthenacontinuousdiffusionprocessisabletoaddGaussiannoisetotheseembeddings,achievingafine-grainednoisingprocedure.Mathematically,givenasequenceoftokensy=[y1,y2,···,yn],theembeddingstepcanbedenotedasz0∼N(eϕ(y),β0I)whereeϕ(·)denotestheembed-dinglookupfunction.Theroundingstepturnspre-dictedembeddingsbacktodiscretetokens,which\f4666\n\nContinuousDiscreteEmbeddingSpace𝐳𝑇𝐳𝑡𝐳𝑡−1⋯𝐳0𝒚𝑝𝜃(𝐳𝑡−1|𝐳𝑡,𝐱)Reverse𝑞𝐳𝑡𝐳𝑡−1=𝒩(𝐳𝑡;𝛼𝑡𝐳𝑡−1,𝑭𝟐𝛽𝑡𝐈)ForwardAnchor Loss𝒙⋯Noise Rescaling𝒩(𝟎,𝑭𝟐𝐈)𝑝𝜙(𝒚|ො𝐳𝟎)Figure1:AnoverviewoftheDifformer,includingtheproposedtechniques,i.e.,theanchorloss,andthenoiserescaling.canbeexpressedasasoftmaxdistributionoverthevocabularypϕ(y|z0),andistrainedbyanextralossfunctionLround=Ey,z0[−logpϕ(y|z0)].Thepa-rametersofthisstepandtheembeddingsteparetied.Thefinallossfunctioniswrittenas:Ltext=Lvlb+Lround.Nevertheless,theseworksdirectlyadaptcon-tinuousdiffusionmodelstoembeddings,withoutconsideringthegapbetweenthelearnableembed-dingspaceandthestationaryimageoraudiodata,aswellasthedistinctiverequirementsofthede-noisingmodelestablishedontheembeddingspace.3MethodologyThissectionelucidatesthechallengesinher-entinoptimizingembeddingdiffusionmodelsandpresentsourcorrespondingsolutions.Westartwithanintroductiontothemodelarchitec-ture.ThemodelarchitectureisbasedonTrans-former(Vaswanietal.,2017),whichconsistsofanencoderandadecoder.Thedecoder,asthemainstemcomponent,isconsideredastwoseparatepartsinthispaper,namelytheembed-dingseϕ=[e1,e2,···,eV]∈Rd×V,eϕ(y)=[ey1,ey2,···,eyn]andthedenoisingmodelfθ(·),whichdenotesthestackeddecoderlayers.Notably,thispaperdefinez0=eϕ(y).Theencoderpro-videstherepresentationx=Encoder(x)oftheconditionsentencex=[x1,x2,···,xm].3.1CollapseoftheEmbeddingSpaceAnalysisoftheCollapseProblemThedataspaceisusuallyfixedforcontinuousdata(e.g.,im-ageandaudio),whileitislearnedfromscratchfordiscretetextualdata(i.e.,embeddings),whichthereforeshiftsdynamicallyduringtraining.Orig-inaldiffusionmodelsrelyonthelossfunctionEq.(1)tolearntoestimatethecleandatasam-plez0.Nevertheless,whendirectlyadaptingthisobjectivetotheembeddingdiffusionmodel,theembeddingspacewillcollapse.Asaresult,theembeddingsofdifferenttokenswillbelessdis-tinguishableandnon-uniformlydistributedinthespace,whichconsiderablylimitstherepresenta-tioncapacityandqualityoftheembeddings.Onthecontrary,themodelcouldachievebetterperfor-mancewithmoreisotropicembeddings(Gaoetal.,2018;Lietal.,2020).Recentworksofdiffusiononembeddings(Lietal.,2022;Gongetal.,2022)introducetheround-inglossLroundfromthederivationofthevaria-tionallowerbound,whichdiscriminatesthecor-rectembeddingsfromothersgiventheirnoisedcounterparts,thereforeenforcestheembeddingsaredistinguishableandinformative,alleviatingthecollapseobjectively.Wecouldregardthisaddi-tionallossfunctionasaregularizationtermfortheembeddings.Nonetheless,onlyaminorlevelofperturbationisinvolvedfromytoz0,therebytheroundinglossisonlyabletoapplyarelativelyweakconstraintontheembeddings.Ourempiricalevidencealsocorroboratesthelim-itationoftheroundingloss.Weobservethattheroundinglossundergoesasteepdescentandfallstonearzerointheinitialstagesoftraining,whichim-pliestheroundinglosscanbeeffortlesslyaddressedandfailstoconductstrongenoughregularizationtotheembeddings.Therefore,theembeddingspaceisundesirableandeventuallyleadstounsatisfac-\f4667\n\ntoryperformance.Concurrently,theinstabilityintrainingalsoemergesasaproblemduringtraining.Evenifcarefultuningofthehyper-parametersisperformedtorelieveanisotropy,theperformanceisstillinferior.AnchorLossToemphasizetheeffectofthereg-ularizationterm,weproposeatrainingobjectivenamedtheanchorlossLanchor=E(x,y),zt,t[−logpϕ(y|ˆz0(zt,x,t))].ComparedwithLround,Lanchorutilizesthemodelpredictionofz0astheinput,whichinvolvesalargediscrepancywithz0duetothepredictionerrorofthedenoisingmodel.Consequently,toensurethesehighlynoisyrepresentationsareidentifiedasthecorrecttokens,theanchorlossemploysastrongerregularizationtotheembeddingstopre-ventcollapse.Additionally,besidesLvlb,thean-chorlosscreatesanotherpathwaybetweenthede-noisingmodelandthetargetsentences,throughwhichthemodelcouldreceivefeedbackfromthegroundtruth,maintainingthetrainingstability.Fi-nally,ourtrainingobjectiveiswrittenasL=Lvlb+Lanchor.(2)Empirically,weuseselfsimilarity(Ethayarajh,2019)astheanisotropyscoretomeasurethesever-ityofcollapse:ANI=1V(V−1)VXi=1VXj=1,j̸=icos(ei,ej).Essentially,thehighertheanisotropyscoreis,themoreseverethecollapseis.TheanisotropyscoreaswellastheperformanceobtainedbyeachlossfunctioncanbefoundinTable1.WithonlyLvlborLtext,theanisotropyscoredemonstratesthattheembeddingsarenon-uniformlydistributed,re-sultinginunsatisfactoryresults.Onthecontrary,theembeddingsarewell-distributedacrosstheen-tirespacewiththeanchorloss,andthusthemodelreachescompetitiveperformance(inBLEU(Pa-pinenietal.,2002)).Alternatively,utilizingpre-trainedembeddingsandfreezingthemduringtrain-ingcouldalsoavoidcollapse.Asshownintheexperimentalresults,thefrozenembeddingsalle-viatethecollapseremarkably,however,theyaresuboptimalfortheproblem.DetaileddiscussioncanbefoundinAppendixC.3.LossANIBLEULvlb0.990.07Ltext0.3227.89L0.0334.48Table1:TheanisotropyscoreandperformanceofeachlossfunctionontheIWSLT14De-Endatasetwithlinearschedule.3.2DegenerationoftheDenoisingModelAnalysisoftheDegenerationProblemThede-signofthenoiseschedule,whichdeterminestheamountofnoiseaddedtothedataateachstep,hassignificantinfluencesonbothforwardandreverseprocesses.Intuitively,denoisingisamorechalleng-ingtaskforthemodelwithhigherlevelsofnoise,andbecomeseasierwheninsufficientcorruptionisapplied,wherethemodelcangeneratethecorrectembeddingswithoutdependingontheconditionandcontext.Asaconsequence,themodeltendstodegeneratetoatrivialsolution.Here,wepro-videin-depthanalysesofthisproblem.Westartbydefiningthedegeneratedmodel,whichdiscardstheconditioninginformationandgenerateseachembeddingbychoosingthenearestonesindepen-dently:Definition1.Foranoisedinputzt=[zt,1,zt,2,···,zt,n],theDegeneratedModelisde-finedasfdg(zt;x)=\"argminey∈eϕL(zt,i,ey)#ni,whereL(zt,i,ey)=∥zt,i−ey∥2−logpϕ(y|zt,i).Itcanbeprovedthatwheninsufficientnoiseisin-troducedduringtraining,thedenoisingmodeltendstofallasthedegeneratedmodeldefinedabove.Theorem1.Givenembeddingseϕ∼Nd×V(0,σeI),theprobabilityofthedegen-eratedmodelbeingaglobalminimumoftheobjectivefunctionLforθconvergesto1as¯β→0andd→∞.Weleavetheproofandillustrationsofthistheo-reminAppendixA.Thisphenomenoncouldbeverifiedquantita-tively.Toanalyzethecapacityofthedenoisingmodelateachnoiselevel,weevaluatetheBLEUscoreofˆz0generatedbythemodelatdifferenttimesteps.Toeliminatetheimpactofthenoise\f4668\n\n0.00.20.40.60.81.0t08162432BLEUOriginalW/ Noise Rescaling(a)0.00.20.40.60.81.0t0.00.20.40.60.81.0DGStLinearCosineSqrt(b)0.00.20.40.60.81.0t0.00.20.40.60.81.0tF=1F=2F=3F=4F=5F=6(c)Figure2:(a)BLEUscoreofmodelsfedwithpureGaussiannoisezTontheIWSLT14De-Endataset.Thevalueoftisnormalizedto[0,1].(b)DGStwithdifferentwidelyusedschedules.(c)Thesqrtschedulerescaledwithdifferentvaluesoftherescalingfactor.schedule,wefeedthemodelwithzT,i.e.,thepurenoise,ratherthanzt.AsillustratedinFig.2a,theBLEUscoredropsdramaticallyattswithlownoiselevels,andtherangewithlowscoresoccu-piesnearlyhalfoftheaxis.Inotherwords,themodeldegeneratessignificantlyandextensively,implyingthenoiselevelsbroughtbytheschedulearefarfrombeingsufficient.NoiseRescalingInmosttextgenerationtasks,adegeneratedmodelisundesirable,asitfailstomaintaincontextualcoh\n...[truncated]",
    "https://proceedings.neurips.cc/paper_files/paper/2023/file/7d866abba506e5a56335e4644ebe18f9-Paper-Conference.pdf": "SNIPPET: NeurIPS Proceedings. Search. Advances in Neural Information Processing Systems 36 (NeurIPS 2023). Edited by: A. Oh and T. Naumann and A. Globerson and K ...\n\nTITLE: (from PDF)\n\nBODY:\nAR-DIFFUSION: Auto-Regressive Diffusion Model for\nText Generation\n\nTong Wu1∗ † , Zhihao Fan2∗†, Xiao Liu3, Hai-Tao Zheng1,8‡, Yeyun Gong3‡, Yelong Shen4,\nJian Jiao5, Juntao Li6, Zhongyu Wei2, Jian Guo7‡, Nan Duan3‡, Weizhu Chen4‡\n1Shezhen International Graduate School, Tsinghua University, 2 Fudan University,\n3Microsoft Research Asia, 4Microsoft Azure AI, 5Microsoft,\n6Soochow University, 7IDEA Research, 8Pengcheng Laboratory\n{yegong, yeshe, nanduan, wzchen}@microsoft.com,\nzheng.haitao@sz.tsinghua.edu.cn, guojian@idea.edu.cn\n\nAbstract\n\nDiffusion models have gained significant attention in the realm of image generation\ndue to their exceptional performance. Their success has been recently expanded\nto text generation via generating all tokens within a sequence concurrently. How-\never, natural language exhibits a far more pronounced sequential dependency in\ncomparison to images, and the majority of existing language models are trained\nwith a left-to-right auto-regressive approach. To account for the inherent sequen-\ntial characteristic of natural language, we introduce Auto-Regressive Diffusion\n(AR-DIFFUSION). AR-DIFFUSION ensures that the generation of tokens on the\nright depends on the generated ones on the left, a mechanism achieved through\nemploying a dynamic number of denoising steps that vary based on token position.\nThis results in tokens on the left undergoing fewer denoising steps than those on\nthe right, thereby enabling them to generate earlier and subsequently influence\nthe generation of tokens on the right. In a series of experiments on various text\ngeneration tasks, including text summarization, machine translation, and common\nsense generation, AR-DIFFUSION clearly demonstrated its superiority over existing\ndiffusion language models and that it can be 100× ∼ 600× faster when achieving\ncomparable results. Our code is available at this https URL.\n\n1\n\nIntroduction\n\nText generation is a fundamental task within the field of natural language processing (NLP). Pre-\ntrained language models like GPT-4 [OpenAI, 2023], LLaMA [Touvron et al., 2023], and Alpaca\n[Taori et al., 2023] have garnered significant attention with their ability to generate fluent and human-\nlike textual content. These models utilize the auto-regressive (AR) Transformer decoders [Vaswani\net al., 2017] to emit generated tokens one-by-one in sequential order from left to right. By leveraging\nthe power of position dependency, AR models are able to enhance the naturalness, coherence, and\nadherence to human language conventions in the generated text [Brown et al., 2020].\n\nRecent studies have shown the remarkable performance of diffusion models in image generation [Ho\net al., 2020], motivating researchers to extend diffusion to text generation [Li et al., 2022a, Gong\net al., 2022, Dieleman et al., 2022, Yuan et al., 2022, Ye et al., 2023]. By introducing timestep, these\nmethods progressively regulate the interpolation between the original tokens and Gaussian noise, then\niteratively denoise for text generation. At each timestep, the diffusion-based text generator predicts\n\n∗Work done during an internship at Microsoft Research Asia.\n†These authors contributed equally to this work.\n‡Corresponding author.\n\n37th Conference on Neural Information Processing Systems (NeurIPS 2023).\n\n\fFigure 1: Model behaviors illustrated on a two-dimensional coordinate system, where the horizontal\naxis stands for the position and the vertical axis represents the diffusion timestep. In the inference\nstage, different models will behave differently. (a) For the typical Diffusion-LM [Li et al., 2022a],\neach token share the identical movement speed v(n1, ti, ti+1) = v(n2, ti, ti+1) = |ti+1 − ti|. (b) For\nAR from the perspective of diffusion models, the tokens have two states based on the degree of interpo-\nlation between the original tokens and Gaussian noise: to be decoded (at timestep t = T ) and already\ndecoded (at timestep t = 0). Specifically, we have v(n1, ti, ti+1) = 0 and v(n2, ti, ti+1) = T . (c)\nIn AR-DIFFUSION, (ne, te) is the coordinate of anchor point. Tokens in different positions exhibit\nvarying movement speeds, such as v(n1, ti, ti+1) > v(n2, ti, ti+1) when n1 < n2.\n\nall tokens simultaneously following Non-Auto-Regression (NAR) [Lewis et al., 2020, Qi et al., 2020,\n2021, Li et al., 2022b], leading to faster decoding speed compared to AR. However, it also inherits\nthe drawback of NAR, namely the sacrifice of inter-token position dependency [Li et al., 2022c] and\nthe drop of generation performance [Bao et al., 2021].\n\nTo conduct a comprehensive analysis, we introduce a two-dimensional coordinate system to track the\ndiffusion timestep of tokens f (·) positioned at various locations. As illustrated in Figure 1, the system\nassigns the token position n ∈ [1, N ] to the horizontal axis and the diffusion timestep t ∈ [0, T ] to\nthe vertical axis. Diffusion-LM [Li et al., 2022a], which is followed by existing diffusion-based text\ngeneration models, is shown in Figure 1(a). It assigns a uniform timestep t to all tokens. In contrast,\ntokens in the AR model depicted in Figure 1(b) exhibit distinct timesteps within a generation step (ti).\nFor instance, the already decoded token at position n1 has a timestep of 0, while the to-be-decoded\ntoken at position n2 has a timestep of T . This approach effectively captures the sequential dependency.\nMotivated by this observation, we introduce AR-DIFFUSION, an auto-regressive diffusion method,\nfor the disparity in token positions and the principle of sequential token identification.\n\nIn AR-DIFFUSION, we propose a multi-level diffusion strategy that includes both sentence-level\nand token-level diffusion. We randomly choose a sentence-level timestep t, and assign dynamic\nmovement speeds v(·) by determining position-sensitive token-level timestep f (n, t) for each token.\nThis enables tokens at the left of a sentence to undergo faster movement from random Gaussian noise\nto token embedding, while those at the right of the sentence experience slower movement to better\nutilize information from previously denoised tokens. During inference, to reduce the significant\nnumber of inference steps (e.g., 2,000) required in Diffusion-LM [Li et al., 2022a], SeqDiffSeq [Yuan\net al., 2022] and GENIE [Lin et al., 2023], we introduce a skipping mechanism that collaborates with\nthe multi-level diffusion strategy to accelerate the process.\n\nExperimental results across various text generation tasks, such as text summarization, machine\ntranslation, and common sense generation, have consistently demonstrated that AR-DIFFUSION\nsurpasses existing text diffusion models, including AR methods in terms of both quality and diversity.\nMoreover, our verification reveals that AR-DIFFUSION requires fewer resources during decoding\nwhile maintaining superior performance. It achieves 100× faster than SeqDiffSeq [Yuan et al., 2022]\nin machine translation and 600× faster than GENIE [Lin et al., 2023] in text summarization while\ndelivering comparable results. Furthermore, it demonstrates promising results even in a challenging\nscenario where decoding is limited to only two steps.\n\n2\n\n𝑡𝑖+1TT𝑡𝑖+1𝑡𝑖𝑡𝑖TimestepTimestepPositionPosition𝑓(𝑛2,𝑡𝑖+1)𝑓(𝑛2,𝑡𝑖)(a) Diffusion-LM(c) AR-DiffusionToken-Level Diffusion Timestep FunctionForward Diffusion ProcessReverse Diffusion Process(𝑛𝑒,𝑡𝑒)𝑛1𝑛2NN𝑛2T𝑡𝑖+1𝑡𝑖TimestepPosition(b) ARN𝑛2𝑛1𝒗𝒏𝟐,𝒕𝒊,𝒕𝒊+𝟏=|𝒕𝒊+𝟏−𝒕𝒊|𝒗𝒏𝟏,𝒕𝒊,𝒕𝒊+𝟏=|𝒕𝒊+𝟏−𝒕𝒊|𝑓(𝑛1,𝑡𝑖+1)𝑓(𝑛1,𝑡𝑖)𝑛1𝑓(𝑛2,𝑡𝑖+1)𝑓(𝑛2,𝑡𝑖)𝒗𝒏𝟐,𝒕𝒊,𝒕𝒊+𝟏=𝑻𝒗𝒏𝟏,𝒕𝒊,𝒕𝒊+𝟏=𝟎𝑓𝑛1,𝑡𝑖=𝑓(𝑛1,𝑡𝑖+1)𝑓(𝑛2,𝑡𝑖+1)𝑓(𝑛2,𝑡𝑖)𝒗(𝒏𝟐,𝒕𝒊,𝒕𝒊+𝟏)𝒗(𝒏𝟏,𝒕𝒊,𝒕𝒊+𝟏)𝑓(𝑛1,𝑡𝑖+1)𝑓(𝑛1,𝑡𝑖)\f2 Preliminary\n\n2.1 Conditional Generative Language Models\n\nIn the field of natural language generation, conditional generative models are commonly implemented\nusing either auto-regressive (AR) or non-auto-regressive (NAR) methods. In AR [Vaswani et al.,\n2017], tokens on the right are predicted based on visible left tokens. The likelihood is given by\npAR(y|x) = (cid:81)N\ni=1 p(yi|y1:i−1; x), where yi denotes the i-th token of y. On the other hand, NAR [Gu\net al., 2017] assumes conditional independence among tokens and generates them uniformly without\ndistinction during decoding, resulting in the likelihood pNAR(y|x) = (cid:81)N\ni=1 p(yi|x). This parallel gen-\neration approach is of lower quality compared to AR, although it offers a substantial speed advantage.\n\n2.2 Diffusion Models for Text Generation\n\nRecently, Li et al. [2022a] propose a natural language generation model based on the diffusion\nprocess, which is typically divided into a forward noising process and a reverse denoising process.\n\nSpecifically, the forward process is a fixed linear Gaussian model, which gradually perturbs the\nrandom variable z0 until it becomes the standard Gaussian distribution. This can be formalized as:\n(1)\ni=1 αi, and αi is a coefficient that monotonically decreases with timestep t, zt is the\n\nq(zt | z0; x) = N (zt;\n\n¯αtz0, (1 − ¯αt)I),\n\n√\n\nwhere, ¯αt = (cid:81)t\nlatent state at timestep t.\n\nThe reverse process is to initiate from standard Gaussian noise and progressively utilize the denoising\ntransition pθ(zt−1|zt; x) for generation.\n\npθ(zt−1 | zt; x) = N (cid:0)zt−1; µθ(zt, t; x), Σθ(zt, t; x)(cid:1),\n(2)\nwhere the mean µθ and variance Σθ are learned from the model. In particular, we follow Li et al.\n[2022a]’s approach of using predefined variance without trainable parameters.\n\nTo extend the continuous diffusion process to discrete text generation, Li et al. [2022a] introduce\nan additional Markov transition from the discrete tokens y to the latent variable z0. In practice, we\nadd an embedding step qϕ(z0|y) = N (z0; Emb(y), (1 − α0)I) in the forward process, and use a\ntrainable rounding step which is parametrized by pθ(y|z0; x) = (cid:81)N\n0; x) in the reverse\nprocess. In each timestep, we utilize an encoder-decoder model gθ(zt, t; x) to approximate z0 [Lin\net al., 2023] in a NAR manner and then estimate µθ(zt, t; x).\n\ni=1 pθ(yi|zi\n\nIn consequence, combined with maximizing the evidence lower bound (ELBO) of log pθ(y|x), our\ntraining objective of the conditional diffusion language model is:\n\n(cid:34)\n\nL = Eqϕ(z0:T |y)\n\n− log pθ(y | z0; x) +\n\nT\n(cid:88)\n\nt=1\n\n∥z0 − gθ(zt, t; x)∥2\n\n.\n\n(3)\n\n(cid:35)\n\n3 Methodology\n\n3.1 Multi-Level Diffusion\n\nIn the typical diffusion process, every token in the text sequence has the same diffusion timestep.\nIn order to leverage the sequential nature of language, we enable tokens to have different diffusion\ntimesteps during the forward and reverse pass. To accomplish this, we propose a multi-level diffusion\nstrategy that includes both sentence-level and token-level diffusion. Firstly, at the sentence-level, we\nfollow Diffusion-LM [Li et al., 2022a] to randomly select a timestep t. Secondly, at the token-level,\nwe incorporate positional information n ∈ [1, N ] based on the sentence-level timestep to regulate\nthe diffusion timestep for the current token. The procedure is illustrated as:\nzt = (cid:0)z1\n\nf (2,t), · · · , zN\n\nf (1,t), z2\n\nf (N,t)\n\n(4)\n\n(cid:1),\n\nwhere N is the given target sentence length, zt is the sentence representation at timestep4 t, zn\nf (n,t)\nis the latent representation for the n-th token at sentence-level timestep t, and f (n, t) is a token-level\n\n4Please note that if we talk about a “timestep” without explicitly indicating that it is for token-level, it should\n\nbe for sentence-level.\n\n3\n\n\ftimestep function that denotes the token-level diffusion timestep determined by token position n\nand sentence-level timestep t.\nWe visualize the token-level timestep (cid:0)n, f (n, t)(cid:1) onto a two-dimensional coordinate system as Fig-\nure 1 , which takes the token position as the horizontal axis and the sentence-level timestep as the\nvertical axis. Furthermore, to provide a more profound description of the characteristics of movement,\nwe define the speed of movement as the following equation.\n\nv(n, ti, ti+1) = f (n, ti+1) − f (n, ti),\n\n(5)\n\nwhere ti and ti+1 are the start and end sentence-level diffusion timesteps. It can be observed that\ntokens in Diffusion-LM share the same movement speed, while those in AR exhibit different speeds.\n\n3.2 Token-Level Diffusion with Dynamic Movement Speed\n\nBased on the speed of movement, we propose a fundamental principle, dynamic movement speed,\nfor designing the token-level diffusion timestep function f (n, t) to take advantage of AR in diffusion.\nSpecifically, elements on the left side of a sentence undergo higher movement speed from random\nGaussian noise to token embedding, while those on the right side experience lower movement speed,\nthereby they can be generated in the later sentence-level timestep and utilize information from\npreviously generated tokens more effectively.\n\nAlgorithm 1 Training Process of AR-DIFFUSION.\n\nInput: Dataset {(x, y)}, maximum timestep number T and maximum target length N .\nOutput: Optimized model parameters θ.\n\n1: Define an anchor point (ne, te)5.\n2: repeat\n3:\n4:\n\nSample (x, y) from the dataset and embed y into z0.\nSample a sentence-level timestep t from the interval [0, N + T ], then the start point is determined by the\nfollowing equation:\n\n(ns, ts) = (cid:0)clip(N − t, 0, N ), clip(t − N, 0, T )(cid:1)\n\n5:\n\nUse the point-slope linear function to determine the token-level timestep f (n, t) in position n:\n\nf (n, t) = clip(cid:0) te − ts\nne − ns\n\n(n − ns) + ts, 0, T (cid:1)\n\n6:\n7:\n\nSample zn\nf (n,t) for each n in different positions with Gaussian reparameterization.\nAccording to equation (3) and equation (9), employ gradient descent to optimize the objective:\n\n(cid:104)\n\nmin\nθ\n\n− log pθ(y | z0; x) +\n\nN\n(cid:88)\n\nn=1\n\n(cid:13)\n(cid:13)gθ(zn\n\nf (n,t), f (n, t); x) − z0\n\n2(cid:105)\n\n(cid:13)\n(cid:13)\n\n8: until converged\n\n(6)\n\n(7)\n\n(8)\n\nFollowing the guidance of the principle, we develop a token-level diffusion strategy with the\nlinear function, which is shown in Figure 1(c).\nIn particular, the procedure is illustrated in\nAlgorithm 1, where clip(x, min, max) function is to clamp all elements in x into the range\n[min, max]. Specifically, in the forward process of diffusion, the start point goes to the left from\n(N, 0) to (0, 0) along the horizontal axis and then moves up to (0, T ) along the vertical axis.\nTherefore, the entire range of sentence-level timestep is extended to [0, N + T ].\n\nIn the reverse diffusion process, the multi-level diffusion follows the formula:\n\n(cid:0)zt, t; x(cid:1) = gθ\n\ngθ\n\n(cid:16)(cid:0)z1\n\nf (1,t), f (1, t)(cid:1), (cid:0)z2\n\nf (2,t), f (2, t)(cid:1), · · · , (cid:0)zN\n\nf (N,t), f (N, t)(cid:1); x\n\n(cid:17)\n\n,\n\n(9)\n\nwhere gθ(zn\n\nf (n,t), f (n, t); x) denotes the n-th element.\n\n5In particular, the anchor point is set as (2 × N, T ) in our implementation. The impact of different choices\n\nof the anchor point is discussed in supplementary material E.\n\n4\n\n\f3.3\n\nInference with Skipping\n\nTy\n...[truncated]",
    "https://aclanthology.org/2024.naacl-long.2.pdf": "SNIPPET: https://aclanthology.org/2024.naacl-long/; DOI: Bib Export formats: BibTeX MODS XML EndNote; PDF: https://aclanthology.org/2024.naacl-long.pdf · PDF (full) Bib ...\n\nTITLE: (from PDF)\n\nBODY:\n22\nProceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics:\nHuman Language Technologies (Volume 1: Long Papers), pages 22–39\nJune 16-21, 2024 ©2024 Association for Computational Linguistics\n\nTextDiffusionModelwithEncoder-DecoderTransformersforSequence-to-SequenceGenerationHongyiYuan12∗,ZhengYuan2,ChuanqiTan2,FeiHuang2,SongfangHuang21TsinghuaUniversity,2AlibabaGroupyuanhy20@mails.tsinghua.edu.cn{yuanzheng.yuanzhen,chuanqi.tcq,f.huang,songfang.hsf}@alibaba-inc.comAbstractThediffusionmodel,anewgenerativemod-elingparadigm,hasachievedgreatsuccessinimage,audio,andvideogeneration.However,consideringthediscretecategoricalnatureofthetext,itisnottrivialtoextendcontinuousdiffusionmodelstonaturallanguage.Inthiswork,weproposeSeqDiffuSeq,atextdiffu-sionmodel,toapproachsequence-to-sequencetextgenerationwithanencoder-decoderTrans-formerarchitecture.Toimprovethegenerationperformance,SeqDiffuSeqisequippedwiththeself-conditioningtechniqueandournewlyproposedadaptivenoisescheduletechnique.Self-conditioningenablesSeqDiffuSeqtobet-terusethepredictedsequenceinformationdur-ingthegenerationprocess.Theadaptivenoiseschedulebalancesthedifficultyofdenoisingacrosstimestepsatthetokenlevel.Exper-imentresultsillustratetheimprovedperfor-manceonfivesequence-to-sequencegenerationtaskscomparedtootherdiffusion-basedmodelsregardingtextqualityandinferencetime.1IntroductionGenerativemodelingisdrawingmoreattentioninrecentyearsofmachinelearningresearchduetothedevelopmentofdiffusionmodels(Hoetal.,2020).Diffusionmodelsdefinetheforwardpro-cessandthereverseprocesswheretheformergrad-uallydiffusesdatatorandomnoisewhilethelatterrecoversdatafromrandomnoiseiteratively,whichhaveshownsuperiorperformanceonsynthesiz-ingimages(Rombachetal.,2021),audios(Kongetal.,2020),andvideos(Hoetal.,2022)overothergenerativemethods,suchasgenerativeadversar-ialnetwork(GAN)(Goodfellowetal.,2014)andnormalizingflow(Kobyzevetal.,2021).Itisnottrivialtoextenddiffusionmodelstothegenerationofnaturallanguages.Mostoftheex-istingdiffusionmodelsareappliedtocontinuous∗WorkdoneatAlibabaDAMOAcademy.Codesarere-leasedinhttps://github.com/Yuanhy1997/SeqDiffuSeqfeaturespace(Hoetal.,2020;NicholandDhari-wal,2021)whiletextsaresequencesofdiscretecategoricaltokens.Recently,researchhasexploredcategoricaldiffusionmodelsindiscretespacefortextgeneration(Hoogeboometal.,2021;Austinetal.,2022).TherealsoexistsresearchsuchasDif-fusionLM(Lietal.,2022)thatappliescontinuousdiffusionmodelstowordembedding.However,theseworksonlyfocusonunconditionalandcon-trolledtextgeneration.Sequence-to-sequencetextgenerationisafun-damentalnaturallanguageprocessingsettingandcoversvariouspracticaldownstreamtasks,suchasdialogue(Nietal.,2021)andmachinetranslation(Liuetal.,2020).Inrecentpractice,researchersresorttoauto-regressive(AR)(Daietal.,2019)ornon-auto-regressive(NAR)(Guetal.,2019)Transformersforthetasks,andachievegoodgen-erationperformance.Usingdiffusionmodels,arecentworknamedDiffuSeq(Gongetal.,2022)appliesthediffusion-basedmethodforsequence-to-sequencetextgeneration.Theydeployencoder-onlyTransformersandpartiallydefinediffusionanddenoisingprocessesonoutputsequences.Inthiswork,weexplorediffusionmodelswithencoder-decoderTransformerarchitectureforsequence-to-sequencegeneration.WeproposeSe-qDiffuSeqwhichextendsthecontinuousdiffusionframeworkproposedinDiffusionLM(Lietal.,2022)tosequence-to-sequencesettings.WeequipSeqDiffuSeqwiththeself-conditioningtechnique(Chenetal.,2022)andournewlyproposedadap-tivenoiseschedule.Self-conditioninghelpsthemodelbettercapturetheinformationfromformerit-erationsduringthegeneration.Theproposedadap-tivenoiseschedulelearnsatoken-levelnoisesched-uletobettercontroltheamountofnoiseinjectedandinformationrecoveredduringtheforwardandreverseprocess(NicholandDhariwal,2021).Weconductexperimentsonfivegenerationtasks.ResultsshowthatSeqDiffuSeqachievescompet-\f23\n\nitiveperformancecomparedwithARandNARbaselinesintermsofgenerationqualityanddiver-sity.SeqDiffuSeqalsoshowsimprovedgenera-tionperformanceandinferencespeedcomparedtotextdiffisonmodelDiffuSeq.Ablationstud-iesdemonstratethatourmodelcanbenefitfromself-conditioningandadaptivenoisescheduletech-niques,andbotharecomplementarytoeachotherinsequence-to-sequencesettings.Tosummarize,themaincontributionsofthisworkareasfollows:1.WeproposeSeqDiffuSeqthatextendsthecontinuoustextdiffusionmodeltosequence-to-sequencetextgenerationwithencoder-decoderTransformerarchitecture.2.Theself-conditioningandnewlyproposedadaptivenoisescheduletechniquecaneffec-tivelyimprovethegenerationqualityofthetextdiffusionmodel.3.ExperimentsshowSeqDiffuSeqachievespromisingperformancewiththepreviousdiffusion-basedmethodDiffuSeqaswellasARandNARmodelsonfivegenerationtasks.2RelatedWorkSincethegreatsuccessofdiffusionmodelsinvi-sion(Hoetal.,2020;Rombachetal.,2021;Songetal.,2021b),researchershaveexploredextend-ingdiffusionmodelstotextgeneration.Consid-eringthediscreteandcategoricalnatureoftexts,MultinomialDiffusion(Hoogeboometal.,2021)andD3PM(Austinetal.,2021)areproposedformodelingcategoricaldata.Theydefinediscretediffusionmodelsusingdiscretecategoricaltransi-tionsdirectlyontexts.DiffusionBERT(Heetal.,2022)followsD3PMandintroducespre-trainedmodelsforlanguagemodeling.Besides,recentresearchalsoexploresconvertingtextsintocon-tinuousfeaturestoadapttodiffusionmodels.BitDiffusion(Chenetal.,2022)encodesdiscretedataasbinarybitsandtreatsthesebinarybitsasrealnumberfeatures.Yuetal.(2022)isproposedtobuildtextdiffusionmodelsincontinuouslatentspace.DiffusionLM(Lietal.,2022)usesthewordembeddingspaceforcontinuousdiffusionmod-elsandintroducesauxiliarylossestoenablejointlearningofembeddingandnetworkparameters.FollowingDiffusionLM,recentresearchexploresimprovingtextgenerationquality(Strudeletal.,2022),andDiffuSeq(Gongetal.,2022)extendsittosequence-to-sequencesettings.ComparedtoDif-fuSeq,weproposeadifferentmodelarchitectureandself-conditioningandadaptivenoisescheduletechniquestoimprovesequence-to-sequencegen-erationperformance.Noiseschedulesindiffusionmodelscontrolthelevelofnoiseinjectedandthelevelofinformationrecoveredintheforwardandreverseprocessre-spectively.Previousresearchinvisionandtextsdemonstratesthatappropriatenoiseschedulede-signcanimprovethegenerationqualityperfor-manceofdiffusionmodels(NicholandDhariwal,2021;Lietal.,2022).Concurrently,Diffusion-BERT(Heetal.,2022)proposesaspindlesched-uleforlanguagemodeling,andCDCD(Dielemanetal.,2022)designsalearnednoisescheduleforlanguagemodelingandmachinetranslation.Dif-ferentfrombothconcurrentworks,SeqDiffuSeqisproposedwithatoken-levelnoiseschedulethatbalancesthedifficultyofdenoisingacrosstimesteps.Gaoetal.(2023)proposesDifformerandisorthogonaltoourwork.3PreliminaryDiffusionmodelisgenerallyformulatedbyade-signedforwarddiffusionprocessandalearntre-versedenoisingprocess.Intheforwarddiffusionprocess,samplesgraduallymixwithrandomnoise,whileinthereversedenoisingprocess,therandomnoiseisgraduallydenoisedtogeneratesyntheticsamples.Weadopttheforwardandreversepro-cessesproposedinDDPM(Hoetal.,2020).Fortheforwardprocess,givenasamplez0fromareal-worlddatadistributionq(z0).Ateachtimestept∈{1,2,···,T},anoisesampleztissam-pledfromzt∼q(zt|zt−1)=N(zt;√αtzt−1,(1−αt)I),whereαtcontrolthenoiseaddedattimestept.Inthisregard,whenTislargeenough,areal-worldsamplewillgraduallyandultimatelydiffusetoastandardGaussiannoisedistribution.Forthereverseprocess,thediffusionmodelusesalearntparameterizeddenoisingdistributionzt−1∼pθ(zt−1|zt)tograduallyrecoversamplesfromnoise.Thedenoisingdistributionisparame-terizedbyθandistofittheposteriordistributionq(zt−1|zt,z0)oftheforwardprocess.q(zt−1|zt,z0)=N(zt−1;˜µ(z0,zt),˜βtI).(1)\f24\n\nInthisequation,˜µ(z0,zt)=√¯αt−1βt1−¯αtz0+√αt(1−¯αt−1)1−¯αtzt,(2)¯αt=tYs=1αs,βt=1−αt,˜βt=1−¯αt−11−¯αtβt.(3)Withlearntdenoisingdistributionpθ,asyntheticreal-worldsamplez0canbegeneratedfrompurerandomnoisezTstep-by-step.4ApproachInthissection,wepresentthemaindesignofourproposedSeqDiffuSeqforsequence-to-sequencelanguagegeneration.TheoverviewofSeqDiffuSeqisdepictedinFigure1.Inthefollowingsections,theinputandoutputsequencesaredenotedaswxandwyrespectively.Forthei-thtokeninwy,thetokenisdenotedaswiy,where0<i≤nandnrepresentsthemaximumoutputsequencewordlength.Inordertoavoidlengthynotations,weomittheindicesreferringtodifferentdatasamples.4.1DiffusionModelForwardProcessTofitdiffusionmodelstosequence-to-sequencesettings,weextendthetextdiffusionmodel,DiffusionLM(Lietal.,2022).Inthesequence-to-sequencesetting,theforwardprocessgraduallychangesthetargetoutputse-quencewytorandomnoise.Diffusingwytopurerandomnoiseisindependentoftheinputsequencewx.Forthesequencewy,weuseanembeddingfunctiongϕtomapthewordtokenswiytocon-tinuouswordembeddinggϕ(wiy)∈Rd,wheredrepresentsthedimensionofembeddingandϕrepresentstheparametersofthewordembeddingfunction.Theembeddingforthesequencewyisdefinedbystackingthetokens’embeddingandisdenotedasgϕ(wy)∈Rn×d.Atthebeginningoftheforwardprocess,aMarkoviantransitionpa-rameterizedbyqϕ(z0|wy)=N(z0;gϕ(wy),β0I)isadded.Extendedbyqϕ(z0|wy),theforwardpro-cesscancontinuetodiffusecontinuousfeaturesofz0iteratively.Foreachtimestept,weapplythediffusiondistributionq(zt|zt−1)togetnoisiersam-ples.Ultimately,theoutputsequencewybecomeszTwhichisnearlypurerandomnoisefollowingstandardGaussiandistribution.ReverseProcessDiffusionmodelsgeneratethesyntheticsamplesbysuccessivelysamplingthede-noisingdistributioninthereverseprocess.Foreachtimesteptinthereverseprocess,alearntdenoisingdistributionpθparameterizedbyθgeneratessam-pleszt−1conditionedontheformernoisiersam-pleszt.Inthesequence-to-sequencesetting,thegeneratedsequencescorrelatetoinputsequences.Therefore,thedenoisingdistributionisaddition-allyconditionedontheinputsequencewx,andpθ=pθ(zt−1|zt,wx).AfterthereversedenoisingprocessreachesT=0,weroundeachcolumnofthegeneratedˆz0toitsnearestwordintheembed-dingspacebytheroundingdistribution˜pϕ(wy|ˆz0)togeneratethefinalwordsequences.TrainingObjectiveWeoptimizeθandembed-dingparametersbyminimizingthevariationalboundofthedatalog-likelihood:LVB=Eqϕ(z0:T,wx,wy)[logq(zT|z0)p(zT)+TXt=2logq(zt−1|z0,zt)pθ(zt−1|zt,wx)−logpθ(z0|z1,wx)+logqϕ(z0|wy)−log˜pϕ(wy|z0)],(4)Thetrainingobjectiveistonarrowdownthedis-crepancybetweenpθ(zt−1|zt,wx)andtheposte-riorq(zt−1|zt,z0)intheforwardprocess.Sinceq(zt−1|zt,z0)followstheformofGaussiandis-tribution,weparameterizethedenoisingdistribu-tionfollowingGaussiandistributionfamilyandpθ(zt−1|zt,wx)=N(zt−1;˜µθ(zt,wx,t),˜βtI),where˜µθ(zt,wx,t)=√¯αt−1βt1−¯αtz0θ(zt,wx,t)+√αt(1−¯αt−1)1−¯αtzt.(5)z0θ(zt,wx,t)isnamedthedenoisingfunctionandpredictstheestimatedoutputembeddingsequencesateachreversestept.ThenaccordingtodensityfunctionsqandpθfollowingGaussiandistribution,theobjectivecanbefurthersimplifiedas:Lsimple=Eqϕ(z0,wx,wy)[TXt=2Eq(zt|z0)∥z0θ(zt,wx,t)−z0∥2+∥˜µ(zT,z0)∥2+∥z0θ(z1,wx,1)−gϕ(wy)∥2−log˜pϕ(wy|z0)],(6)whereq(zt|z0)=N(zt;√¯αtz0,(1−¯αt)I)foreffi-cientsamplingofztduringtraining,andµT(z0)=√¯αTz0.WeleavethedetailedderivationtoAp-pendixB.Thetrainingobjectivebecomestofit\f25\n\nFigure1:TheoverviewofSeqDiffuSeqwithanencoder-decoderTransformersarchitecture.gϕ(wy)andthedenoisingfunctionz0θ(zt,wx,t),whichwecanmodelwithencoder-decoderTrans-formersarchitectures.Duringtraining,thesam-plingdistributionqϕcontainstrainableparame-tersofwordembedding.Wecanbackpropagatethroughthiswithreparameterizationtrick(KingmaandWelling,2013).DenoisingwithEncoder-DecoderFrameworkUnlikeDiffuSeq(Gongetal.,2022)usingencoder-onlyTransformerarchitecture,weproposeusinganencoder-decoderTransformersarchitecturetomodeltheinputandoutputtextsequences.Forz0θ(zt,wx,t),weusetheencodertoprocessthein-putsequenceswxandusethedecodertomodelthenoisyoutputsequencezt.Followingthepre-viouswork(Lietal.,2022),weinjecttimestepinformationtbyaddingtimestepembeddingtozt.Usingtheencoder-decoderarchitecturehascom-putationalconvenienceduringgenerationbecausetheinputsequenceswxonlyrequireoneforwardcomputationthroughtheencodernetworkduringthewholereverseprocess.Consideringthereverseprocessrequiresthousandsofiterationstogeneratetheoutputsequencesofhighquality,thesavingofcomputationalresourcescanbesignificant.Duringtrainingandgeneration,thefunctionz0θgeneratesdenoisedsamplesatthesequencelevel.Thereforemakingpredictionsfromthedenoisingfunctionz0θresemblesthenon-autoregressivenatu-rallanguagegeneration.Inthisregard,weuseade-coderwithfullattentionmatricesinsteadofcausalattentionmatricestomodelztatthesequencelevel.4.2Self-ConditioningAteachtimesteptinthereverseprocess,thedenoisingfunctionz0θ(zt,wx,t)makesoutputse-quencepredictionsbasedonthenoisiersamplezt.ztissampledfromtheformerdenoisingdis-tributionbymixingformersequencepredictionˆzt0=z0θ(zt+1,wx,t+1),zt+1andrandomnoise.Inthisregard,partoftheinformationcontainedintheformerpredictionˆzt0isdiscarded.Bit-Diffusion(Chenetal.,2022)proposedtheself-conditioningtechniquemitigatingthiswasteofinformationbyadditionallytakingformersequencepredictionsasinputs.Thedenoisingfunctionisformulatedasz0θ(zt,ˆzt0,wx,t).Self-conditioningmayenablethedenoisingfunctiontorefinetheformersequencepredictionsratherthanmakenewpredictionsfromscratch.Itisempiricallyverifiedthattheself-conditioningtechniquecanboosttheperformanceoftextdiffusionmodels(Strudeletal.,2022).TofitthetechniqueintotheTransformersmodel-ingofz0θinoursequence-to-sequencesetting,thesequencefeaturesˆzt0fromtheformerpredictionsareconcatenatedwithnoisiersequencefeaturesztontheembeddingdimension.Hence,thedi-mensionofinputfeaturesofTransformerdecoderbecomesn×2d.SincetheformersequencesattimesteptaresampledsuccessivelyfromTtotwhichiscomputational-tediousduringtrain-ing,wetakeanefficienttrainingscheme.Withhalfprobability,z0θ(zt,ˆzt0,wx,t)istrainedbyset-tingtheinputˆzt0to0.Otherwise,ˆzt0isfirstesti-matedbyz0θ(zt,0,wx,t)andthenisusedforself-conditioningtraining.Underthesecondcircum-stance,wedonotbackpropagatethroughthefirstforwardpropagateestimatedˆzt0.4.3AdaptiveNoiseScheduleInthedomainofvisionandaudio,thegeneratedsamplequality(NicholandDhariwal,2021)andlikelihoodestimation(Kingmaetal.,2021)maypotentiallybenefitfromdifferentappropriatetimeschedules.Previousresearchusesdifferentsimplefunctionssuchaslinearfunction(Hoetal.,2020)\f26\n\norcosinefunction(NicholandDhariwal,2021)ofαagainsttimestepttodesignnoiseschedules.Suchdesignsmayresultsinunbalanceddenoisingdifficultiesforeachstepandleadtounsatisfyinggenerationquality.Someworksproposedtoallevi-atethisproblembyimportancesampling(Lietal.,2022)orlossreweighing(Gongetal.,2022).Weproposeanoveladaptivenoisescheduleatthetoken-level.Firstly,weproposetoadaptivelyadjustthetimeschedulesduringtrainingtomakethedenoisingdifficultiesofz0θpredictingoutputsequenceincreaselinearlywithrespecttothetimestep.Secondly,weseparatelysetadaptivenoiseschedulefordifferenttokenpositions,unlikeprevi-oustextdiffusionresearchthatonlydesignsnoiseschedulesonthewholesequencelevel.Sincetheintrinsicfeaturesforembeddingsequencesaredif-ferentacrosstokenpositionswithin,w\n...[truncated]",
    "https://aclanthology.org/2023.findings-acl.721.pdf": "SNIPPET: (Tang et al., Findings 2023); Copy Citation: BibTeX. Markdown MODS XML Endnote More options… PDF: https://aclanthology.org/2023.findings-acl.721.pdf · PDF Cite ...\n\nTITLE: (from PDF)\n\nBODY:\n11359\nFindings of the Association for Computational Linguistics: ACL 2023, pages 11359–11386\nJuly 9-14, 2023 ©2023 Association for Computational Linguistics\n\nCanDiffusionModelAchieveBetterPerformanceinTextGeneration?BridgingtheGapbetweenTrainingandInference!ZechengTang∗PinzhengWang∗KeyanZhouJuntaoLi†ZiqiangCaoMinZhangInstituteofComputerScienceandTechnology,SoochowUniversity,China{zctang,pzwang,kyzhou123}@stu.suda.edu.cn;{ljt,zqcao,minzhang}@suda.edu.cnAbstractDiffusionmodelshavebeensuccessfullyadaptedtotextgenerationtasksbymappingthediscretetextintothecontinuousspace.How-ever,thereexistnonnegligiblegapsbetweentrainingandinference,owingtotheabsenceoftheforwardprocessduringinference.Thus,themodelonlypredictsbasedonthepreviouslygeneratedreversenoiseratherthanthenoisecomputedbytheforwardprocess.Besides,thewidely-useddownsamplingstrategyinspeed-inguptheinferencewillcausethemismatchofdiffusiontrajectoriesbetweentrainingandin-ference.Tounderstandandmitigatetheabovetwotypesoftraining-inferencediscrepancies,welaunchathoroughpreliminarystudy.Basedonourobservations,weproposetwosimpleyeteffectivemethodstobridgethegapsmentionedabove,namedDistancePenaltyandAdaptiveDecaySampling.Extensiveexperimentson6generationtasksconfirmthesuperiorityofourmethods,whichcanachieve100×→200×speedupwithbetterperformance.Ourcodeisavailableathttps://github.com/CODINNLG/Bridge_Gap_Diffusion.1IntroductionWiththeprevalenceofAIGC(ArtificialIntelli-genceGeneratedContent)inrecentyears,genera-tivemodels(KingmaandWelling,2013;Goodfel-lowetal.,2020)havebeenreceivingmoreattention.Asoneoftherepresentativegenerativemodels,dif-fusionmodels(Sohl-Dicksteinetal.,2015;Songetal.,2020)haveachievedgreatsuccessonmyri-adsofgenerationtaskswithcontinuousdata,suchasimage(Songetal.,2020;Rameshetal.,2022;Rombachetal.,2022),audiogeneration(Kongetal.,2020),andmoleculegeneration(Hoogeboometal.,2022),byiterativelyrefiningtheinputnoisetomatchadatadistribution.Morerecently,diffu-sionmodelshavebeensuccessfullyadaptedtotext∗Equalcontribution.†CorrespondingAuthor.Figure1:Overviewofdiffusionmodelfortextgenera-tion,whereztdenotestheintermediatenoiseatstept.generation(Lietal.,2022;Gongetal.,2022;Linetal.,2022)byfirstleveraginganextraembeddingmodulethatmapsthediscretedataintothecontin-uousspaceandthenrecoveringthetextfromthecontinuousspacewithroundingstrategy(Lietal.,2022)orlogitsprojection(Strudeletal.,2022).Atypicaldiffusion-basedtextgenerationmodelcontainsonereverseprocess(fromnoisetodata)andoneforwardprocess(fromdatatonoise),whichisshowninFigure1.Moreconcretely,bothofthetwoprocessescanbeviewedasMarkovchains,wheretheforwardprocessgraduallyper-turbsthedataintoGaussianNoisewhilethere-verseprocessrecoverstheoriginaldatastepbystepconditionedonthecorrelatednoisefromthefor-wardprocess.Thetrainingstageinvolvesbothoftheabovetwoprocesses,whiletheinferencestageonlyconsistsofthereverseprocess,i.e.,themodelpredictsbasedonthepreviousnoiseoutputtedbythemodelitselfratherthanthecorrelatedforwardnoise.Suchdiscrepancybetweentrainingandin-ference,alsocalledexposurebias(Ranzatoetal.,2015),leadstoerroraccumulationasthedenois-ingstepsgrowduringtheinferencestage(Huszár,2015;WisemanandRush,2016).Anotherdrawbackofthediffusionmodelisthatitrequiresmultipleiterativedenoisingstepstopro-ducethefinalresultssincethereverseprocessshouldapproximatetheforwardprocess(Hoetal.,2020),whichusuallyinvolvesthousandsofsteps.Numerousiterativereversestepsofdiffusionmod-elsareinevitablytime-consumingfortextgenera-\f11360\n\ntion.Forinstance,adiffusionmodeltakesaround12hoursononesingleNVIDIAA100GPUtofin-ishtheinferenceof10Ksentenceswithalengthof128whiletheCMLM-basednon-autoregressivemodel(Ghazvininejadetal.,2019)onlytakesafewminutes1.Toacceleratetheinferencespeedintextgeneration,downsampling(NicholandDhari-wal,2021)isleveraged(Lietal.,2022;Gaoetal.,2022;Gongetal.,2022),thoughmuchfasterbutatthecostofperformanceowingtothegapbetweenthedownsampledstepsininferenceandthefulldiffusiontrajectoryinthetrainingstage.Toexploretheinsightsandthepotentialimprove-mentoftheaforementionedtraining-inferencegaps,weconductapreliminarystudywithadiffusionmodel(Gongetal.,2022)onthestorygenera-tiontaskandmainlyobservethat:(1)injectingthenoisegeneratedbythemodelitselfintothetrainingstagecanimprovethemodelperformance,and(2)theuniformdownsamplingstrategyintheinferencethattreatseachstepequallyimpairsthemodelperformance,andadaptivesamplingstrategyshouldbeappliedfordifferentgenerationstages.Accordingly,weproposetwosimpleyeteffectivestrategies:DistancePenaltyandAdaptiveDecaySampling,tobridgethetraining-inferencegapsandacceleratetheinferenceprocess.Experimentson6generationtasksof3differentsettings(directed,open-ended,andcontrollable)showthesuperior-ityofourmethodswithoutchangingtheoriginalarchitectureofthediffusionmodeloraddingmoreparameters.Surprisingly,ourmethodscanachieve100×speedupwithperformanceimprovementor200×accelerationwithcompetitiveresults.2Background2.1DiffusionModelDiffusionmodelsareoneoftheprevalentgener-ativemodels(Sohl-Dicksteinetal.,2015;Songetal.,2020;NicholandDhariwal,2021),whichcantransferanarbitrarydatadistributionintotheGaussiannoisewiththeforwardprocessandre-coverthedatafromthepurenoisewiththereverseprocessandbothtwoprocessescanberegardedasaMarkovchain.Specifically,giventhetimestepsT={0,1,···,T}andtheoriginaldatadis-tributionz0attimestept=0,theforwardpro-cessgraduallyperturbsitintotheGaussiannoise1BothdiffusionmodelandCMLMmodelsharethesamebackbonemodel,i.e.,Transformer(Vaswanietal.,2017).zT∼N(0,I)attimestept=T:q(zt|zt−1)=N(zt;p1−βtzt−1,βtI),(1)whereztrepresentstheintermediatenoiseattimesteptandβt∈(0,1)isthescalingfactor,control-lingtheamountofaddednoiseattimestept.Thereversediffusionprocessrecoverstheinitialdatadistributionz0fromtheGaussiannoisezTbypredictingthenoiseofcurrenttimesteptanddenoisingitintothenextreversestatezt−1:pθ(zt−1|zt)=N(zt−1;µθ(zt,t),Σθ(zt,t)),(2)whereµθandΣθcanbeimplementedbyneuralnetworksfθ,e.g.,Transformer2:µθ(zt,t)=1√αt(zt−βt√1−¯αtfθ(zt,t)),(3)whereαt=1−βtand¯αt=Qti=1αi.TrainingThetrainingobjectiveofthediffusionmodelistomaximizethemarginallikelihoodofdatalogpθ(z0),andthesimplifiedtrainingobjec-tivecanbewrittenas(Hoetal.,2020):Lsimple=TXt=1Eq(zt|z0)||µθ(zt,t)−ˆµ(zt,z0)||2,(4)whereˆµ(zt,z0)isthemeanofq(zt−1|z0,zt),anditisworthnotingthateachintermediatenoiseztcanbeobtaineddirectlywithouttheprevioushistoryduringthetrainingstage(Equation12).InferenceTheinferencestageonlyconsistsofthereverseprocess.Tosamplezt−1∼pθ(zt−1|zt)inEquation2,reparameterizationstrategy(KingmaandWelling,2013)isleveraged:zt−1=µθ(zt,t)+σtϵ,(5)whereϵ∼N(0,I),σ2t=βt,andztisinitializedwithpureGaussiannoiseinthebeginning.MoredetailsaboutthetrainingandinferencestagesaswellasthederivationsareshowninAppendixA.2.2DiffusionModelforTextGenerationThecoreofapplyingdiffusionmodelsfortextgen-erationtaskisthetransitionbetweendiscretespaceandcontinuousspace.Existingworksmainlyintro-ducetheembeddingfunction(Lietal.,2022)E(·)tomapthediscretetextw={w1,w2,···,wL}2Σθisoftensetasσ2tI(Hoetal.,2020),whereσ2t=βt.\f11361\n\noflengthLintothecontinuousspaceE(w)={E(w1),E(w2),···,E(wL)}∈RLd.Thus,thediffusionmodelcanhandlediscretetextgenera-tionbyaddinganextraforwardstepbeforet=0,denotedasq(z0|w)=N(E(w),σ0I),andan-otherstepattheendofthereverseprocess,i.e.,pθ(w|z0).MoredetailsaregiveninAppendixB.2.3InferenceSpeedupOnecriticalpointthatpreventstheusabilityofdiffusionmodelsintextgenerationistheirslowsamplingspeedduringinferenceduetothelongreversetrajectory,whichmakeseachdiffusionstepsimpleandeasytoestimate(Sohl-Dicksteinetal.,2015).Toacceleratetheinferencespeedintextgenerationtasks,currentworks(Lietal.,2022;Gaoetal.,2022)applythedownsamplingstrat-egy(NicholandDhariwal,2021)thatpicksthesubsetT′={t′1,t′2,···,t′k}fromthefulldiffu-siontrajectoryandeachintermediatereversestepcanbeobtainedby:z′t−1=µθ(z′t,t′)+σ′tϵ.3GapsbetweenTrainingandInferenceFromtheabovedescriptionofdiffusionmodels,wecansummarizetwogaps:(1)thereverseprocessattimesteptininferenceisconditionedonthepre-dictednoisezt+1bythemodelitselfwhilezt+1canbeobtaineddirectlywiththeforwardcomputationq(zt+1|z0)duringtraining,and(2)thedown-sampledtimesubsetT′ininferenceisinconsistentwiththefulldiffusiontrajectoryTintrainingstagewhenapplyingthedownsamplingmethodforinfer-encespeedup.Tocalibratetheeffectsofthesetwotypesoftraining-inferencegaps,welaunchastudyonthestorygenerationtaskinthissection.3.1StudySettingsWeimplementthediffusionmodelwiththetrans-formermodelandselecttheROCStories(ROC)corpus(Mostafazadehetal.,2016)forthestorygenerationtask.Specifically,giventhepromptorthesourcesentencewxandthereferencewy,weapplythepartiallynoisingstrategy(Gongetal.,2022)fortraining(AppendixA).WeutilizeBLEU(B-2)score(Papinenietal.,2002)toreflectthegenerationprecision(thehigher,thebetter),Lexi-calRepetition(LR-2)score(Shaoetal.,2019)toshowthediversityoftext(thelower,thebetter),ROUGE(R-2)torepresenttherecallofgenerationresult(thehigher,thebetter)andPerplexity(PPL)toreflectsthefluency(thelower,thebetter).More(a)B-2scores.(b)LR-2scores.(c)PPLscores.Figure2:Evaluationresultsofnoiseinjection,wherethenumberinabscissarepresentsγ2andγ1=1−γ2.implementationdetailsareinAppendixC.3.2AnalysisTrainingwithPredictedNoiseTomitigatethetraining-inferencegap,itisnaturaltoinjectpartofthepredictednoisesintothetrainingstagebyreplacingtheforwardnoisezt+1inpθ(zt|zt+1)withthepredictednoisez′t+1fromthe(t+1)-thstepofthereverseprocessorinjectingthepredictednoiseintoztbyreplacing||µθ(zt,t)−ˆµ(zt,z0)||2inEquation4withγ1||µθ(zt,t)−ˆµ(zt,z0)||2+γ2||µθ(zt,t)−ˆµ(z′t,t)||2,wherezt∼q(zt|z0)andz′t∼pθ(zt|z′t+1).Wereporttheevaluationre-sultsinFigure2withdifferentsettingsofγ1andγ2andcanmainlyobservethatreplacingtheforwardnoisewiththepredictednoise(γ2=1,γ1=0)doesmitigatethetraining-inferencegapbyachiev-ingabetterperformancethanthevanillatrainingscheme(γ2=0,γ1=1),andtheinjectingstrat-egyperformsbetterthanthereplacingone.Moredetailsaboutnoisereplacementoperationandeval-uationresultsareshowninAppendixD.1.SamplingStrategyDownsamplingcanacceler-atetheinferencebyuniformlyselectingthesubsetsT′fromthefulldiffusiontrajectoryTbutatthecostofperformance.Suchauniformsamplingstrategytreatseachreversestepequallywhilene-glectingthediscrepanciesamongthemincontri-butiontothefinalresult.Toexplorewhethersuchanequal-stepsamplingstrategybringstheperfor-mancedecrease,wesimplycomparedifferentnon-uniformsamplingschemes.Alongwiththereversesteps,wesplitthereverseprocessintothreestages[κ1,κ2,κ3]anddownsampledifferentnumbersofstepsforeachstagebutkeepthetotaldownsam-pledstepsthesame3.AsshowninFigure3,wecanobservethatwhendownsamplingmorestepsfromκ1(orangecurve),themodelcanachieve3Fortotalnumberofdownsampledsteps20,wecansample{[12,4,4],[4,12,4],[4,4,12],[8,4,8]}stepsas[κ1,κ2,κ3].\f11362\n\n(a)B-2ofnon-uniformsampling.(b)R-2ofnon-uniformsampling.Figure3:Comparisonbetweennon-uniformstepsof[κ1,κ2,κ3]andtheoriginaluniformscheme,wherethex-axisrepresentsthedenoisingsteps,andy-axisillus-tratestheevaluationresultsforeachmetric.Thedotsoneachcurveindicatethenumberofdown-sampledsteps.abetterperformancethanotherdownsamplingschemes(greencurve,redcurve,andpurplecurve)andevenexceedtheoriginalfullreversesteps(bluecurve).Inotherwords,theequal-stepuniformdownsamplingschemedoeslimitthemodelcapa-bility,andthesimplenon-uniformdownsamplingstrategycanmitigatesuchissueandmeanwhileacceleratetheinferencespeed.ExtensiveTrialsAsmentionedabove,thegapbroughtbythedifferentdiffusiontrajectoriesintheinferencestage,i.e.,downsampledreversestepsv.s.thefullreversesteps,furtheraggravatesthetraining-inferencediscrepancy.Inviewthatsim-plyinjectingthepredictedreversenoiseintrainingcaneffectivelynarrowthegapsbetweentrainingandinference,itisalsoappealingtomakesuchastrategyadapttothedownsampleddiffusiontra-jectories,i.e.,introducingthedownsampledre-versenoisesinthetrainingstage.Forinstance,wecaninjectthepredictedreversenoisedownsam-pledfromthereversestepsof(t,t+δ]intothed-th(d∼(t,t+δ])forwardnoisetocomputethet-thstepreversenoise,i.e.,replacingtheforwardnoisezt+1inpθ(zt|zt+1)withzd∼(t,t+δ].Intuitively,addingaperturbationwithareason-ablerangeofvaluesintrainingcanmakethemodelmorerobusttowardstheperturbationduringinfer-ence,whileanunconstrainedperturbationvaluemightriskthemodeltraining,e.g.,thetrainingcollapseinauto-regressivetextgenerationmod-Figure4:Euclideandistancebetweenthepredictedre-versenoiseandtheforwardnoiseofaconvergedmodel.els(Zhangetal.,2019b).Forourpurposes,thediscrepancybeforeandafterinjectingthedown-sampledreversenoiseineachtrainingstepshouldfallinarationalrange,whichmainlydependsonthetimesteptandthechoiceofδ.Toexploremoreinsights,wedepictthediscrepancybetweenpre-dictedreversenoisesandforwardnoisesalongwith200randomlyselectedcontinuoustimestepswiththeEuclideandistance,whichisconsistentwiththetrainingobjectiveinEquation4.Tosimplifythestudyexperiment,wedownsampleatimestepforeverytwentysteps4.AsshowninFigure4,wecanobservethat(1)thediscrepancybetweenpre-dictedreversenoisesandforwardnoisesisgettinglargeralongwiththeincreaseoftimestept(reddiagonalarrow),and(2)thedifferencesbetweentheforwardnoiseattimesteptandthepredictedreversenoisefromttot+δarebecominglargeralongwiththeincreaseoftimestep(yellowhor-izontalarrow).Thus,therangeofdownsampledreversenoisestepsshouldbegraduallynarrowedalongwiththeincreaseoftimestep.3.3PotentialImprovementBasedontheanalysismentionedabove,wecanconcludethat:(1)injectingthepredictedreversenoiseintothetrainingstagecanmitigatethetraining-inferencegaps,(2)theschemeofuniformdownsamplingininferencewhichtreatseachstepequallyharmsthemodelperformance,andanon-uniformadaptivemethodshouldbedesigned,and(3)inspiredby(1)and(2),wecaninjectthedown-sampledreversenoisesintothetrainingstagewhiletherangeofdownsampledstepsshouldbegradu-4Weutilizethediffusionmodeltrainedwith240Ksteps.MoreimplementationdetailsareshowninAppendixD.2\f11363\n\nallynarrowedasthetimestepincreases.4MethodWeproposetwosimpleyeteffectivemethods:Dis-tancePenaltyinthepost-trainingstageandAdap-tiveSparseSamplingininferencetobridgethegapswithoutintroducinganyarchitecturemodifi-cationtodiffusionmodels.Thus,itcanbeflexiblyadaptedtodifferentdiffusionmodelvariants.4.1DistancePenaltyWefirstintroducetheDistancePenaltystrategy,whichinjectstheDownsampledpredictedreversenoiseintothepost-trainingstageofdiffusionmod-elsthatconsistsofTtimesteps.5Forbetterillustra-tion,weutilizenewsymbolsK={0,1,···,K}forthetimestepsinthepost-trainingstagetodis-tinguishfromtheoriginaldiffusiontrajectoryTinthetrainingstage.TheoverviewoftheDistancePenaltystrategyisshowninFigure5.DownsamplingRangeinTrainingToobtainarationalpredictedreversenoiseforeachstepk,i.e.,conductthedownsamplingoperationintherangeRk={k−1,···,k−h},andmitigatethetraining-inferencegaps,weconstrainthetotalamountofnoisesinRkwiththethresholdωkadj:ωkadj=√1−¯αKk′,(6)where√1−¯αKdenotesthescalingfactorthatcontrolsthevarianceofnoiseaccumulatedatstepK(AppendixA),andk′isthenumberofthepre\n...[truncated]",
    "https://blog.genlaw.org/pdfs/genlaw_icml2024/42.pdf": "SNIPPET: Abstract The EU Artificial Intelligence Act (AIA) estab-lishes legal principles for certain types of AI sys-tems. While prior work has sought to clarify some of these principles, little attention has been paid to robustness and cybersecurity. This pa-per aims to fill this gap. We identify legal chal-lenges in provisions related to robustness and cy-bersecurity for high-risk AI systems (Art. 15 ...\n\nTITLE: (from PDF)\n\nBODY:\nDiffusion Unlearning Optimization for Robust and Safe Text-to-Image Models\n\nYong-Hyun Park 1 Sangdoo Yun 2 3 Jin-Hwa Kim 2 3 Junho Kim 2 Geonhui Jang 4 Yonghyun Jeong 5 6\nJunghyo Jo 1 Gayoung Lee 2\n\n1. Introduction\n\nRecently, as the performance of text-to-image models (Rom-\nbach et al., 2022; Ho et al., 2020) has significantly improved,\nthere have been many concerns about their negative social\nimpact. For example, these models can be used to gener-\nate explicit or violent images, and often create copyrighted\nimages. Blocking inappropriate content with classifiers is\none available approach, but attackers can bypass this by us-\ning the publicly available model weights. This poses a risk\nfor many services and companies that publish their model\nweights, ultimately hindering the advancement of T2I model\nresearch.\n\nTo solve this problem, recent studies (Gandikota et al., 2023;\nKumari et al., 2023; Zhang et al., 2023a; Heng & Soh, 2024;\nGandikota et al., 2024) have aimed to remove unwanted con-\ncepts from the models. While removing target concepts, it is\ndesired that the performance on non-target concepts remains\nas close to the original model as possible. Existing stud-\nies mainly adopted methods to block the flow of prompts\ncontaining unsafe keywords within the model. Since the\nblocking is only applied to specific prompts, it has the ad-\nvantage of preserving non-target concepts. However, they\nhave the disadvantage of being vulnerable to adversarial\nprompt attacks as shown in Figure 1. Recent studies (Tsai\net al., 2023; Pham et al., 2023; Yang et al., 2024) have shown\nthat adversarial attacks using prompts are possible even in\nblack-box scenarios. This demonstrates that visual features\nthemselves need to be unlearned to prevent vulnerability to\nsuch prompt attacks.\n\nWe propose a method to prevent the model from creating\nunsafe visual features regardless of the prompt. This dif-\nfers from existing methods that shallowly block information\nconveyed from the prompt. The biggest challenge is that\n\n1Department of Physics Education, Seoul National University\n2NAVER AI Lab 3AI Institute of Seoul National University or SNU\nAIIS 4School of Industrial and Management Engineering, Korea\nUniversity 5NAVER Cloud 6Korea Institute for Advanced Study\n(KIAS). Correspondence to: Junghyo Jo <jojunghyo@snu.ac.kr>,\nGayoung Lee <gayoung.lee@navercorp.com>.\n\nProceedings of the 41 st International Conference on Machine\nLearning, Vienna, Austria. PMLR 235, 2024. Copyright 2024 by\nthe author(s).\n\nFigure 1. Task Visualization: Existing prompt-based unlearning\nmethods primarily focused on unlearning the prompt’s embedding\nor the dependent cross-attention layers. While these methods do\nnot compromise the quality of unrelated topic images, they have\nthe drawback of being vulnerable to adversarial prompts.\n\nunlearning visual features may reduce the image generation\nquality for unrelated topics. To prevent this, we propose a\nmethod to precisely guide the model to forget only the un-\nwanted concepts. First, we used SDEdit to generate paired\nground truth images that remove the unsafe concepts from\nimages containing these concepts. Using paired data for\nsupervised learning is common in prompt-based unlearn-\ning methods. However, since we aim for the model to not\ngenerate the unsafe visual features regardless of the prompt,\nsupervised learning is not suitable. Therefore, we used the\nDirect Preference Optimization (DPO) method to guide the\nmodel to prefer generating the paired ground truth images\nover the images containing unsafe concepts.\n\nWe demonstrate that using paired data for preference op-\ntimization is effective in selectively unlearning visual fea-\ntures. To this end, we show that our method is robust against\nadversarial prompt attacks, which existing prompt-based un-\nlearning methods are vulnerable to.\n\n2. Related Work\n\nRecently, there has been active research on safety mech-\nanisms to prevent text-to-image models from generating\nimages with unwanted concepts. One prominent approach\nis fine-tuning-based unlearning, which is advantageous as\nit avoids the need for training from scratch. Notable works\nlike ESD (Gandikota et al., 2023), CA (Kumari et al., 2023),\nUCE (Gandikota et al., 2024), Forget-me-not (Zhang et al.,\n2023a), and SA (Heng & Soh, 2024) have developed meth-\nods to handle unsafe prompts during training.\n\n1\n\nA naked woman in the forest Adversarial promptnoisesafeunsafeModel\fDiffusion Unlearning Optimization for Robust and Safe Text-to-Image Models\n\nFigure 2. To guide the model to unlearn only the desired concept\nwithout losing the ability to generate unrelated concepts, we gener-\nate synthetic paired data using SDEdit. We use\nfor publication\npurposes.\n\nFigure 3. Qualitative results on nudity attacked by Ring-A-Bell\nfor publication purposes.\nmethod. We use\n\nHowever, these strategies are often susceptible to adversarial\nprompt attacks (Tsai et al., 2023; Pham et al., 2023; Yang\net al., 2024; Chin et al., 2023; Zhang et al., 2023b; Han\net al., 2024). Our research aims to develop a robust safety\nmechanism that can withstand red teaming efforts.\n\n3. Method\n\nWe aim to remove visual features associated with unsafe con-\ncepts from the model. Although we experimented with dif-\nfusion models, we believe that this approach can be broadly\napplied to other image synthesis methods as well. The loss\nfunction of the diffusion model (Ho et al., 2020) is com-\nmonly expressed as follows:\n\nLDSM = Ex0∼q(x0),xt∼q(xt|x0)[||ϵ − ϵθ(xt)||2\n2]\n\n(1)\n\nWhere x0 is an image and xt is a noisy image sampled from\nq(xt|x0) = N (\n1 − αtI). ϵθ(·) is the model that\npredicts the added noise ϵ.\n\nαtx0,\n\n√\n\n√\n\nDirect Preference Optimization (DPO) (Rafailov et al.,\n2024), which is a type of preference optimization, has been\nstudied for application to diffusion models in previous re-\nsearch (Wallace et al., 2023). The final equation in this\npaper is summarized as follows. For detailed derivation,\nplease refer to the referenced paper.\n\nLDiffusion-DPO ≤ − E[log σ(−βT ω(λt)(\n\ndpref − ddispref ))]\n\ndpref = ∥ϵ − ϵθ(x+\n\nt , t)∥2\n\n2 − ∥ϵ − ϵϕ(x+\n\nt , t)∥2\n2\n\nddispref = (cid:0)∥ϵ − ϵθ(x−\n\nt , t)∥2\n\n2 + ∥ϵ − ϵϕ(x−\n\nt , t)∥2\n2\n\n(2)\n\n(3)\n\n(cid:1)\n\n(4)\n\nt is a prefered noisy image and x−\n\nWhere x+\nt is an unprefered\nnoisy image. ϵϕ denotes the pretrained model and ϵθ is the\nfine-tuned model.\n\n2\n\nFigure 4. Quantitative results on nudity. With same prior preserva-\ntion score, our method is more robust on (left) Ring-A-Bell and\n(right) Concept Inversion.\n\nBy using this method, we can increase the probability that\nthe diffusion model generates preferred images while grad-\nually decreasing the probability of generating dispreferred\nimages. We apply this to the unlearning task by replacing\ndispreferred images with unsafe images and preferred im-\nages with safe images. To generate these safe and unsafe\npairs, we used SDEdit (Meng et al., 2021) to create syn-\nthetic paired data, as shown in Figure 2. Additionally, we\ndiscovered a trick to help the model more reliably maintain\nits prior. This involves ensuring that the noise predicted by\nthe two models is similar for complete noise. We added the\nfollowing term to the loss function for optimization.\n\nLprior = ||ϵϕ(xT ) − ϵθ(xT )||2\n2\n\n(5)\n\n4. Experiments\n\nWe conducted unlearning for nudity and applied two types\nof red-teaming attacks: Ring-A-Bell (Tsai et al., 2023)\nand Concept Inversion (Pham et al., 2023). To generate\nthe dataset, we used prompts containing ‘naked’ as un-\nsafe prompts and replaced them with prompts containing\n‘dressed’ to create paired sets. Using these, we generated 64\npairs of images for the unlearning training.\n\nCompared to ESD (Gandikota et al., 2023), UCE (Gandikota\net al., 2024), and SPM (Lyu et al., 2023) unlearning methods,\nour approach demonstrated significantly more robustness\nagainst prompt attacks. We presented qualitative results in\n\n******SD1.4vESDUCESPMOurs\fDiffusion Unlearning Optimization for Robust and Safe Text-to-Image Models\n\nPham, M., Marshall, K. O., Cohen, N., Mittal, G., and\nHegde, C. Circumventing concept erasure methods for\ntext-to-image generative models. In The Twelfth Interna-\ntional Conference on Learning Representations, 2023.\n\nRafailov, R., Sharma, A., Mitchell, E., Manning, C. D., Er-\nmon, S., and Finn, C. Direct preference optimization:\nYour language model is secretly a reward model. Ad-\nvances in Neural Information Processing Systems, 36,\n2024.\n\nRombach, R., Blattmann, A., Lorenz, D., Esser, P., and\nOmmer, B. High-resolution image synthesis with latent\ndiffusion models. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pp.\n10684–10695, 2022.\n\nTsai, Y.-L., Hsu, C.-Y., Xie, C., Lin, C.-H., Chen, J.-Y., Li,\nB., Chen, P.-Y., Yu, C.-M., and Huang, C.-Y. Ring-a-bell!\nhow reliable are concept removal methods for diffusion\nmodels? arXiv preprint arXiv:2310.10012, 2023.\n\nWallace, B., Dang, M., Rafailov, R., Zhou, L., Lou, A., Pu-\nrushwalkam, S., Ermon, S., Xiong, C., Joty, S., and Naik,\nN. Diffusion model alignment using direct preference\noptimization. arXiv preprint arXiv:2311.12908, 2023.\n\nYang, Y., Hui, B., Yuan, H., Gong, N., and Cao, Y.\nSneakyprompt: Jailbreaking text-to-image generative\nmodels. In 2024 IEEE Symposium on Security and Pri-\nvacy (SP), pp. 123–123. IEEE Computer Society, 2024.\n\nZhang, E., Wang, K., Xu, X., Wang, Z., and Shi, H. Forget-\nme-not: Learning to forget in text-to-image diffusion\nmodels. arXiv preprint arXiv:2303.17591, 2023a.\n\nZhang, R., Isola, P., Efros, A. A., Shechtman, E., and Wang,\nO. The unreasonable effectiveness of deep features as a\nperceptual metric. In Proceedings of the IEEE conference\non computer vision and pattern recognition, pp. 586–595,\n2018.\n\nZhang, Y., Jia, J., Chen, X., Chen, A., Zhang, Y., Liu, J.,\nDing, K., and Liu, S. To generate or not? safety-driven\nunlearned diffusion models are still easy to generate un-\nsafe images... for now. arXiv preprint arXiv:2310.11868,\n2023b.\n\nFigure 3. For quantitative evaluation, we used NudeNet (Be-\ndapudi, 2019) to determine the defense success rate and\ncalculated LPIPS score (Zhang et al., 2018) for unrelated\ntopics as a prior preservation score. The results are pre-\nsented in Figure 4.\n\nReferences\n\nBedapudi, P.\ntection.\nNudeNet/, 2019.\n\nNudenet:\n\nlightweight nudity de-\nhttps://github.com/notAI-tech/\n\nChin, Z.-Y., Jiang, C.-M., Huang, C.-C., Chen, P.-Y., and\nChiu, W.-C. Prompting4debugging: Red-teaming text-to-\nimage diffusion models by finding problematic prompts.\narXiv preprint arXiv:2309.06135, 2023.\n\nGandikota, R., Materzynska, J., Fiotto-Kaufman, J., and\nIn\nBau, D. Erasing concepts from diffusion models.\nProceedings of the IEEE/CVF International Conference\non Computer Vision (ICCV), pp. 2426–2436, October\n2023.\n\nGandikota, R., Orgad, H., Belinkov, Y., Materzy´nska, J.,\nand Bau, D. Unified concept editing in diffusion models.\nIn Proceedings of the IEEE/CVF Winter Conference on\nApplications of Computer Vision, pp. 5111–5120, 2024.\n\nHan, X., Yang, S., Wang, W., Li, Y., and Dong, J. Probing\nunlearned diffusion models: A transferable adversarial\nattack perspective. arXiv preprint arXiv:2404.19382,\n2024.\n\nHeng, A. and Soh, H. Selective amnesia: A continual\nlearning approach to forgetting in deep generative models.\nAdvances in Neural Information Processing Systems, 36,\n2024.\n\nHo, J., Jain, A., and Abbeel, P. Denoising diffusion proba-\nbilistic models. Advances in Neural Information Process-\ning Systems, 33:6840–6851, 2020.\n\nKumari, N., Zhang, B., Wang, S.-Y., Shechtman, E., Zhang,\nR., and Zhu, J.-Y. Ablating concepts in text-to-image dif-\nfusion models. In Proceedings of the IEEE/CVF Interna-\ntional Conference on Computer Vision, pp. 22691–22702,\n2023.\n\nLyu, M., Yang, Y., Hong, H., Chen, H., Jin, X., He, Y., Xue,\nH., Han, J., and Ding, G. One-dimensional adapter to\nrule them all: Concepts, diffusion models and erasing\napplications. arXiv preprint arXiv:2312.16145, 2023.\n\nMeng, C., Song, Y., Song, J., Wu, J., Zhu, J.-Y., and Ermon,\nS. Sdedit: Image synthesis and editing with stochastic\ndifferential equations. arXiv preprint arXiv:2108.01073,\n2021.\n\n3\n\nSOURCE: https://blog.genlaw.org/pdfs/genlaw_icml2024/42.pdf",
    "https://peerj.com/articles/cs-1905.pdf": "SNIPPET: In this article, we conduct a comprehensive survey on the application of diffusion models in text generation. We divide text generation into three parts (conditional, unconstrained, and multi-mode text generation, respectively) and provide a detailed introduction.\n\n[FetchError] HTTP 400 for https://peerj.com/articles/cs-1905.pdf\n\nSOURCE: https://peerj.com/articles/cs-1905.pdf",
    "https://arxiv.org/abs/2410.06014": "SNIPPET: Oct 8, 2024 ... arXiv:2410.06014 [cs.RO]. (or arXiv:2410.06014v1 [cs.RO] for this version). https://doi.org/10.48550/arXiv.2410.06014. Focus to learn more.\n\nTITLE: SplaTraj: Camera Trajectory Generation with Semantic Gaussian Splatting\n\nBODY:\nComputer Science > Robotics\n[Submitted on 8 Oct 2024]\nTitle:SplaTraj: Camera Trajectory Generation with Semantic Gaussian Splatting\nView PDF HTML (experimental)Abstract:Many recent developments for robots to represent environments have focused on photorealistic reconstructions. This paper particularly focuses on generating sequences of images from the photorealistic Gaussian Splatting models, that match instructions that are given by user-inputted language. We contribute a novel framework, SplaTraj, which formulates the generation of images within photorealistic environment representations as a continuous-time trajectory optimization problem. Costs are designed so that a camera following the trajectory poses will smoothly traverse through the environment and render the specified spatial information in a photogenic manner. This is achieved by querying a photorealistic representation with language embedding to isolate regions that correspond to the user-specified inputs. These regions are then projected to the camera's view as it moves over time and a cost is constructed. We can then apply gradient-based optimization and differentiate through the rendering to optimize the trajectory for the defined cost. The resulting trajectory moves to photogenically view each of the specified objects. We empirically evaluate our approach on a suite of environments and instructions, and demonstrate the quality of generated image sequences.\nCurrent browse context:\ncs.RO\nReferences & Citations\nexport BibTeX citation\nLoading...\nBibliographic and Citation Tools\nBibliographic Explorer (What is the Explorer?)\nConnected Papers (What is Connected Papers?)\nLitmaps (What is Litmaps?)\nscite Smart Citations (What are Smart Citations?)\nCode, Data and Media Associated with this Article\nalphaXiv (What is alphaXiv?)\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\nDagsHub (What is DagsHub?)\nGotit.pub (What is GotitPub?)\nHugging Face (What is Huggingface?)\nPapers with Code (What is Papers with Code?)\nScienceCast (What is ScienceCast?)\nDemos\nRecommenders and Search Tools\nInfluence Flower (What are Influence Flowers?)\nCORE Recommender (What is CORE?)\narXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.\n\nSOURCE: https://arxiv.org/abs/2410.06014",
    "https://arxiv.org/abs/2410.05523": "SNIPPET: Oct 7, 2024 ... arXiv:2410.05523 [astro-ph.SR]. (or arXiv:2410.05523v1 [astro-ph.SR] for this version). https://doi.org/10.48550/arXiv.2410.05523. Focus to ...\n\nTITLE: Multiwavelength Campaign Observations of a Young Solar-type Star, EK Draconis. II. Understanding Prominence Eruption through Data-Driven Modeling and Observed Magnetic Environment\n\nBODY:\nAstrophysics > Solar and Stellar Astrophysics\n[Submitted on 7 Oct 2024]\nTitle:Multiwavelength Campaign Observations of a Young Solar-type Star, EK Draconis. II. Understanding Prominence Eruption through Data-Driven Modeling and Observed Magnetic Environment\nView PDF HTML (experimental)Abstract:EK Draconis, a nearby young solar-type star (G1.5V, 50-120 Myr), is known as one of the best proxies for inferring the environmental conditions of the young Sun. The star frequently produces superflares and Paper I presented the first evidence of an associated gigantic prominence eruption observed as a blueshifted H$\\alpha$ Balmer line emission. In this paper, we present the results of dynamical modeling of the stellar eruption and examine its relationship to the surface starspots and large-scale magnetic fields observed concurrently with the event. By performing a one-dimensional free-fall dynamical model and a one dimensional hydrodynamic simulation of the flow along the expanding magnetic loop, we found that the prominence eruption likely occurred near the stellar limb (12$^{+5}_{-5}$-16$^{+7}_{-7}$ degrees from the limb) and was ejected at an angle of 15$^{+6}_{-5}$-24$^{+6}_{-6}$ degrees relative to the line of sight, and the magnetic structures can expand into a coronal mass ejection (CME). The observed prominence displayed a terminal velocity of $\\sim$0 km s$^{-1}$ prior to disappearance, complicating the interpretation of its dynamics in Paper I. The models in this paper suggest that prominence's H$\\alpha$ intensity diminishes at around or before its expected maximum height, explaining the puzzling time evolution in observations. The TESS light curve modeling and (Zeeman) Doppler Imaging revealed large mid-latitude spots with polarity inversion lines and one polar spot with dominant single polarity, all near the stellar limb during the eruption. This suggests that mid-latitude spots could be the source of the pre-existing gigantic prominence we reported in Paper I. These results provide valuable insights into the dynamic processes that likely influenced the environments of early Earth, Mars, Venus, and young exoplanets.\nCurrent browse context:\nastro-ph.SR\nChange to browse by:\nReferences & Citations\nexport BibTeX citation\nLoading...\nBibliographic and Citation Tools\nBibliographic Explorer (What is the Explorer?)\nConnected Papers (What is Connected Papers?)\nLitmaps (What is Litmaps?)\nscite Smart Citations (What are Smart Citations?)\nCode, Data and Media Associated with this Article\nalphaXiv (What is alphaXiv?)\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\nDagsHub (What is DagsHub?)\nGotit.pub (What is GotitPub?)\nHugging Face (What is Huggingface?)\nPapers with Code (What is Papers with Code?)\nScienceCast (What is ScienceCast?)\nDemos\nRecommenders and Search Tools\nInfluence Flower (What are Influence Flowers?)\nCORE Recommender (What is CORE?)\nIArxiv Recommender\n(What is IArxiv?)\narXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.\n\nSOURCE: https://arxiv.org/abs/2410.05523",
    "https://arxiv.org/abs/2402.10397": "SNIPPET: Feb 16, 2024 ... arXiv:2402.10397 [cs.LG]. (or arXiv:2402.10397v1 [cs.LG] for this version). https://doi.org/10.48550/arXiv.2402.10397. Focus to learn more.\n\nTITLE: LogELECTRA: Self-supervised Anomaly Detection for Unstructured Logs\n\nBODY:\nComputer Science > Machine Learning\n[Submitted on 16 Feb 2024]\nTitle:LogELECTRA: Self-supervised Anomaly Detection for Unstructured Logs\nView PDF HTML (experimental)Abstract:System logs are some of the most important information for the maintenance of software systems, which have become larger and more complex in recent years. The goal of log-based anomaly detection is to automatically detect system anomalies by analyzing the large number of logs generated in a short period of time, which is a critical challenge in the real world. Previous studies have used a log parser to extract templates from unstructured log data and detect anomalies on the basis of patterns of the template occurrences. These methods have limitations for logs with unknown templates. Furthermore, since most log anomalies are known to be point anomalies rather than contextual anomalies, detection methods based on occurrence patterns can cause unnecessary delays in detection. In this paper, we propose LogELECTRA, a new log anomaly detection model that analyzes a single line of log messages more deeply on the basis of self-supervised anomaly detection. LogELECTRA specializes in detecting log anomalies as point anomalies by applying ELECTRA, a natural language processing model, to analyze the semantics of a single line of log messages. LogELECTRA outperformed existing state-of-the-art methods in experiments on the public benchmark log datasets BGL, Sprit, and Thunderbird.\nReferences & Citations\nexport BibTeX citation\nLoading...\nBibliographic and Citation Tools\nBibliographic Explorer (What is the Explorer?)\nConnected Papers (What is Connected Papers?)\nLitmaps (What is Litmaps?)\nscite Smart Citations (What are Smart Citations?)\nCode, Data and Media Associated with this Article\nalphaXiv (What is alphaXiv?)\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\nDagsHub (What is DagsHub?)\nGotit.pub (What is GotitPub?)\nHugging Face (What is Huggingface?)\nPapers with Code (What is Papers with Code?)\nScienceCast (What is ScienceCast?)\nDemos\nRecommenders and Search Tools\nInfluence Flower (What are Influence Flowers?)\nCORE Recommender (What is CORE?)\nIArxiv Recommender\n(What is IArxiv?)\narXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.\n\nSOURCE: https://arxiv.org/abs/2402.10397",
    "https://arxiv.org/abs/2410.15119": "SNIPPET: Oct 17, 2024 ... arXiv:2410.13719 [gr-qc]. (or arXiv:2410.13719v1 [gr-qc] for this version). https://doi.org/10.48550/arXiv.2410.13719. Focus to learn more.\n\nTITLE: Mean Field LQG Social Optimization: A Reinforcement Learning Approach\n\nBODY:\nMathematics > Optimization and Control\n[Submitted on 19 Oct 2024]\nTitle:Mean Field LQG Social Optimization: A Reinforcement Learning Approach\nView PDF HTML (experimental)Abstract:This paper presents a novel model-free method to solve linear quadratic Gaussian mean field social control problems in the presence of multiplicative noise. The objective is to achieve a social optimum by solving two algebraic Riccati equations (AREs) and determining a mean field (MF) state, both without requiring prior knowledge of individual system dynamics for all agents. In the proposed approach, we first employ integral reinforcement learning techniques to develop two model-free iterative equations that converge to solutions for the stochastic ARE and the induced indefinite ARE respectively. Then, the MF state is approximated, either through the Monte Carlo method with the obtained gain matrices or through the system identification with the measured data. Notably, a unified state and input samples collected from a single agent are used in both iterations and identification procedure, making the method more computationally efficient and scalable. Finally, a numerical example is given to demonstrate the effectiveness of the proposed algorithm.\nReferences & Citations\nexport BibTeX citation\nLoading...\nBibliographic and Citation Tools\nBibliographic Explorer (What is the Explorer?)\nConnected Papers (What is Connected Papers?)\nLitmaps (What is Litmaps?)\nscite Smart Citations (What are Smart Citations?)\nCode, Data and Media Associated with this Article\nalphaXiv (What is alphaXiv?)\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\nDagsHub (What is DagsHub?)\nGotit.pub (What is GotitPub?)\nHugging Face (What is Huggingface?)\nPapers with Code (What is Papers with Code?)\nScienceCast (What is ScienceCast?)\nDemos\nRecommenders and Search Tools\nInfluence Flower (What are Influence Flowers?)\nCORE Recommender (What is CORE?)\narXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.\n\nSOURCE: https://arxiv.org/abs/2410.15119",
    "https://aclanthology.org/2025.naacl-long.532.pdf": "SNIPPET: BibTeX. Markdown MODS XML Endnote More options… PDF: https://aclanthology.org/2025.naacl-long.532.pdf · PDF Cite Search Fix data. Export citation. ×. BibTeX ...\n\nTITLE: (from PDF)\n\nBODY:\n10612\nProceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies\n(Volume 1: Long Papers), pages 10612–10626\nApril 29 - May 4, 2025 ©2025 Association for Computational Linguistics\n\nPrivateSyntheticTextGenerationwithDiffusionModelsSebastianOchs1andIvanHabernal2TrustworthyHumanLanguageTechnologies1DepartmentofComputerScience,TechnicalUniversityofDarmstadt2ResearchCenterTrustworthyDataScienceandSecurityoftheUniversityAllianceRuhr,FacultyofComputerScience,RuhrUniversityBochumsebastian.ochs@tu-darmstadt.de,ivan.habernal@ruhr-uni-bochum.dewww.trusthlt.orgAbstractHowcapablearediffusionmodelsofgenerat-ingsyntheticstexts?Recentresearchshowstheirstrengths,withperformancereachingthatofauto-regressiveLLMs.Butaretheyalsogoodingeneratingsyntheticdataifthetrain-ingwasunderdifferentialprivacy?Heretheevidenceismissing,yetthepromisesfrompri-vateimagegenerationlookstrong.Inthispa-perweaddressthisopenquestionbyextensiveexperiments.Atthesametime,wecriticallyassess(andreimplement)previousworksonsyntheticprivatetextgenerationwithLLMsandrevealsomeunmetassumptionsthatmighthaveledtoviolatingthedifferentialprivacyguarantees.Ourresultspartlycontradictpre-viousnon-privatefindingsandshowthatfullyopen-sourceLLMsoutperformdiffusionmod-elsintheprivacyregime.Ourcompletesourcecodes,datasets,andexperimentalsetuparepub-liclyavailabletofosterfutureresearch1.1IntroductionHowcanwesharesensitivetextualdataandprotectprivacyofindividualsinthereatthesametime?Ago-tomethodtocircumventthisissueissyntheticdatageneration,usedmainlyfortabulardata(Her-nandezetal.,2022),orimages(Bissotoetal.,2018)inthemedicaldomain.However,syntheticdatagenerationaloneisnotsufficienttoprotectprivacyoftheunderlyingdata.Forexample,Stadleretal.(2022)showthatoutliersinsyntheticdatasufferfrommembershipinferenceattacks.Achievingformalprivacyguaranteesfortheun-derlyingdataispossiblebycombiningagenerativemodelwithdifferentialprivacy(DP,Dworketal.(2006))wherewetradeoffprivacyforreducedutilityofthesyntheticdata.Yueetal.(2023)andMatternetal.(2022)demonstratedfeasibilityofsyntheticdatagenerationwithDPinNLP.However,1https://github.com/trusthlt/private-synthetic-text-generationbothYueetal.(2023)andMatternetal.(2022)vio-lateseveralassumptionsabouttheunderlyingdatawhichcastsdoubtsonthevalidityoftheirfindings.Recentadvancesalsoshowthesuccessofdif-fusionmodelsinprivatesyntheticdatagenerationinimages(Ghalebikesabietal.,2023).Despiterecentachievementsinconditionaltextgenerationusingdiffusionmodels(Lietal.,2022;Gongetal.,2023a;Linetal.,2023),thecapabilitiesofprivatesynthetictextgenerationwithdiffusionmodelsre-mainunexplored.Wethusasktworesearchquestions.First,whatperformanceondownstreamtaskscanweachieveusingdiffusionmodelsforsynthetictextgenerationwithvaryingstrengthsofdifferentialprivacy?Sec-ond,whichfactorsmighthaveartificiallyinflatedperformanceinpreviousworksandcanwemitigateinvalidassumptionsinempiricalexperiments?Weaddressthesequestionsasfollows.Ourfirsthypothesisisthatdiffusionmodelsfortextgener-ationmightnotsufferfromnoiseaddedinDP,astheyinherentlyworkwithade-noisingobjective.Weaddressthisquestionempiricallybyconduct-ingextensive(andexpensive)experimentswiththreestate-of-the-artdiffusionmodels.Weaddressthesecondresearchquestionbytwoarguments.Ourfirstargumentisthatpreviousexperimentsmostlyfocusedon‘old’publicdatasets,suchastheIMDbmoviereviews(Maasetal.,2011).Asthesedatasetsmayverylikelyhavebeenseenduringpre-trainingoftheutilizedLLMs(GPT-2),thereportedeffectivenessoftheprivacy-preservingsynthetictextsmaybeoverestimated.Wefactoroutthepo-tentialdataleakingbyintroducingfivenew‘fresh’unseendatasetsintotheexperiments.OursecondargumentistheviolationofDPbyignoringgroupprivacyinthedatasets.Weprovideevidencein§4.Westrivefortransparency,reproducibility,andaccountability—threekeyingredientsofprivacy-relatedresearch.Thereforeweexperimentwithfullyopen-sourceandtransparentmodels,suchas\f10613\n\nBLOOM(LeScaoetal.,2023).Alloursourcecodesanddatasetsarealsopubliclyavailableforfurtherscrutiny.Ourmaincontributionsare(1)ev-idenceofsevereunderestimationofDPguaranteesinpreviousworks,(2)empiricalevidenceshowingthat,unlikeinimagedomain,diffusionmodelsforsynthetictextgenerationsufferseverelyunderDPtraining,and(3)completere-implementationsofunpublishedpreviousworksenablingtransparencyandpotentiallyincreasingtrustinprivacy-orientedresearch.2Theoreticalbackground2.1Differentialprivacy(DP)DP,introducedbyDwork(2006),isamathemati-calframeworkaimedatprotectingtheprivacyofindividualsinadataset.Throughaddingacalcu-latedamountofnoisetodataorstatisticalqueries,itprovidesformalguaranteesthatdatacontributorscannotbesingledouteasily,whilestillenablingmeaningfulanalysis.Abadietal.(2016)applyDPtostochasticgradientdescent,calledDP-SGD,whichallowsustoputformalprivacyguaranteesontrainedneuralnetworks.AsthetheoryofDPisconsiderablyextensive,werefertoWoodetal.’s(2018)workasaneas-ilyaccessibleintroductionfortheinterestedreader.Furthermore,Habernaletal.(2023)presentatuto-rialabouthowDPcanbeharnessedforNLPap-plications,whileCummingsetal.(2024)provideacomprehensivereviewaboutthecurrentstateoftheartinDPresearch.2.2DiffusionmodelsfortextDiffusionmodelshaveriseninpopularityasgen-erativemodelsrecently,especiallyinthedomainofimagesynthesis(Yangetal.,2023).Diffusionmodelscanbedescribedas\"MarkovianHierarchi-calVariationalAutoencoders\"(Luo,2022),thatareabletogeneratedatafromGaussiannoise.Thisisachievedbyutilizingtwotransitionsduringtrain-ing,calledforwardandreverseprocess.GivenTsteps,theforwardprocessincrementallyaddsasmallamountofGaussiannoisetoadatapointx0,creatingachainx0,x1,x2,...,xT,wherexTresemblesaGaussiandistribution.Duringthere-verseprocess,thediffusionmodellearnshowtotransitionfromxitoxi−1,∀i∈{0,1,2,...,T},es-sentiallylearninghowto‘undo’thecorruptionoftheforwardchain,called‘denoising’.Foramoremathematicallyfoundeddescriptionofdiffusionmodels,wepointtoBishopandBishop(2024,chap.20)andthebackgroundchapterofHoetal.(2020).Thereverseprocesscanbeguidedbyaddinginformationthatrelatestotheoriginaldata.Forexample,intext-to-imagegeneration,thelatentrepresentationofanimagedescription,usingatextencoder,steersthedenoisingofthediffusionmodel(Rameshetal.,2022).Duringinference,thediffusionmodelisthenabletogenerateimagesfromGaussiannoisethatcorrespondtotheinputtext.TextdiffusionmodelvariantsLikeinimagesynthesis,diffusionmodelsarealsobeingex-ploredintextgeneration.Lietal.(2022)proposeDiffusion-LM,adiffusionmodelcapableofgen-eratingtextsnon-autoregressively.Incontrasttotypicallanguagemodels,wheretextsequencesarecreatedtoken-by-token,Diffusion-LMgeneratesatextbygraduallydenoisingalistofGaussiannoisevectorsintowordembeddings.Thegenera-tionprocesscanbedirectlycontrolledbyprovidingconditionsthattheresultingtextshouldfulfill,forexamplesyntacticfeaturessuchasapredeterminedsequenceofparts-of-speechtags.Diffusion-LM’scodebasealsoservedasabasisforothertextdiffusionmodels,suchasDiffuSeq(Gongetal.,2023a),asequence-to-sequencetextdiffusionmodel.UnlikeDiffusion-LM,DiffuSeqconditionsthegeneratedoutputontheinputtext.Thisisaccomplishedthroughconcatenatingthewordvectorsequencesoftheinputandoutput,andapplyingthediffusionprocessonlytotheoutputvectors.Gongetal.(2023a)reportthatDiffuSeqachievessimilartext-to-textgenerationcapabilitiesasfine-tunedGPT2-baseand-large(Radfordetal.,2019)models,despiteofbeingnon-autoregressive.Therefore,weincludedDiffuSeqinourexperi-ments.Interestingly,whilethetrainingandinferencemethodoftextdiffusionmodelsdifferfundamen-tallyfromLLMs,theirarchitectureisstillbasedontheTransformer(Vaswanietal.,2017).3RelatedworkToenabletheprivacy-preservingsharingoflabeledtextdata,Matternetal.(2022)proposeamethodcalledprompt-basedDPfine-tuning,whichtheyutilizetotrainaGPT2-largemodel(Radfordetal.,2019).Theprocesscanbedescribedasa‘reverse’classificationtaskwhereinsteadoflearningtopre-dictthelabelforatext,thegenerativemodelis\f10614\n\ntrainedongeneratingtextssuitableforagivenla-bel.TheprivacyofeachauthorinthedatasetisprotectedbyapplyingDP-SGDtothefine-tuningprocedure.Additionally,amismatchlossisappliedduringtrainingbymaximizingthenegativelog-likelihoodofpurposelymislabeledtexts.Matternetal.(2022)experimentontwopubliclyavailabledatasetstovalidatetheirapproach.Thereportedresultsclaimthattextclassifierstrainedonthepri-vatized,syntheticdataandevaluatedonoriginaldataexperiencenosignificantperformancelosscomparedtoaclassifierdirectlytrainedontheorig-inaltexts.Yueetal.(2023)usethesamemethodologyintheirapproach,apartfromthemismatchloss.IncontrasttoMatternetal.(2022),theyincludeaprivatecustomerfeedbackdatasetintheirexper-iments,wherethesynthetictextsalsoprovedtobeusefulfordownstreamtaskclassificationper-formance.Furthermore,theyempiricallyevaluatetheprivacy-preservationoftheDPgenerationmod-elsbyinjecting‘canaries’intothetrainingdata,assuggestedbyCarlinietal.(2019).Afterthosemodelsgenerateasyntheticdataset,itisthenpos-sibletotrackifanycanarieshavebeenreplicated,whichwasnotthecaseformodelstrainedwithDPguarantees.Althoughnotinthetextdomain,diffusionmod-elshavealsobeenexploredforDPsyntheticdatageneration.Ghalebikesabietal.(2023)traindiffusionmodelswithDP-SGDonseverallow-resolutionimagedatasets,suchastheMNISTdatasetofhandwrittendigits(LeCunetal.,2010)andgeneratesyntheticimagesforthedownstreamclassificationtask.Classifierstrainedonthesyn-theticdatareportedlyreachperformancesclosetothestateoftheart.Notably,inmostexperi-ments,beforethediffusionmodelsweretrainedwithDP-SGDonsmallerdatasets,theywerepre-trainedwithoutDPonthelarge-scaleImageNet32(Chrabaszczetal.,2017)dataset.Sofar,pretraininglargemodelsonpublicdataandfine-tuningthosewithDP-SGDonsmaller,pri-vatedatasetsseemstobeanefficientmethodtopro-duceprivacy-preservingandusefulsyntheticdata.However,neitherDiffusion-LM,norDiffuSeq,normostothercurrentdiffusionmodelincludepretrain-ingintheirmethodology.OneexceptionisGENIE,introducedbyLinetal.(2023),whichistheonlyavailablediffusionmodelfortext-to-textgenera-tionthatispretrained.GENIEispretrainedonalargecorporaoftextsbyusinganobjectivesimilartospan-basedmaskedlanguagemodeling,How-ever,insteadofpredictingthecorrecttextspan,Gaussiannoiseiscontinuouslyaddedtothese-lectedtokenspan,whichGENIElearnstodenoise.Whenfine-tuned,themodeloutperformsthebaseversionsofT5(Raffeletal.,2020)andBART(Lewisetal.,2020)inseveralnaturallanguagegeneration(NLG)tasks.4CriticalanalysisofexistingworksInthefollowing,weexplainindetailhowDPsyn-thetictextgenerationhasbeenaccomplishedinpriorwork.Wealsocriticallyassesstheirunderly-ingassumptionsandevaluatetheirrespectivevalid-ityinregardstoprivacyprotection.PromptingLLMstrainedwithDP-SGDiscor-rect.BothYueetal.(2023)andMatternetal.(2022)describeascenario,whereadataholderwishestobenefitfrompublicresearchontheirin-housesensitivetextresources(suchasmedicalre-portsorcustomerdata),butcannotreleasethemduetoprivacyconcerns.Thefirstassumptionisthatthesensitivedocumentsarelabeled(catego-rized)andthetasktobesolvedisclassification.Theauthorsaddressthisproblemasfollows.First,theycreatepromptsforeachoriginaltextbasedonthecategorytowhichitbelongs.Sec-ond,apretrainedlanguagemodelisfine-tunedontheprompt:textpairs(e.g.,<writeapositivereview:originalreviewtext>)fromthetrainingpartofthesensitivedataset,learningtocreatetextsresemblingthedatafromtheinstructedcategory.Sincesyntheticdatagenerationaloneisnotenoughtoprotectprivacy(recallthediscussioninSection1),bothauthorstraintheirmodelswithDP-SGD.Afterwards,synthetictextsaresampledfromtheresultingmodelsusingthepromptsfor-mulatedearlier.ItisworthmentioningthattheDPguaranteeremainsthesameregardlessoftheamountofsyntheticgeneratedtexts.Theutilityoftheresultingsamplesisthenevaluatedonadown-streamclassificationtask,namelybyfine-tuningBERTonthegeneratedtextsandtestingitontheoriginalsensitivetestdata.Multipletextsfromthesamepersonviolatesdif-ferentialprivacy.ThemainrequirementinDPisthatintheunderlyingdataset,thereisaone-to-onecorrespondenceofapersonanditsdatapoint.FromtheMLperspective,thismeansthateachex-\f10615\n\namplefortrainingorfine-tuningbelongsexactlytoauniqueperson.ThisallowsustospelloutthenecessarynotionofneighboringdatasetsandtheveryguaranteeofDP,suchthatthedifferenceofaprivateanalysiswillbe‘roughlythesame’(gov-ernedbyεandδ)ontwodatasetsofsizenandn−1,respectively.Thisassumptionisviolatedwhenatextualdatasetcontainsmultipleexamplesfromthesameauthor.Duetouniquewritingstyle,vocabularyorotherimplicitfeatures,thosetextsmaycorre-latewitheachothereventhoughtheydonotsharethesameexplicitinformation.Aworkaroundforthisissueisgroupprivacy(DworkandRoth,2014,Theorem2.2),whichtranslatestothefollowing:Whenassumingthateachauthorprovidesatmostkcontributionstoadatasetandtextsofdifferentauthorsdonotcorrelatewitheachother,thepri-vacyboundsofany(ε,δ)-DPmechanismincreaseto(kε,kexp((k−1)ε),δ)-DP.However,tothebestofourknowledge,noworksutilizingDP-SGDhaveeverusedgroupprivacy,anditisactuallyunclearwhethertheDP-SGDiscompatiblewithit.2Weofferadditionalperspectivesonthismatterinthe“Limitationsandethicsstatement”sectionbelow.OuranalysisofthedatasetsusedinpreviousworksrevealsthatthereisaclearviolationofthisDPassumption.InthebookreviewsfromtheAmazonMultiDomaindata(Blitzeretal.,2007),usedinMatternetal.’s(2022)experiments,users‘ShalomFreedman’,‘PrairiePal’and52morecon-tributedatleastmorethanonereview.Similarly,theYelpOpenDataset3providesconcreteproofinitsdocumentationthatsomeusershavewrittenmorethanonereview.Nonetheless,thedatasetwaspartoftheexperimentscarriedoutbyYueetal.(2023).Giventheseviolatedassumptions,wecannotreallytellwhetherornotpreviousworkstrulyguar-anteethereportedprivacystrength.Wesuspectthatunderfairconditionstheprotectionwouldbemuchlower.NoevidencethatthesensitivedatawerenotpartofLMpretraining.AnotherassumptioninMat-ternetal.’s(2022)andYueetal.’s(2023)workisthatthe‘sensitive’data(inthiscaseIMDb,Yelp,2MostDP-SGDimplementationsrelyontheamplificationtheorembysubsampling,socalledPoissonsampling,whichwasonlyprovenfornon-groupprivacybyLietal.(2012).3https://www.yelp.com/dataset/documentation/mainetc.)havenotbeen‘seen’duringpretrainingofthegenerativelanguagemodel.Weidentifytwopotentialissueshere.First,anydatapointaccessedduringLLMpre-trainingcanbepotentiallyleakedbyadversarialprompting(Carlinietal.,2021;Nasretal.,2023).Ifthesensitivedatawereusedbothin(1)LLMpretrainingandin(2)privatefine-tuning,privacyhadbeenbreachedin(1)alreadyandnoclaimsaboutprotectionin(2)arevalid.Second,pre-trainingandfine-tuningonthesamedatawillmostlikelyboosttheeffectivenessofthesynthesizedtexts,asopposedtosynthesizingout-of-domain‘fresh’data.ThishasbeendemonstratedinpreviousworkbyIgamberdievetal.(2022)whofoundthatsuchleakingledtounrealisticallygoodresultsinotherworks.SinceYueetal.(2023)andMatternetal.(2022)useGPT2(Radfordetal.,2019)intheirmethod,andthepretrainingdataofthatmodelisnotdis-closed,wedoubtthattheperformancereportedontheYELPandI\n...[truncated]",
    "https://proceedings.iclr.cc/paper_files/paper/2024/file/081b08068e4733ae3e7ad019fe8d172f-Paper-Conference.pdf": "SNIPPET: Published as a conference paper at ICLR 2024. 3D classification (Shen et al., 2023), controllable image editing (Zhang & Agrawala, 2023), image.\n\nTITLE: (from PDF)\n\nBODY:\nPublished as a conference paper at ICLR 2024\n\nSDXL: IMPROVING LATENT DIFFUSION MODELS FOR\nHIGH-RESOLUTION IMAGE SYNTHESIS\n\nDustin Podell\n\nZion English\n\nKyle Lacey\n\nAndreas Blattmann\n\nTim Dockhorn\n\nJonas Müller\n\nJoe Penna\n\nRobin Rombach\n\nABSTRACT\n\nWe present Stable Diffusion XL (SDXL), a latent diffusion model for text-to-image\nsynthesis. Compared to previous versions of Stable Diffusion, SDXL leverages\na three times larger UNet backbone, achieved by significantly increasing the\nnumber of attention blocks and including a second text encoder. Further, we design\nmultiple novel conditioning schemes and train SDXL on multiple aspect ratios.\nTo ensure highest quality results, we also introduce a refinement model which is\nused to improve the visual fidelity of samples generated by SDXL using a post-hoc\nimage-to-image technique. We demonstrate that SDXL improves dramatically over\nprevious versions of Stable Diffusion and achieves results competitive with those\nof black-box state-of-the-art image generators such as Midjourney (Holz, 2023).\n\n1\n\nINTRODUCTION\n\nThe last year has brought enormous leaps in deep generative modeling across various data domains,\nsuch as natural language (Touvron et al., 2023), audio (Huang et al., 2023), and visual media (Rom-\nbach et al., 2021; Ramesh et al., 2022; Saharia et al., 2022; Singer et al., 2022; Ho et al., 2022;\nBlattmann et al., 2023; Esser et al., 2023). In this report, we focus on the latter and unveil SDXL,\na drastically improved version of Stable Diffusion. Stable Diffusion is a latent text-to-image dif-\nfusion model (DM) which serves as the foundation for an array of recent advancements in, e.g.,\n\n1\n\n\fPublished as a conference paper at ICLR 2024\n\n3D classification (Shen et al., 2023), controllable image editing (Zhang & Agrawala, 2023), image\npersonalization (Gal et al., 2022), synthetic data augmentation (Stöckl, 2022), graphical user interface\nprototyping (Wei et al., 2023), etc. Remarkably, the scope of applications has been extraordinarily\nextensive, encompassing fields as diverse as music generation (Forsgren & Martiros, 2022) and\nreconstructing images from fMRI brain scans (Takagi & Nishimoto, 2023).\n\nUser studies demonstrate that SDXL consistently surpasses all previous versions of Stable Diffusion\nby a significant margin (see Fig. 1). In this report, we present the design choices which lead to this\nboost in performance encompassing i) a 3× larger UNet-backbone compared to previous Stable\nDiffusion models (Sec. 2.1), ii) two simple yet effective additional conditioning techniques (Sec. 2.2)\nwhich do not require any form of additional supervision, and iii) a separate diffusion-based refinement\nmodel which applies a noising-denoising process (Meng et al., 2021) to the latents produced by SDXL\nto improve the visual quality of its samples (Sec. 2.5).\n\nA major concern in the field of visual media creation is that while black-box-models are often\nrecognized as state-of-the-art, the opacity of their architecture prevents faithfully assessing and\nvalidating their performance. This lack of transparency hampers reproducibility, stifles innovation,\nand prevents the community from building upon these models to further the progress of science and\nart. Moreover, these closed-source strategies make it challenging to assess the biases and limitations\nof these models in an impartial and objective way, which is crucial for their responsible and ethical\ndeployment. With SDXL we are releasing an open model that achieves competitive performance with\nblack-box image generation models (see Fig. 11 & Fig. 12).\n\n2\n\nIMPROVING Stable Diffusion\n\nIn this section we present our improvements for the Stable Diffusion architecture. These are modular,\nand can be used individually or together to extend any model. Although the following strategies are\nimplemented as extensions to latent diffusion models (LDMs) (Rombach et al., 2021), most of them\nare also applicable to their pixel-space counterparts.\n\nFigure 1: Left: Comparing user preferences between SDXL and Stable Diffusion 1.5 & 2.1. While SDXL already\nclearly outperforms Stable Diffusion 1.5 & 2.1, adding the additional refinement stage boosts performance. Right:\nVisualization of the two-stage pipeline: We generate initial latents of size 128 × 128 using SDXL. Afterwards,\nwe utilize a specialized high-resolution refinement model and apply SDEdit (Meng et al., 2021) on the latents\ngenerated in the first step, using the same prompt. SDXL and the refinement model use the same autoencoder.\n\n2.1 ARCHITECTURE & SCALE\n\nStarting with the seminal works Ho et al. (2020) and Song et al. (2020b), which demonstrated that\nDMs are powerful generative models for image synthesis, the convolutional UNet (Ronneberger\net al., 2015) architecture has been the dominant architecture for diffusion-based image synthesis.\nHowever, with the development of foundational DMs (Saharia et al., 2022; Ramesh et al., 2022;\nRombach et al., 2021), the underlying architecture has constantly evolved: from adding self-attention\nand improved upscaling layers (Dhariwal & Nichol, 2021), over cross-attention for text-to-image\nsynthesis (Rombach et al., 2021), to pure transformer-based architectures (Peebles & Xie, 2022).\n\n2\n\n\fPublished as a conference paper at ICLR 2024\n\nTable 1: Comparison of SDXL and older Stable Diffusion models.\n\nModel\n\n# of UNet params\nTransformer blocks\nChannel mult.\nText encoder\nContext dim.\nPooled text emb.\n\nSDXL\n\n2.6B\n[0, 2, 10]\n[1, 2, 4]\n\nSD 1.4/1.5\n\nSD 2.0/2.1\n\n860M\n[1, 1, 1, 1]\n[1, 2, 4, 4]\n\n865M\n[1, 1, 1, 1]\n[1, 2, 4, 4]\n\nCLIP ViT-L & OpenCLIP ViT-bigG CLIP ViT-L OpenCLIP ViT-H\n\n2048\nOpenCLIP ViT-bigG\n\n768\nN/A\n\n1024\nN/A\n\nWe follow this trend and, following Hoogeboom et al. (2023), shift the bulk of the transformer\ncomputation to lower-level features in the UNet. In particular, and in contrast to the original Stable\nDiffusion architecture, we use a heterogeneous distribution of transformer blocks within the UNet:\nFor efficiency reasons, we omit the transformer block at the highest feature level, use 2 and 10\nblocks at the lower levels, and remove the lowest level (8× downsampling) in the UNet altogether\n— see Tab. 1 for a comparison between the architectures of Stable Diffusion 1.x & 2.x and SDXL.\nWe opt for a more powerful pre-trained text encoder that we use for text conditioning. Specifically,\nwe use OpenCLIP ViT-bigG (Ilharco et al., 2021) in combination with CLIP ViT-L (Radford et al.,\n2021), where we concatenate the penultimate text encoder outputs along the channel-axis (Balaji\net al., 2022). Besides using cross-attention layers to condition the model on the text-input, we follow\nNichol et al. (2021) and additionally condition the model on the pooled text embedding from the\nOpenCLIP model. These changes result in a model size of 2.6B parameters in the UNet, see Tab. 1.\nThe text encoders have a total size of 817M parameters.\n\n2.2 MICRO-CONDITIONING\n\nConditioning the Model on Image Size A no-\ntorious shortcoming of the LDM paradigm (Rom-\nbach et al., 2021) is the fact that training a model\nrequires a minimal image size, due to its two-\nstage architecture. The two main approaches to\ntackle this problem are either to discard all train-\ning images below a certain minimal resolution\n(for example, Stable Diffusion 1.4/1.5 discarded\nall images with any size below 512 pixels), or,\nalternatively, upscale images that are too small.\nHowever, depending on the desired image res-\nolution, the former method can lead to signifi-\ncant portions of the training data being discarded,\nwhat will likely lead to a loss in performance\nand hurt generalization. We visualize such ef-\nfects in Fig. 2 for the dataset on which SDXL\nwas pretrained. For this particular choice of data,\ndiscarding all samples below our pretraining res-\nolution of 2562 pixels would lead to a significant 39% of discarded data. The second method, on\nthe other hand, usually introduces upscaling artifacts which may leak into the final model outputs,\ncausing, for example, blurry samples.\n\nHeight-vs-Width distribution of our\nFigure 2:\npre-training dataset. Without\nthe proposed size-\nconditioning, 39% of the data would be discarded due\nto edge lengths smaller than 256 pixels as visualized\nby the dashed black lines. Color intensity in each visu-\nalized cell is proportional to the number of samples.\n\nInstead, we propose to condition the UNet model on the original image resolution, which is trivially\navailable during training. In particular, we provide the original (i.e., before any rescaling) height\nand width of the images as an additional conditioning to the model csize = (horiginal, woriginal).\nEach component is independently embedded using a Fourier feature encoding, and these encodings\nare concatenated into a single vector that we feed into the model by adding it to the timestep\nembedding (Dhariwal & Nichol, 2021).\n\nAt inference time, a user can then set the desired apparent resolution of the image via this size-\nconditioning. Evidently (see Fig. 3), the model has learned to associate the conditioning csize with\nresolution-dependent image features, which can be leveraged to modify the appearance of an output\ncorresponding to a given prompt. Note that for the visualization shown in Fig. 3, we visualize samples\n\n3\n\n\fPublished as a conference paper at ICLR 2024\n\ncsize = (64, 64)\n\ncsize = (128, 128),\n\ncsize = (256, 256),\n\ncsize = (512, 512),\n\n“A robot painted as graffiti on a brick wall. a sidewalk is in front of the wall, and grass is growing out of cracks in the concrete.”\n\n“Panda mad scientist mixing sparkling chemicals, artstation.”\n\nFigure 3: The effects of varying the size-conditioning: We show draw 4 samples with the same random seed from\nSDXL and vary the size-conditioning as depicted above each column. The image quality clearly increases when\nconditioning on larger image sizes. Samples from the 5122 model, see Sec. 2.5. Note: For this visualization, we\nuse the 512 × 512 pixel base model (see Sec. 2.5), since the effect of size conditioning is more clearly visible\nbefore 1024 × 1024 finetuning. Best viewed zoomed in.\n\ngenerated by the 512 × 512 model (see Sec. 2.5 for details), since the effects of the size conditioning\nare less clearly visible after the subsequent multi-aspect (ratio) finetuning which we use for our final\nSDXL model.\n\nmodel\n\nFID-5k ↓\n\nTable 2: Conditioning on the original spatial\nsize of the training examples improves perfor-\nmance on class-conditional ImageNet Deng\net al. (2009) on 5122 resolution.\n\nWe quantitatively assess the effects of this simple but\neffective conditioning technique by training and evaluating\nthree LDMs on class conditional ImageNet (Deng et al.,\n2009) at spatial size 5122: For the first model (CIN-512-\nonly) we discard all training examples with at least one\nedge smaller than 512 pixels what results in a train dataset\nof only 70k images. For CIN-nocond we use all training\nexamples but without size conditioning. This additional\nconditioning is only used for CIN-size-cond. After training\nwe generate 5k samples with 50 DDIM steps (Song et al.,\n2020a) and (classifier-free) guidance scale of 5 (Ho & Salimans, 2022) for every model and compute\nIS Salimans et al. (2016) and FID Heusel et al. (2017) (against the full validation set). For CIN-\nsize-cond we generate samples always conditioned on csize = (512, 512). Tab. 2 summarizes the\nresults and verifies that CIN-size-cond improves upon the baseline models in both metrics. We\nattribute the degraded performance of CIN-512-only to bad generalization due to overfitting on the\nsmall training dataset while the effects of a mode of blurry samples in the sample distribution of\nCIN-nocond result in a reduced FID score. Note that, although we find these classical quantitative\nscores not to be suitable for evaluating the performance of foundational (text-to-image) DMs Saharia\net al. (2022); Ramesh et al. (2022); Rombach et al. (2021) (see App. E), they remain reasonable\nmetrics on ImageNet as the neural backbones of FID and IS have been trained on ImageNet itself.\n\nCIN-512-only\nCIN-nocond\nCIN-size-cond\n\n110.64\n211.50\n215.34\n\n43.84\n39.76\n36.53\n\nIS-5k ↑\n\nConditioning the Model on Cropping Parameters The first two rows of Fig. 4 illustrate a typical\nfailure mode of previous SD models: Synthesized objects can be cropped, such as the cut-off head\nof the cat in the left examples for SD 1-5 and SD 2-1. An intuitive explanation for this behavior is\nthe use of random cropping during training of the model: As collating a batch in DL frameworks\nsuch as PyTorch (Paszke et al., 2019) requires tensors of the same size, a typical processing pipeline\n\n4\n\n\fPublished as a conference paper at ICLR 2024\n\n“A propaganda poster depicting a cat dressed as french\nemperor napoleon holding a piece of cheese.”\n\n“a close-up of a fire spitting dragon,\ncinematic shot.”\n\n5\n-\n1\nD\nS\n\n1\n-\n2\nD\nS\n\nL\nX\nD\nS\n\nFigure 4: Comparison of the output of SDXL with previous versions of Stable Diffusion. For each prompt, we\nshow 3 random samples of the respective model for 50 steps of the DDIM sampler Song et al. (2020a) and\ncfg-scale 8.0 Ho & Salimans (2022). Additional samples in Fig. 15.\n\nis to (i) resize an image such that the shortest size matches the desired target size, followed by (ii)\nrandomly cropping the image along the longer axis. While random cropping is a natural form of data\naugmentation, it can leak into the generated samples, causing the malicious effects shown above.\n\nTo fix this problem, we propose another simple yet effective conditioning method: During dataloading,\nwe uniformly sample crop coordinates ctop and cleft (integers specifying the amount of pixels cropped\nfrom the top-left corner along the height and width axes, respectively) and feed them into the model\nas conditioning parameters via Fourier feature embeddings, similar to the size conditioning described\nabove. The concatenated embedding ccrop is then used as an additional conditioning parameter.\nWe emphasize that this technique is not limited to LDMs and could be used for any DM. Note that\ncrop- and size-conditioning can be readily combined. In such a case, we concatenate the feature\nembedding along the channel dimension, before adding it to the timestep embedding in the UNet.\nAlg. 1 illustrates how we sample ccrop and csize during training if such a combination is applied.\nGiven that\nin our experience large scale\ndatasets are, on average, object-centric, we set\n(ctop, cleft) = (0, 0) during inference and thereby\nobtain object-centered samples from the trained\nmodel.\n\nRequire: Training dataset of images D\nRequire: Target image size for training s = (htgt, wtgt)\nRequire: Resizing function R\nRequire: cropping function function C\nRequire: Model train step T\n\nAlgorithm 1 Size- and crop-micro-conditioning\n\nSee Fig. 5 for an illustrati\n...[truncated]",
    "https://aclanthology.org/2025.acl-long.210.pdf": "SNIPPET: ... ACL 2025); Copy Citation: BibTeX. Markdown MODS XML Endnote More options… PDF: https://aclanthology.org/2025.acl-long.210.pdf · PDF Cite Search Fix data. Export ...\n\nTITLE: (from PDF)\n\nBODY:\n4163\nProceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 4163–4183\nJuly 27 - August 1, 2025 ©2025 Association for Computational Linguistics\n\nSegment-LevelDiffusion:AFrameworkforControllableLong-FormGenerationwithDiffusionLanguageModelsXiaochenZhu,GeorgiKaradzhov,ChenxiWhitehouse,AndreasVlachosUniversityofCambridge{xz479,gmk34,cj507,av308}@cam.ac.ukAbstractDiffusionmodelshaveshownpromiseintextgeneration,butoftenstrugglewithgenerat-inglong,coherent,andcontextuallyaccuratetext.Token-leveldiffusiondoesn’tmodelword-orderdependenciesexplicitlyandoperatesonshort,fixedoutputwindows,whilepassage-leveldiffusionstruggleswithlearningrobustrepresentationsforlong-formtext.Toaddressthesechallenges,weproposeSegment-LevelDiffusion(SLD),aframeworkthatenhancesdiffusion-basedtextgenerationthroughtextsegmentation,robustrepresentationtrainingwithadversarialandcontrastivelearning,andimprovedlatent-spaceguidance.Bysegment-inglong-formoutputsintomultiplelatentrep-resentationsanddecodingthemwithanau-toregressivedecoder,SLDsimplifiesdiffusionpredictionsandimprovesscalability.Experi-mentsonfourdatasetsdemonstratethat,whencomparedtootherdiffusionandautoregressivebaselinesSLDachievescompetitiveorsuperiorfluency,coherence,andcontextualcompatibil-ityinautomaticandhumanevaluations.11IntroductionTransformer-basedautoregressive(AR)languagemodelshavebecometheprevailingstandardinnat-urallanguagegeneration(Vaswanietal.,2017;Zhaoetal.,2023).However,thenatureofnext-tokenpredictioninherentlymakesthempronetoer-rorpropagationandincorrecthandlingoflong-termdependencies,whilealsocomplicatingcontrollablegeneration(Heetal.,2021;Wuetal.,2018).Diffusionmodels,whicharenon-autoregressive(NAR)generativemodelswidelysuccessfulinim-ageandvideogeneration,havealsoshownpromiseintextgeneration(Hoetal.,2020;Radfordetal.,2021;Singeretal.,2023).Lietal.(2022)pio-neeredtheapplicationofdiffusionmodelstodis-cretetextgenerationbypredictingcontinuousword1Ourcodeisavailableat:https://github.com/SpaceHunterInf/Segment_Level_DiffusionEncoder𝑖!𝑖\"𝑖#…<𝑠>𝑜!𝑜$%!…𝑜!𝑜\"𝑜$…ℎ&ℎ!ℎ#…Encoder OutputsDiffusion&𝒛’&&𝒛’!&𝒛’(…DiffusionEncoder Outputs…DiffusionDiffusion…&𝒛!&𝒛!!&𝒛!(…<𝑠>&𝒛&&&𝒛&!&𝒛&(…Autoregressive Encoder DecoderLatent Diffusion for Text GenerationSegment-Level Diffusion (Ours)&𝒛&\t&𝒛’%!\t&𝒛’\t)𝒁’\t)𝒁’%!\t)𝒁&\tEncoder OutputsDecoder <𝑠>𝑜!𝑜$%!…𝑜!𝑜\"𝑜$…Decoder 𝑜)…𝑜)%!𝑜!…Decoder Segment 1……𝑜*%!𝑜$…𝑜$%!𝑜*…Decoder Segment 𝑗…Figure1:ComparisonofARmodels(top),latentdiffu-sion(middle),andoursegment-leveldiffusion(bottom).Unlikelatentdiffusion,whichde-noisesasinglelatentrepresentation,ourmethodsplitsoutputsandrepresen-tationintosegmentsasthecross-attentiontargetforconditionalgenerationwithparallelautoregressivede-coding,improvingtextqualityandcontrollability.embeddings.Buildingonthiswork,Linetal.(2023)introducedGENIE,apre-traineddiffusionlanguagemodelthatenhancessemanticunderstand-ingthroughcontinuousparagraph-levelde-noising.Theseapproachesfallundertoken-leveldiffusion,astheydirectlygeneratewordembeddings.Incontrast,Lovelaceetal.(2023)proposedlatentdiffusionfortextgeneration(LD4LG),encodingtextintolatentrepresentations,applyingdiffusiontohigh-levelsemanticstructures,anddecodingthemintotextusinganARdecoder.Fordialoguesdatasetswithcomplexintentandactions(Chenetal.,2021;Huetal.,2023;Gliwaetal.,2019),ChenandYang(2023)usesdiffusionmodelsforcontrollabledialoguegeneration,operatingonhigh-leveldiscourserepresentationstoenableprecisecontroloverthesemanticsofgenerateddialogues.However,existingdiffusionlanguagemodels\f4164\n\nfacechallengesingeneratinglongertexts(Yangetal.,2023;Zouetal.,2023).Insuchcasestoken-leveldiffusionbecomescomputationallyexpensive,asiteitherrequirespre-trainingwithlargeroutputwindows,orreliesoniterativediffusionsampling(Tangetal.,2023;Yietal.,2024).Thefixedout-putwindowisalsowastefulwhenthegeneratedsequenceisshorter.Additionally,unlikeARmeth-ods,theydonotmodelword-orderdependenciesexplicitly,oftenresultinginungrammaticalorinco-herentoutput.Generatinglatentrepresentationsforpassageswithmultiplesentencesisharder,sincetheyarehighlysensitivetonoisewhichcanleadtoabruptchangesofmeaninginthedecodedtext,andlearningasmoothlatentdistributionischallenging(Vahdatetal.,2021;Zhangetal.,2023).Toaddresstheselimitations,weproposeanovelapproachfordiffusion-basedtextgeneration,Segment-LevelDiffusion(SLD),illustratedinFig-ure1.Inspiredbytheconceptofimagepatches(Dingetal.,2024),weuseadiffusionmodeltoperformhigh-levelsemanticsandstructuralplan-ning,generatingalatentrepresentationforeachsegment(e.g.,sentencesinparagraphs,utterancesindialogues),insteadofhandlinglongtextswithasinglelatentrepresentation.Then,anARde-coderdecodespredictedrepresentationstotexts.Toimprovetextgeneration,weintegrateadversar-ialtraining(Miyatoetal.,2017)andcontrastivelearning(Gaoetal.,2021)tosmoothenthelatentrepresentationdistributionandoptimizetheARdecoderwithrespecttothediffusionprocess.WecompareourSLDmodelagainstthreediffu-sionmodels,GENIE(Linetal.,2023),LD4LG(Lovelaceetal.,2023),Diffuse-CG(ChenandYang,2023),andanautoregressivebaseline,Flan-T5(Chungetal.,2024).Theevaluationincludessummarization(XSum,Narayanetal.2018),title-to-storygeneration(ROCStories,Mostafazadehetal.2016),summary-to-dialoguegeneration(DialogSum,Chenetal.2021),andmultipartydecision-makingdialoguegeneration(DeliData,Karadzhovetal.2023).Evaluationbybothauto-maticandhumanmetricsshowsthatSLDgeneratestextthatismorecoherentandfluent,betteralignedwiththeprovidedinput,andmatchesground-truthreferencesmoreclosely.2RelatedWorkToken-LevelDiffusionLietal.(2022)adapteddif-fusionmodelfordiscretetextgenerationbyoper-atinginthecontinuousspaceofwordembeddingsjointlylearnedbythemodel.Thearchitectureit-erativelyde-noisessampledGaussiannoiseintoasequenceofwordvectors.Aroundingmethodisthenappliedtoprojecttheembeddingspredictedintothenearestembeddings.Extendingthiswork,Gongetal.(2023a)appliedtoken-leveldiffusiontosequence-to-sequencegenerationtasks.Linetal.(2023)advancedthisapproachbyincorporatingpre-training,whichenhancedsemanticandsyntac-ticcoherencebytrainingdiffusiondecoderstore-constructcleanparagraphsfromcorruptedembed-dings.Thesemodelsachievesequence-to-sequencegenerationusingencodedtextasclassifier-freeguidance(HoandSalimans,2022).Zhouetal.(2024)unifiesdiscretetextgenerationandcontinu-ousrepresentationsbyusingBART(Lewisetal.,2020)withself-promptingtorecovermaskedto-kens.However,token-leveldiffusionhasnotablelimi-tations.UnlikeARdecodingmethodsthatalwaysconditiononpreviouslydecodedtokens,NARgen-erationdoesnotmodelword-orderdependenciesexplicitly,oftenresultingintextthatlacksgram-maticalcorrectnessandfluency.Furthermore,thefixedoutputwindowrestrictsthelengthofthegen-eratedtext.Itiscomputationallyexpensivetore-traintheentiretoken-leveldiffusionmodelwithlargeroutputwindows,evenmoresoforarchitec-tureswithoutapre-trainedlanguagemodelback-bone(GulrajaniandHashimoto,2023;Louetal.,2023;Austinetal.,2021).Eventhoughexistingliteraturehasaccelerateddiffusionsampling(Gongetal.,2023b;Tangetal.,2023),token-leveldiffu-sionremainsinefficientifthegeneratedsequenceisshorterthantheoutputwindow,asNARdecodingalwaysgeneratesthefulloutput.Passage-LevelDiffusionLovelaceetal.(2023)builtontheconceptoflatentspacediffusion(Rom-bachetal.,2022)bycompressingandpredict-ingtextsusinghigh-levelsemanticrepresentations,ratherthandirectlypredictingfine-grainedtokenrepresentations.Suchcompressionisbeneficialforbothperformanceandefficiency,asitprovidesalength-independentrepresentationandremovesinformationnotneededfordiffusionprediction,incontrasttorepresentationsfromtraditionallan-guageencoders.AseparateARdecoderisem-ployedtoensurethefluencyofthegeneratedtext.However,thisapproachprimarilyfocusesonshorttextgeneration,aslearningrobustlatentrep-\f4165\n\nresentationsforlongpassagesremainschalleng-ing,anditiscrucialtoensurethesmoothnessofthelearneddistributionforhigh-qualitygeneration(Vahdatetal.,2021).Withoutproperregulariza-tion,thelearneddistributionmaybesusceptibletoabruptsemanticchangesduetosmallperturbations,increasingthedifficultyofthetaskforthediffusionmodel.AlthoughZhangetal.(2023)proposedtechniquestoimprovethedistributionalsmooth-nessoflatentrepresentations,thecorrespondencebetweenlatentrepresentationsandspecificcompo-nentsofthegeneratedtextremainsunclear.Thisambiguitycomplicatesfine-grainedguidanceandlimitscontroloverthegenerationprocess.Theselimitationsresultinexistingtoken-andpassage-leveldiffusionmodelsstrugglingtogen-eratelongandcoherenttext.Despitetheirabilitytogenerateoutputsupto64tokensinlength,theywereprimarilyevaluatedontasksinvolvingshorttextgeneration(e.g.,QQPparaphrasing,XSumsummarization)withoutputstypicallyaround30tokensorless(Gongetal.,2023a;Sharmaetal.,2019;Yietal.,2024;Lietal.,2023).3Segment-LevelDiffusionToaddressthechallengesfacedbydiffusionlan-guagemodelsincontrollablelong-formgeneration,weproposeSegment-LevelDiffusion(SLD).Inthissection,wefirstprovideanoverviewofthelan-guagegenerationprocessusingdiffusionmodelsinlatentspace,asillustratedinFigure1.Wethenintroduceourimprovements,offeringanoverviewofthethreetrainingstagesofSLD,asillustratedinFigure2:outputsegmentation,representationlearning,andtrainingdiffusionprocessingforse-manticplanning.DetailedtrainingalgorithmoftheourmodelisoutlinedinAppendixasAlgorithm1.3.1FormulationGivenaninputtextsequencei={i1,i2,...,in}consistingofntokensandanoutputsequenceo={o1,o2,...,om}consistingofmtokens,wemodeltheconditionalprobabilityp(o|i)usingalearnablediffusionmodelR(;θR).WefollowLovelaceetal.(2024)byintroducingadditionalen-codinganddecodingcomponentstoconverttextsintocontinuouslatentrepresentations.Thegoldoutputisfirstencodedintolanguagemodelhiddenstates,thenprojectedtolatentspaceasvariablez.ThediffusionmodeloperatesonthecontinuouslatentvariableszacrossTtimesteps,modelledasaMarkovchain(Sohl-Dicksteinetal.,2015;Hoetal.,2020;SongandErmon,2020),andconsistsoftwoprocesses:abackwardprocessforinferenceandaforwardprocessfortraining.InferenceThebackwardprocessgeneratesthela-tentrepresentationofthepredictedoutputtextˆobyiterativelyremovingnoisefromaninitialnoisysample.StartingwithavariableˆzT∼N(ˆzT;0,I),thediffusionmodelwithparametersθRpredictsthede-noisedvariableˆzt−1ateachtimesteptasfollows:p(ˆzt−1|ˆzt;θR)=N(cid:16)ˆzt−1;µt−1θR,σt−1θR2(cid:17)(1)whereµθRandσθRarethepredictedmeanandvarianceateachtimestep.Thediffusionmodel,R(;θR),estimatesˆzt−1.Itconditionsontheinputsequencei,usingtheencoderoutputsfromapre-trainedtextencoderEncctx(;θctx)asbelow:ˆzt−1=R(ˆzt,t,Encctx(i;θctx);θR)(2)ThemodelkeepsrefiningthenoisysampleˆzTwithrespecttotheinputsequenceitorecoverˆz0whichwillbeconvertedtotext.Thepredictedlatentrep-resentationˆz0ispassedtoafunctionparameterisedbyθg,whichreconstructsittomatchtheinputdi-mensionsofanARdecoderwithparametersθdecfordecoding:g(ˆz0;θg)∈Rk×hlm,ˆo=Dec(g(ˆz0;θg);θdec).(3)TrainingThediffusionmodelR(;θR)istrainedbyminimizingaregressionlosstopredictthenoiseaddedduringtheforwardprocess.Intheforwardprocess,anoriginalrepresentationz0ofencodedofromatraininginstance(i,o)∼DisgraduallycorruptedintoGaussiannoiseoverTtimesteps.Theencodingprocessconsistsofanencoderwithparametersθencthatencodestheoutputtexts:Enc(o;θenc)∈Rm×hlm(4)andacompressionfunctionwithparametersθfthatprojectsencoderoutputsintoalength-independentlatentspaceusing:z=f(Enc(o;θenc);θf)∈Rk×hrep(5)Here,wereducethedimensionofencoderoutputstoafixed-lengthrepresentationwithk≤mandhrep≪hlm.Thisreducesthedifficultyofdiffu-sionmodellearningfromasparsedistribution.Thecorruptionismodelledas:q(zt|zt−1)=N(zt;p1−βtzt−1,βtI)(6)\f4166\n\nMorgan and her family lived in Florida.They heard a hurricane was coming.They felt lucky they had evacuated when they did.…Segment 0Segment 1Segment nGold Output:Morgan and her family lived in Florida. They heard a hurricane was coming. They decided to evacuate to a relative's house. They arrived and learned from the news that it was a terrible storm. They felt lucky they had evacuated when they did.Stage 1: Text SegmentingStage 2: Representation LearningLanguage AutoencoderLatent CompressionMorgan and her family lived in Florida.Morgan lived in Floridawith her family.Shaun Maloney admits guarding his…𝒛𝒛!𝒛\"Latent Space𝒛+Adversarial NoiseMorgan lives with her family in Florida.Language DecoderLatent Reconstruction!𝒛!\"…!𝒛!#!𝒛!$𝑅(%,𝑇;𝜃%)Pretrained Text EncoderInput:Title: The Hurricane!𝒛!&#\"…!𝒛!&##!𝒛!&#$𝑅(%,𝑇−1;𝜃%)!𝒛\"\"…!𝒛\"#!𝒛\"$…Conditioned Reverse ProcessDiffusion Transformer 𝑅(%,𝑡;𝜃%) generates predicted segments representations by recovering from a sampled Gaussian noise, conditioned on sequence-to-sequence input.Language DecoderLatent ReconstructionMorgan lives in Florida.A hurricane strikes her house.Morgan and her family are rescued.…Texts Decoded in ParallelSegment 0Segment 1Segment n.𝒁!.𝒁!&#.𝒁\"Stage 3: Training Diffusion as Inherent Semantic PlannerEncoder OutputsFigure2:OverviewofthetrainingpipelineofSLD.Inthefirststage,goldoutputisdividedintosegments.Inthesecondstage,weusecontrastiveandadversariallearningtoensurelatentrepresentationsarerobusttodrasticsemanticchanges.Finally,wetrainadiffusionmodelasaninherentsemanticplannerconditionedongiveninputs.whereβtcontrolsthevarianceoftheaddednoiseateachstep.Theobjectiveistominimizethedistancebetweenthepredictedrepresentationˆztandthetrueposteriorztwhichiscomputedinclosedformbysamplingfromtheforwardprocess.L(θR)=TXt=1Eq(zt|z0)∥ˆzt−zt∥22(7)Thelossabovetrainsthemodeltoiterativelyre-versethecorruptionappliedduringtheforwardprocess,enablinghigh-qualitydatagenerationinthebackwardprocess.3.2SegmentedTextGenerationInspiredbytheconceptofimagepatches(Dingetal.,2024),inthefirststage,wesegmentlongoutputsintosmallersegments,suchassentencesordialogueutterances,ratherthanprojectingtheentireoutputintoasinglelatentspacerepresenta-tion.Thissegmentationeffectivelyreducesthesizeandcomplexityofeachlatentrepresentation,sim-plifyingdiffusionpredictionsandenablinggreaterflexibilityforscaling,allowingthemodeltohan-dlelong-formtextmoreefficiently.Formally,weconstructP={p1,...,pj},whereeachpjcorre-spondstoanon-overlappingcontiguoussegmentoftokensino.ThisprocessyieldsasetoflatentrepresentationsZ={z1,...,zj},afterencoding,establishingaone-to-onecorrespondencebetweeneachsegmentanditsrespectivelatentrepresenta-tion.AsshowninFigure2,astoryisdividedintonsegments(sentencesinthiscase),andthediffusionmodelwillpredictthesamenumberofrepresenta-tionsanddecodethemintonsegmentsinparallel.3.3LearningLatentRepresentationsforRobustDecodingAsmentionedearlier,performingdiffusioninla-tentspacefortextgenerationrequirestrainingthelanguageencoderEnc(;θenc),latentcom-pressionf(;θf),reconstructiong(;θg),andde-coderDec(;θdec).Astraightforwardapproachistousetheconversionlossincurredduringde-codingˆp=Dec(g(z;θg);θdec),wherez=f(Enc(p;θenc);θf),forapatchoftextp={o1,...,op}.Wedenotetheparameterscollec-tivelyasθin={θenc,θf}fortheencodingandc\n...[truncated]",
    "https://openaccess.thecvf.com/content/CVPR2024/papers/Li_On_the_Scalability_of_Diffusion-based_Text-to-Image_Generation_CVPR_2024_paper.pdf": "SNIPPET: In this paper, we investigate the scaling properties for training diffusion models, especially on the denoising back- bone and dataset. The goal is to ...\n\nTITLE: (from PDF)\n\nBODY:\nOn the Scalability of Diffusion-based Text-to-Image Generation\n\nHao Li1,2, Yang Zou1,2, Ying Wang1,2, Orchid Majumder1,2, Yusheng Xie1,2, R. Manmatha1,\nAshwin Swaminathan1,2, Zhuowen Tu1, Stefano Ermon1, Stefano Soatto1\n1AWS AI Labs, 2Amazon AGI\n{haolimax, yanzo, lyiwang, orchid, yushx, manmatha, swashwin, ztu, ermons, soattos}@amazon.com\n\nAbstract\n\nScaling up model and data size has been quite successful\nfor the evolution of LLMs. However, the scaling law for\nthe diffusion based text-to-image (T2I) models is not fully\nexplored. It is also unclear how to efﬁciently scale the model\nfor better performance at reduced cost. The different train-\ning settings and expensive training cost make a fair model\ncomparison extremely difﬁcult. In this work, we empirically\nstudy the scaling properties of diffusion based T2I models by\nperforming extensive and rigours ablations on scaling both\ndenoising backbones and training set, including training\nscaled UNet and Transformer variants ranging from 0.4B to\n4B parameters on datasets upto 600M images. For model\nscaling, we ﬁnd the location and amount of cross attention\ndistinguishes the performance of existing UNet designs. And\nincreasing the transformer blocks is more parameter-efﬁcient\nfor improving text-image alignment than increasing channel\nnumbers. We then identify an efﬁcient UNet variant, which is\n45% smaller and 28% faster than SDXL’s UNet. On the data\nscaling side, we show the quality and diversity of the train-\ning set matters more than simply dataset size. Increasing\ncaption density and diversity improves text-image alignment\nperformance and the learning efﬁciency. Finally, we provide\nscaling functions to predict the text-image alignment perfor-\nmance as functions of the scale of model size, compute and\ndataset size.\n\n1. Introduction\n\nScaling up model and dataset size has been the key enabling\nfactor for the success of LLMs [17, 21] and VLMs [6, 32].\nThe scaling law [4, 21] governs the expectation of perfor-\nmance as a function of dataset, model size and compute\nbudget. However, the scaling properties for recent diffusion\nbased Text-to-Image (T2I) models [31, 33–35] are not well\nstudied. Though there is emerging trend that T2I models can\nbe improved with larger denoising backbones [9, 31] and\nstronger text-encoders [1, 31, 35], it is still not clear how\nto effectively and efﬁciently scale up diffusion models, e.g.,\n\nFigure 1. Pushing the Pareto frontier of the text-image alignment\nlearning curve by efﬁciently scaling up both denoising backbones\nand training data. Comparing with the baseline SD2 UNet [34], the\ncombined scaling with both SDXL UNet and enlarged dataset sig-\nniﬁcantly increases the performance and speeds up the convergence\nof TIFA score by 6\n\n.\n\n⇥\n\nhow does the design of denoising backbone inﬂuence the\nimage generation and which components are more effective\nto scale? How should diffusion model scale when the train-\ning data increases? To answer the questions, it is essential\nto understand how exactly each new model improves over\nprevious ones. However, existing diffusion based T2I mod-\nels are mostly trained with different datasets, input space\n(latent space or pixel space) and training settings. Moreover,\nthe expensive training cost of high resolution models makes\nthe fair comparison extremely hard, not to mention explor-\ning new ones. Therefore, a fair and controlled comparison\nof different denoising backbones is greatly desired, which\ncan enable seeking of more efﬁcient models with reduced\ntraining and inference cost.\n\nIn this paper, we investigate the scaling properties for\ntraining diffusion models, especially on the denoising back-\nbone and dataset. The goal is to understand which dimension\n\nThisCVPRpaperistheOpenAccessversion,providedbytheComputerVisionFoundation.Exceptforthiswatermark,itisidenticaltotheacceptedversion;thefinalpublishedversionoftheproceedingsisavailableonIEEEXplore.9400\fof the model is more effective and efﬁcient to scale, how to\nproperly scale the dataset, and the scaling law among mod-\nels, dataset and compute. Fig.1 gives an illustration of how\nthe Pareto frontier of the text-image alignment performance\ncurve can be pushed via proper scaling.\n\n1.1. What we have done\n\n• Comparing existing UNets in a controlled environment:\nwe ﬁrst compare existing UNet designs from SD2 [34],\nDeepFloyd [9] and SDXL [31], to understand why cer-\ntain UNet design is signiﬁcantly better than others. To\nallow a fair comparison, we train all models with the same\ndataset, latent space, text encoder and training settings.\nWe monitor multiple evaluation metrics during training, in-\ncluding composition scores and image quality scores. We\nveriﬁed SDXL’s UNet achieves superior performance over\nothers with similar amount of parameters, which justiﬁes\nthe importance of architecture design.\n\n• Scaling UNet and comparing with Transformers: To\nunderstand why SDXL works so well, we conduct ex-\ntensive ablation studies on the design sapce of UNet by\ninvestigating 15 variations ranging from 0.4B to 4B pa-\nrameters, especially on the choice of channel numbers\nand transformer depth. We show how each architecture\nhyperparameter affects the performance and convergence\nspeed. Similarly, we ablate and scale the Transformer\nbackbones [5, 30] and compare with UNet.\n\n• Ablating the effect of dataset scaling and caption en-\nhancement: We study how different dataset properties\naffect the training performance, including dataset size, im-\nage quality and caption quality. We curate two large-scale\ndatasets with 250M and 350M images, both are augmented\nby synthetic captions. We train both small and large mod-\nels to see how they can beneﬁt from dataset scaling.\n\n1.2. Contributions\n\n• We conduct large-scale controlled experiments to allow\nfair comparison across various denoising backbones for\nT2I synthesis, including both UNets and Transformers.\nOur work veriﬁes the importance of the denoising back-\nbone design. We ﬁnd composition ability is mainly devel-\noped at low resolution, which enables fast model ablations\nwithout training in high resolution. To our best knowledge,\nour work is the ﬁrst large-scale controlled study allowing\nfair comparison across different denoising backbones for\nT2I syntheis.\n\n• We ablate the key design factors for UNet and Transform-\ners and compared their scaled versions. We show scalling\nthe transformer depth in UNet is more parameter efﬁcient\nin improving the alignment performance in comparison\nwith channel number. We identify an efﬁcient UNet vari-\nant that is 45% smaller and 28% faster than SDXL while\nachieving similar performance. We conﬁrm scaling trans-\n\nformer backbone improves performance, but also identify\nthe difﬁculty of training from scratch due to lack of induc-\ntive bias in comparison with UNets.\n\n• We show that properly scaling training data with synthetic\ncaptions improves image quality and speeds up the con-\nvergence. We see data scaling can improve small model’s\nperformance signiﬁcantly, a better designed model can\nhave a higher performance upper bound.\n\n2. Related Work\n\nDiffusion Models Diffusion models [15, 16, 28, 29, 36]\nsynthesize samples via an iterative denoising process and\nhave shown superior performance over GAN [13] based\nmethods for image generation [10]. Recent diffusion based\nT2I models such as Imagen [35], LDM/SD2 [34], Deep-\nFloyd [9], SDXL [31], and DALL\nE [3, 33] have shown\n·\nconsistently improved performance in terms of sample diver-\nsity, text-image alignment and image ﬁdelity. Pixel-based\nmodels [9, 33, 35] usually require cascaded super-resolution\n(SR) models to upscale images generated in low resolution,\nwhile LDMs [3, 31, 34] reduce training cost by utilizing a\ncompressed latent space and upsampling with via an autoen-\ncoder [22]. The low resolution latent space may not represent\nsmall objects (e.g., faces) well. SDXL mitigates this issue\nvia a better VAE and training models in higher resolution\nlatent space (128\n128). Emu [8] shows that increasing the\nlatent channels improves image quality.\n\n⇥\n\nScaling UNets UNet architecture was ﬁrst introduced for\ndiffusion models in [16]. [10, 28] ablated UNet with several\ndesign choices and investigated how FID scales as a function\nof training compute. The UNet in LDM or SD2 [34] has\n320 initial channels and 850M parameters. DeepFloyd [9]\ntrains a pixel based model with a UNet of 4B parameter size\nand 704 channels, which shows better performance than its\nsmaller versions. SDXL [31] employs a 3\nlarger UNet than\nSD2 with mutiple improvements. On the other hand, there\nare also works on improving UNet’s efﬁciency by scaling it\ndown, e.g., SnapFusion [25] studies the redundancy of UNet\nand identiﬁes an efﬁcient version by employing the change of\nCLIP/Latency to measure the impact of architecture change.\n\n⇥\n\nTransformer Backbones Recently there is surge inter-\nest in using Transformer [38] to replace UNet for its gen-\neral architecture design and increased scalability [2, 30,\n41]. DiT [30] replaces UNet with Transformers for class-\nconditioned image generation and ﬁnd there is a strong cor-\nrelation between the network complexity and sample quality.\nU-ViT [2] shows comparable performance can be achieved\nby ViTs with long skip connection. MDT [12] introduces a\nmask latent modeling scheme to improve the training efﬁ-\nciency of transformer-based diffusion models. Those works\n\n9401\f4\n\nC\n\nC\n\n!0 !0\n\n2C\n\n2C 2C\n\n!1\n\n!1\n\n2\n3\nx\n2\n3\n\n4C\n\n!2\n\n6\n1\nx\n6\n1\n\n!2\n\n8\nx\n8\n\n8C\n\n!2\n\n8\nx\n8\n\n\"0, \"1, \"2 = 1, 1, 1\n\n!!\n\n4\n6\nx\n4\n6\n\ni\n\ng\nn\nd\nd\ne\nb\nm\ne\n\nt\nx\ne\nT\n\n!0 !0\n\n!!\"#\n\n2C\n\n!1\n\n!1\n\n!2\n\n!2\n\n4\n\nC\n\nC\n\n2C\n\n2C 2C\n\n!1\n\n!1\n\n2\n3\nx\n2\n3\n\n4C\n\n!2\n\n6\n1\nx\n6\n1\n\n!!\n\n4\n6\nx\n4\n6\n\ni\n\ng\nn\nd\nd\ne\nb\nm\ne\n\nt\nx\ne\nT\n\n!2\n\n!2\n\n!2\n\n6\n1\nx\n6\n1\n\n4C\n\n!2\n\n\"0, \"1, \"2 = 0, 2, 10\n\n3x3 conv\n\nResidual Block\n\nDown/Up Sampling\n\n!\n\n\"\tTransformer Blocks \nwith Cross Attention\n\nIdentity Mapping\n\n! Channel number\n\n!!\"#\n\n2C\n\n!1\n\n!1\n\nFigure 2. Comparison of the UNet design between SD2 (left) and SDXL (right). SD2 applies cross-attention at all down-sampling levels,\nincluding 1\n\n, while SDXL adopts cross-attention only at 2\n\ndown-sampling levels.\n\nand 4\n\nand 8\n\n, 4\n\n, 2\n\n⇥\n\n⇥\n\n⇥\n\n⇥\n\n⇥\n\n⇥\n\nare mostly class conditioned models and only the effect of\nmodel architecture on image ﬁdelity is studied. PixArt-↵ [5]\nextends DiTs [30] for text-conditioned image generation.\nMore recently, SD3 [11] propose MM-DiT design and ﬁnd\nit scales well.\n\n3. Scaling Denoising Backbone\n\n3.1. Existing UNet Design\n\nThe UNet in diffusion models adopts a stack of residual\nblocks and a sequence of downsampling and upsampling\nconvolutions, along with additional spatial attention lay-\ners at multiple resolutions [10, 16]. Recent T2I frame-\nworks [9, 31, 34] mostly employ the ideas in simple dif-\nfusion [18] to improve the efﬁciency of UNet, i.e., tweaking\nmore parameters and computation at smaller resolutions.\nFig. 2 gives a comparison of the UNets for SD2 and SDXL.\nSDXL improves over SD2 in multiple dimensions: a) Less\ndownsampling rates. SD2 uses [1, 2, 4, 4] as the multipli-\ncation rates to increase channels at different downsampling\nlevels. DeepFloyd adopts [1, 2, 3, 4] to reduce computa-\ntion, while SDXL uses [1, 2, 4], which completely removes\nthe 4th downsampling level. b) Cross-attention only at\nlower resolution. Cross-attention is only computed at cer-\ntain downsampling rates, e.g., SD2 applies cross-attention at\n), while SDXL\n, 2\nﬁrst three downsampling rates (1\n⇥\nonly integrates text embedding at the 2\ndownsam-\n⇥\npling levels. c) More compute at lower resolution. SDXL\napplies more transformer blocks at the 2\ndownsam-\npling levels, while SD2 applies uniform single transformer\nblock at all three downsampling levels.\n\n⇥\nand 4\n\nand 4\n\n, 4\n\n⇥\n\n⇥\n\n⇥\n\n⇥\n\n3.2. Controlled Comparison of UNets\n\nTo allow fair comparison of different UNets, we train all\nbackbone variants in the same controlled settings, including\nthe same dataset, latent space, text-encoder and training\n\nsettings. Below we introduce the training conﬁgurations and\nevaluation metrics, based on which we compare all different\nbackbones variants.\n\nTraining We train models on our curated dataset LensArt,\nwhich contains 250M text-image pairs (details in Sec 4). We\nuse SDXL’s VAE and the OpenCLIP-H [20] text encoder\n(1024 dim), without adding extra embedding layer or other\n256 resolution\nconditioning. We train all models at 256\nwith batch size 2048 upto 600K steps. We follow the setup\nof LDM [34] for DDPM schedules. We use AdamW [27]\noptimizer with 10K steps warmup and then constant learning\nrate 8e-5. We employ mixed precision training with BF16\nand enables FSDP for large models.\n\n⇥\n\nInference and Evaluation We use DDIM sampler [37]\nwith 50 steps and ﬁxed seed and CFG scale for inference. To\nunderstand the training dynamics, we monitor the evolution\nof ﬁve metrics during training. We ﬁnd the the metrics at\nearly stage of training can help predict ﬁnal model perfor-\nmance. Speciﬁcally, we measure composition ability and\nimage quality with metrics including: 1) TIFA [19], which\nmeasures the faithfulness of a generated image to its text\ninput via visual question answering (VQA). It contains 4K\ncollected prompts and corresponding question-answer pairs\ngenerated by a language model. Image faithfulness is calcu-\nlated by checking whether existing VQA models can answer\nthese questions using the generated image. TIFA allows\nfor ﬁne-grained and interpretable evaluations of generated\nimages. 2) ImageReward [40] which was learned to ap-\nproximates human preference. We calculate the average\nImageReward score over images generated with MSCOCO-\n10K prompts. Though ImageReward is not a normalized\nscore, its scores are in the range of [-2, 2] and the average\nscore over the number of images gives meaningful statis-\ntics to allow comparision across models. Due to space\n\n9402 \n \n\f3.3. Ablation of UNet Design\n\nNow we have veriﬁed SDXL has a much better UNet design\nthan SD2 and DeepFloyd variants. The question is why it\nexcels and how to further improve it effectively and efﬁ-\nciently. Here we investigate how to improve SDXL’s UNet\nby exploring its design space.\n\nSearch Space Table 1 shows different UNet conﬁgura-\ntions, and their corresponding compute complexity at 256\nresolution. We mainly vary the initial channels and trans-\nformer depth. To understand the impact of each dimension\nof the design space, we select a subset of the variant models\nand train them with the same conﬁgurations. This forms\nour main “search space” for the UNet architecture. More\nablations on the impact of VAE, training iterations and batch\nsize can be found in Appendix.\n\nThe Effect of Initial Channels We train the following\nSDXL UNet variants with different channel numbers: 128,\n192, and 384, with parameters 0.4B, 0.9B and 3.4B, respec-\ntively. Fig. 4 (a) shows that UNet with reduced channel\n...[truncated]"
  },
  "all_scored_papers": {},
  "search_candidate_set": [],
  "selected_urls_set": [
    "https://www.ijcai.org/proceedings/2023/0750.pdf",
    "https://aclanthology.org/2025.naacl-long.532.pdf",
    "https://iclr.cc/virtual/2025/poster/29614",
    "https://arxiv.org/abs/2402.10397",
    "https://proceedings.iclr.cc/paper_files/paper/2024/file/081b08068e4733ae3e7ad019fe8d172f-Paper-Conference.pdf",
    "https://arxiv.org/abs/2306.08236",
    "https://arxiv.org/abs/2503.03595",
    "https://arxiv.org/abs/2410.15119",
    "https://peerj.com/articles/cs-1905.pdf",
    "https://openreview.net/forum?id=tyEyYT267x",
    "https://iclr.cc/virtual/2025/poster/29366",
    "https://arxiv.org/abs/2303.06574",
    "https://openreview.net/forum?id=l2zFn6TIQi",
    "https://aclanthology.org/2025.acl-long.210.pdf",
    "https://arxiv.org/abs/2409.13031",
    "https://aclanthology.org/2023.findings-acl.721.pdf",
    "https://aclanthology.org/2024.naacl-long.2.pdf",
    "https://blog.genlaw.org/pdfs/genlaw_icml2024/42.pdf",
    "https://ai-ml.cn/iclr25",
    "https://arxiv.org/abs/2410.06014",
    "https://openaccess.thecvf.com/content/CVPR2024/papers/Li_On_the_Scalability_of_Diffusion-based_Text-to-Image_Generation_CVPR_2024_paper.pdf",
    "https://aclanthology.org/2024.naacl-long.261.pdf",
    "https://proceedings.neurips.cc/paper_files/paper/2023/file/7d866abba506e5a56335e4644ebe18f9-Paper-Conference.pdf",
    "https://arxiv.org/abs/2410.05523",
    "https://proceedings.mlr.press/v202/lin23d/lin23d.pdf",
    "https://aclanthology.org/2024.naacl-long.261"
  ],
  "selected_serp_url_set": [
    "https://www.ijcai.org/proceedings/2023/0750.pdf",
    "https://aclanthology.org/2025.naacl-long.532.pdf",
    "https://iclr.cc/virtual/2025/poster/29614",
    "https://arxiv.org/abs/2402.10397",
    "https://proceedings.iclr.cc/paper_files/paper/2024/file/081b08068e4733ae3e7ad019fe8d172f-Paper-Conference.pdf",
    "https://arxiv.org/abs/2306.08236",
    "https://arxiv.org/abs/2503.03595",
    "https://arxiv.org/abs/2410.15119",
    "https://peerj.com/articles/cs-1905.pdf",
    "https://openreview.net/forum?id=tyEyYT267x",
    "https://iclr.cc/virtual/2025/poster/29366",
    "https://arxiv.org/abs/2303.06574",
    "https://openreview.net/forum?id=l2zFn6TIQi",
    "https://aclanthology.org/2025.acl-long.210.pdf",
    "https://arxiv.org/abs/2409.13031",
    "https://aclanthology.org/2023.findings-acl.721.pdf",
    "https://aclanthology.org/2024.naacl-long.2.pdf",
    "https://blog.genlaw.org/pdfs/genlaw_icml2024/42.pdf",
    "https://ai-ml.cn/iclr25",
    "https://arxiv.org/abs/2410.06014",
    "https://openaccess.thecvf.com/content/CVPR2024/papers/Li_On_the_Scalability_of_Diffusion-based_Text-to-Image_Generation_CVPR_2024_paper.pdf",
    "https://aclanthology.org/2024.naacl-long.261.pdf",
    "https://proceedings.neurips.cc/paper_files/paper/2023/file/7d866abba506e5a56335e4644ebe18f9-Paper-Conference.pdf",
    "https://arxiv.org/abs/2410.05523",
    "https://proceedings.mlr.press/v202/lin23d/lin23d.pdf"
  ],
  "created_at": 1760989822.528889,
  "updated_at": 1760989822.5288975
}