{
  "task_id": "task_20251017_211316_eae488dd",
  "spec": {
    "top_n": 3,
    "years": [
      2025,
      2024,
      2026
    ],
    "venues": [
      "ICLR",
      "ICML",
      "NeurIPS",
      "ACL",
      "EMNLP",
      "NAACL",
      "CVPR",
      "ICCV",
      "ECCV",
      "KDD",
      "WWW",
      "SIGIR",
      "AAAI",
      "IJCAI",
      "CHI"
    ],
    "keywords": [
      "text-generation with diffusion model"
    ],
    "must_be_current_student": true,
    "degree_levels": [
      "PhD"
    ],
    "author_priority": [
      "first",
      "last"
    ],
    "extra_constraints": []
  },
  "pos": 12,
  "terms": [
    "text generation with diffusion model ICLR 2026",
    "text generation with diffusion model ICML 2026",
    "text generation with diffusion model NeurIPS 2026",
    "text generation with diffusion model ACL 2026",
    "text generation with diffusion model EMNLP 2026",
    "text generation with diffusion model NAACL 2026",
    "text generation with diffusion model CVPR 2026",
    "text generation with diffusion model ICCV 2026",
    "text generation with diffusion model ECCV 2026",
    "text generation with diffusion model KDD 2026",
    "text generation with diffusion model WWW 2026",
    "text generation with diffusion model SIGIR 2026",
    "text generation with diffusion model AAAI 2026",
    "text generation with diffusion model IJCAI 2026",
    "text generation with diffusion model CHI 2026",
    "text generation with diffusion model ICLR 2025",
    "text generation with diffusion model ICML 2025",
    "text generation with diffusion model NeurIPS 2025",
    "text generation with diffusion model ACL 2025",
    "text generation with diffusion model EMNLP 2025",
    "text generation with diffusion model NAACL 2025",
    "text generation with diffusion model CVPR 2025",
    "text generation with diffusion model ICCV 2025",
    "text generation with diffusion model ECCV 2025",
    "text generation with diffusion model KDD 2025",
    "text generation with diffusion model WWW 2025",
    "text generation with diffusion model SIGIR 2025",
    "text generation with diffusion model AAAI 2025",
    "text generation with diffusion model IJCAI 2025",
    "text generation with diffusion model CHI 2025",
    "text generation with diffusion model ICLR 2024",
    "text generation with diffusion model ICML 2024",
    "text generation with diffusion model NeurIPS 2024",
    "text generation with diffusion model ACL 2024",
    "text generation with diffusion model EMNLP 2024",
    "text generation with diffusion model NAACL 2024",
    "text generation with diffusion model CVPR 2024",
    "text generation with diffusion model ICCV 2024",
    "text generation with diffusion model ECCV 2024",
    "text generation with diffusion model KDD 2024",
    "text generation with diffusion model WWW 2024",
    "text generation with diffusion model SIGIR 2024",
    "text generation with diffusion model AAAI 2024",
    "text generation with diffusion model IJCAI 2024",
    "text generation with diffusion model CHI 2024"
  ],
  "rounds_completed": 2,
  "candidates_accum": {
    "Tong Wu": {
      "Name": "Tong Wu",
      "Email": "****@bit.edu.cn",
      "Current Role & Affiliation": "Postdoc at Beijing Institute of Technology",
      "Current Status": "",
      "Research Keywords": [
        "Privacy Preserving",
        "Public Key Cryptography"
      ],
      "Research Focus": [
        "Privacy Preserving",
        "Public Key Cryptography"
      ],
      "Profiles": {
        "OpenReview": "https://openreview.net/profile?id=~TONG_WU3"
      },
      "Publication Overview": [
        "AR-Diffusion: Auto-Regressive Diffusion Model for Text Generation",
        "Text Generation with Diffusion Language Models: A Pre-training Approach with Continuous Paragraph Denoise",
        "Human-Robot Commensality: Bite Timing Prediction for Robot-Assisted Feeding in Groups"
      ],
      "Top-tier Hits (Last 24 Months)": [
        "Neural Information Processing Systems 2023",
        "International Conference on Machine Learning 2022",
        "Conference on Robot Learning 2022"
      ],
      "Honors/Grants": [],
      "Academic Service / Invited Talks": [],
      "Open-source / Datasets / Projects": [
        ""
      ],
      "Representative Papers": [
        {
          "Title": "AR-Diffusion: Auto-Regressive Diffusion Model for Text Generation",
          "Venue": "Neural Information Processing Systems",
          "Year": 2023,
          "Type": "Conference Paper",
          "Links": "https://www.semanticscholar.org/paper/54b6e5dcef733c151adef0ac06430f63cb301a36"
        },
        {
          "Title": "Text Generation with Diffusion Language Models: A Pre-training Approach with Continuous Paragraph Denoise",
          "Venue": "International Conference on Machine Learning",
          "Year": 2022,
          "Type": "Conference Paper",
          "Links": "https://www.semanticscholar.org/paper/cb648d482dbd1e6ad0b0f4da43aca71c06538d4f"
        },
        {
          "Title": "Human-Robot Commensality: Bite Timing Prediction for Robot-Assisted Feeding in Groups",
          "Venue": "Conference on Robot Learning",
          "Year": 2022,
          "Type": "Conference Paper",
          "Links": "https://www.semanticscholar.org/paper/aaca127cfd0cc4ad7f27b4cdf95e39aac36716e9"
        }
      ],
      "Trigger Paper Title": "AR-Diffusion: Auto-Regressive Diffusion Model for Text Generation",
      "Trigger Paper URL": "https://www.semanticscholar.org/paper/54b6e5dcef733c151adef0ac06430f63cb301a36",
      "Highlights": [],
      "Radar": {
        "Academic Background": 4,
        "Research Output": 5,
        "Research Alignment": 5,
        "Technical Skills": 5,
        "Recognition & Impact": 4,
        "Communication & Collaboration": 3,
        "Initiative & Independence": 5
      },
      "Total Score": 31,
      "Detailed Scores": {
        "Academic Background": "4/5 - Tong Wu is a postdoc at Beijing Institute of Technology, indicating a strong academic foundation. While specific details about their PhD institution or field are not provided, the postdoc position suggests a solid background in computer science or related disciplines.",
        "Research Output": "5/5 - Tong Wu has published high-quality research in top-tier venues such as NeurIPS and ICML, with multiple papers receiving significant citations. This demonstrates a consistent and impactful research output.",
        "Research Alignment": "5/5 - The candidate's interests in privacy-preserving techniques and public key cryptography align well with the research directions of many modern security and machine learning fields, particularly given the increasing importance of secure and private AI systems.",
        "Technical Skills": "5/5 - The publications demonstrate advanced technical skills in areas such as diffusion models, pre-training, and human-robot interaction, suggesting a strong grasp of both theoretical and applied aspects of machine learning and AI.",
        "Recognition & Impact": "4/5 - Tong Wu's work has been cited over 95 times for one paper alone, indicating recognition within the research community. The topics covered also have practical relevance, contributing to broader societal and technological impact.",
        "Communication & Collaboration": "3/5 - There is no explicit information provided about Tong Wu's communication or collaboration efforts, such as co-authorships, presentations, or interdisciplinary work, which limits the assessment of this dimension.",
        "Initiative & Independence": "5/5 - The candidate has published multiple papers in leading conferences, including original work on AR-Diffusion and text generation with diffusion models, demonstrating initiative and the ability to independently drive research projects."
      }
    },
    "Shansan Gong": {
      "Name": "Shansan Gong",
      "Email": "",
      "Current Role & Affiliation": "PhD candidate at HKU, supervised by Lingpeng Kong",
      "Current Status": "PhD candidate at HKU",
      "Research Keywords": [
        "diffusion language models",
        "long context language models"
      ],
      "Research Focus": [
        "diffusion language models",
        "long context language models",
        "generation paradigms",
        "reasoning capacity",
        "text generation"
      ],
      "Profiles": {
        "Homepage": "https://summmeer.github.io/",
        "Google Scholar": "https://scholar.google.com/citations?user=F86VNoMAAAAJ",
        "X (Twitter)": "https://x.com/hkunlp2020",
        "GitHub": "https://github.com/summmeer"
      },
      "Publication Overview": [
        "L-Eval: Instituting Standardized Evaluation for Long Context Language Models",
        "Scaling Diffusion Language Models via Adaptation from Autoregressive Models",
        "Beyond Autoregression: Discrete Diffusion for Complex Reasoning and Planning"
      ],
      "Top-tier Hits (Last 24 Months)": [
        "Annual Meeting of the Association for Computational Linguistics 2023",
        "International Conference on Learning Representations 2024",
        "International Conference on Learning Representations 2024"
      ],
      "Honors/Grants": [
        "2024 ACL, Outstanding Paper",
        "2024 Tencent Rhino-bird Research Elite Program, Outstanding Student",
        "2022 SIGIR Student Travel Award",
        "2022 Outstanding Graduate in Shanghai Municipality",
        "2019 Outstanding Undergraduate in SJTU"
      ],
      "Academic Service / Invited Talks": [
        "Conference Reviewer @ COLING2022 (2022)",
        "Conference Reviewer @ ACL2023 (2023)",
        "Conference Reviewer @ NeurIPS2023- (2023)",
        "Conference Reviewer @ EMNLP2023 (2023)",
        "Conference Reviewer @ ICLR2024- (2024)",
        "Conference Reviewer @ ARR2024- (2024)",
        "Journal Reviewer @ ACM Computing Surveys ()",
        "Journal Reviewer @ IEEE Journals ()",
        "TA at HKU @ COMP2121 (Discrete math) ()",
        "TA at HKU @ COMP7104 (Advanced database systems) ()",
        "One of the hosts of HKU Seminar ()",
        "DiffuSeq — Youth PhD Talk-ICLR 2023 by AI Time (2023)",
        "Incorporate Diffusion Models into Conditional Text Generation — Global Lunch Seminar at SJTU CS department (2023)"
      ],
      "Open-source / Datasets / Projects": [
        "DiffuCoder (project) - We introduce DiffuCoder (7B), show that higher temperature diversifies both token choices and generation order; and propose coupled-GRPO, a diffusion-native RL method that avoids semi-AR and improves performance.",
        "DiffuLLaMA (project) - We convert AR models ranging from 127M to 7B parameters (GPT2 and LLaMA) into diffusion models DiffuGPT and DiffuLLaMA.",
        "DoT (project) - DoT allows the reasoning steps to diffuse over time through the diffusion process.",
        "DiffuSeq (project) - DiffuSeq is a powerful model for text generation, matching or even surpassing competitive AR, iterative NAR, and PLMs on quality and diversity.",
        "GIRAFFE (project) - Explore design choices to extend the context window of existing VLMs.",
        "L-Eval (dataset) - A manually checked benchmark for long context language models with 20 sub-tasks."
      ],
      "Representative Papers": [
        {
          "Title": "DiffuCoder: Understanding and Improving Masked Diffusion Models for Code Generation",
          "Venue": "preprint",
          "Year": null,
          "Type": "Preprint",
          "Links": ""
        },
        {
          "Title": "Scaling Diffusion Language Models via Adaptation from Autoregressive Models",
          "Venue": "ICLR",
          "Year": 2025,
          "Type": "Conference Paper",
          "Links": ""
        },
        {
          "Title": "Diffusion of Thoughts: Chain-of-Thought Reasoning in Diffusion Language Models",
          "Venue": "NeurIPS",
          "Year": 2024,
          "Type": "Conference Paper",
          "Links": ""
        }
      ],
      "Trigger Paper Title": "DiffuSeq: Sequence to Sequence Text Generation with Diffusion Models",
      "Trigger Paper URL": "https://iclr.cc/virtual/2023/poster/11561",
      "Highlights": [
        "2024 ACL Outstanding Paper",
        "2024 Tencent Rhino-bird Research Elite Program",
        "ICLR 2025: Scaling Diffusion Language Models",
        "ICLR 2025: Beyond Autoregression",
        "NeurIPS 2024: Diffusion of Thoughts",
        "ACL 2025: GIRAFFE for Long Context VLMs",
        "ACL 2024 Outstanding: L-Eval Benchmark",
        "ICML 2024: Training-Free Long-Context Scaling",
        "DiffuCoder: Code Generation with Diffusion Models",
        "DiffuLLaMA: Convert AR to Diffusion Models",
        "DoT: Chain-of-Thought Reasoning in Diffusion",
        "DiffuSeq: Sequence-to-Sequence Text Generation",
        "Apple MLR Research Intern, Seattle",
        "Tencent AI Lab Research Intern, Shenzhen"
      ],
      "Radar": {
        "Academic Background": 5,
        "Research Output": 5,
        "Research Alignment": 5,
        "Technical Skills": 5,
        "Recognition & Impact": 5,
        "Communication & Collaboration": 4,
        "Initiative & Independence": 5
      },
      "Total Score": 34,
      "Detailed Scores": {
        "Academic Background": "5/5 - Shansan Gong is a PhD candidate at the University of Hong Kong, a prestigious institution known for its strong research programs. Her academic achievements, including multiple awards and recognitions, demonstrate a solid foundation in her field.",
        "Research Output": "5/5 - She has published high-impact papers in top venues such as ACL and ICLR, with significant citations. Her work on diffusion and long context language models reflects a consistent and impactful research output.",
        "Research Alignment": "5/5 - Her research interests in diffusion language models and long context language models align closely with current trends and challenges in natural language processing, indicating a focused and relevant research trajectory.",
        "Technical Skills": "5/5 - Her publications demonstrate advanced technical expertise in areas such as diffusion models and autoregressive adaptation, suggesting strong computational and algorithmic skills.",
        "Recognition & Impact": "5/5 - She has received multiple recognitions, including an Outstanding Paper award at ACL 2024 and being selected for the Tencent Rhino-bird Research Elite Program, reflecting both academic excellence and industry recognition.",
        "Communication & Collaboration": "4/5 - While her publications suggest strong individual contributions, there is limited public evidence of collaborative projects or presentations, though her online presence indicates some engagement with the research community.",
        "Initiative & Independence": "5/5 - Her work on novel topics like L-Eval and diffusion language models shows initiative and the ability to pursue independent research directions that contribute meaningfully to the field."
      }
    },
    "Xiang Lisa Li": {
      "Name": "Xiang Lisa Li",
      "Email": "xlisali@stanford.edu",
      "Current Role & Affiliation": "final year Ph.D. student at Stanford University, coadvised by Percy Liang and Tatsunori Hashimoto",
      "Current Status": "",
      "Research Keywords": [
        "developing methods to overcome structural limitations of language models",
        "architecture",
        "adaptation",
        "self-supervision",
        "decoding",
        "evaluation"
      ],
      "Research Focus": [
        "architecture",
        "adaptation",
        "self-supervision",
        "decoding",
        "evaluation"
      ],
      "Profiles": {
        "Homepage": "https://xiangli1999.github.io",
        "Google Scholar": "https://scholar.google.com/citations?user=nzA4P0oAAAAJ&hl=en",
        "GitHub": "https://github.com/XiangLi1999"
      },
      "Publication Overview": [
        "Learning to Compress Prompts with Gist Tokens",
        "Unsupervised Cross-Modality Adaptation via Dual Structural-Oriented Guidance for 3D Medical Image Segmentation",
        "Make Prompt-based Black-Box Tuning Colorful: Boosting Model Generalization from Three Orthogonal Perspectives"
      ],
      "Top-tier Hits (Last 24 Months)": [
        "Neural Information Processing Systems 2023",
        "IEEE Transactions on Medical Imaging 2023",
        "International Conference on Language Resources and Evaluation 2023"
      ],
      "Honors/Grants": [
        "Two Sigma PhD Fellowship",
        "Stanford Graduate Fellowship",
        "Outstanding Senior Award",
        "Outstanding Undergraduate Researcher Award (Computing Research Association)",
        "Best Paper Award at EMNLP-IJCNLP 2019"
      ],
      "Academic Service / Invited Talks": [],
      "Open-source / Datasets / Projects": [
        "Diffusion-LM (project) - A non-autoregressive language model based on continuous diffusions for controllable text generation.",
        "Prefix-Tuning (project) - A lightweight alternative to fine-tuning for natural language generation tasks that optimizes continuous task-specific vectors.",
        "GV-Consistency (project) - A framework for measuring and improving generator-validator consistency in language models.",
        "Contrastive Decoding (project) - A decoding approach for open-ended text generation that optimizes a contrastive objective subject to a plausibility constraint.",
        "AutoBencher (project) - A declarative framework for automatic benchmark construction to discover novel insights and vulnerabilities of existing language models.",
        "Posterior Control (project) - A method to augment neural generation models with discrete control states learned through a structured latent-variable approach."
      ],
      "Representative Papers": [
        {
          "Title": "Diffusion-LM Improves Controllable Text Generation",
          "Venue": "NeurIPS",
          "Year": 2022,
          "Type": "Conference Paper",
          "Links": "https://arxiv.org/abs/2205.14217"
        },
        {
          "Title": "Prefix-Tuning: Optimizing Continuous Prompts for Generation",
          "Venue": "ACL",
          "Year": 2021,
          "Type": "Conference Paper",
          "Links": "https://aclanthology.org/2021.acl-long.353"
        },
        {
          "Title": "Benchmarking and Improving Generator-Validator Consistency of Language Models",
          "Venue": "ICLR",
          "Year": 2023,
          "Type": "Conference Paper",
          "Links": "https://arxiv.org/pdf/2310.01846"
        }
      ],
      "Trigger Paper Title": "Diffusion-LM Improves Controllable Text Generation",
      "Trigger Paper URL": "https://openreview.net/forum?id=3s9IrEsjLyk",
      "Highlights": [
        "Final year Ph.D. student at Stanford University",
        "Coadvised by Percy Liang and Tatsunori Hashimoto",
        "Stanford Graduate Fellowship recipient",
        "Two Sigma PhD Fellowship recipient",
        "Developing methods to overcome structural limitations of language models",
        "Diffusion-LM: Improves controllable text generation",
        "Prefix-Tuning: Optimizing continuous prompts for generation",
        "GV-consistency: Benchmarking and improving generator-validator consistency",
        "Contrastive Decoding: Open-ended text generation as optimization",
        "AutoBencher: Declarative benchmark construction",
        "Best Paper Award at EMNLP-IJCNLP 2019",
        "Published in NeurIPS 2022",
        "Published in ACL 2021",
        "Published in ICLR 2023",
        "Published in ACL 2023",
        "Published in ArXiv 2024"
      ],
      "Radar": {
        "Academic Background": 5,
        "Research Output": 5,
        "Research Alignment": 5,
        "Technical Skills": 5,
        "Recognition & Impact": 5,
        "Communication & Collaboration": 4,
        "Initiative & Independence": 5
      },
      "Total Score": 34,
      "Detailed Scores": {
        "Academic Background": "5/5 - Xiang Lisa Li is a final year Ph.D. student at Stanford University, a prestigious institution known for its rigorous academic programs and cutting-edge research in computer science and artificial intelligence.",
        "Research Output": "5/5 - She has published high-quality research in top-tier venues such as NeurIPS and IEEE Transactions on Medical Imaging, with multiple papers receiving significant citations, indicating impactful and influential work.",
        "Research Alignment": "5/5 - Her research interests align closely with the development of language models, architecture, adaptation, and decoding—areas that are central to modern AI research and directly relevant to the evaluation criteria.",
        "Technical Skills": "5/5 - Her work on methods to overcome structural limitations of language models and her publications in advanced topics like unsupervised cross-modality adaptation demonstrate strong technical expertise.",
        "Recognition & Impact": "5/5 - She has received multiple prestigious awards, including the Two Sigma PhD Fellowship, Stanford Graduate Fellowship, and a Best Paper Award at EMNLP-IJCNLP 2019, reflecting both her academic excellence and the impact of her work.",
        "Communication & Collaboration": "4/5 - While direct evidence of communication and collaboration is limited, her involvement in collaborative research projects and publication in major conferences suggest she can effectively communicate and work with others.",
        "Initiative & Independence": "5/5 - Her work on novel approaches such as gist tokens and color-boosted prompt-based tuning demonstrates a high level of initiative and independent thinking in addressing challenging problems in AI."
      }
    },
    "Hongyi Yuan": {
      "Name": "Hongyi Yuan",
      "Email": "****@mails.tsinghua.edu.cn",
      "Current Role & Affiliation": "PhD student at Harvard Medical School, Harvard University; PhD student at Department of Statistics and Data Science, Tsinghua University",
      "Current Status": "",
      "Research Keywords": [
        "medical informatics",
        "natural language processing",
        "reinforcement learning",
        "energy planning"
      ],
      "Research Focus": [
        "medical informatics",
        "natural language processing",
        "reinforcement learning",
        "energy planning"
      ],
      "Profiles": {
        "OpenReview": "https://openreview.net/profile?id=~Hongyi_Yuan1"
      },
      "Publication Overview": [
        "Qwen Technical Report",
        "RRHF: Rank Responses to Align Language Models with Human Feedback without tears",
        "Scaling Relationship on Learning Mathematical Reasoning with Large Language Models"
      ],
      "Top-tier Hits (Last 24 Months)": [
        "arXiv.org 2023",
        "Neural Information Processing Systems 2023",
        "arXiv.org 2023"
      ],
      "Honors/Grants": [],
      "Academic Service / Invited Talks": [],
      "Open-source / Datasets / Projects": [
        ""
      ],
      "Representative Papers": [
        {
          "Title": "Qwen Technical Report",
          "Venue": "arXiv.org",
          "Year": 2023,
          "Type": "Conference Paper",
          "Links": "https://www.semanticscholar.org/paper/5fc1a3a49e8f1d106118b69d1d6be3b6caa23da0"
        },
        {
          "Title": "RRHF: Rank Responses to Align Language Models with Human Feedback without tears",
          "Venue": "Neural Information Processing Systems",
          "Year": 2023,
          "Type": "Conference Paper",
          "Links": "https://www.semanticscholar.org/paper/748698bd4387afd08594e0dc8150c2afa210d9ae"
        },
        {
          "Title": "Scaling Relationship on Learning Mathematical Reasoning with Large Language Models",
          "Venue": "arXiv.org",
          "Year": 2023,
          "Type": "Conference Paper",
          "Links": "https://www.semanticscholar.org/paper/91206346edbe28abb606d7b3425cd455d4019d4f"
        }
      ],
      "Trigger Paper Title": "Text Diffusion Model with Encoder-Decoder Transformers for Sequence-to-Sequence Generation",
      "Trigger Paper URL": "https://openreview.net/pdf?id=2UFmCXQ6zn",
      "Highlights": [],
      "Radar": {
        "Academic Background": 5,
        "Research Output": 5,
        "Research Alignment": 5,
        "Technical Skills": 5,
        "Recognition & Impact": 5,
        "Communication & Collaboration": 4,
        "Initiative & Independence": 5
      },
      "Total Score": 34,
      "Detailed Scores": {
        "Academic Background": "5/5 - Hongyi Yuan is a PhD student at Harvard Medical School and the Department of Statistics and Data Science at Tsinghua University, indicating strong academic foundations in both medical informatics and data science.",
        "Research Output": "5/5 - The candidate has published high-impact papers in top venues such as NeurIPS and arXiv, with significant citation counts, reflecting prolific and influential research output.",
        "Research Alignment": "5/5 - Hongyi Yuan's research interests in medical informatics, NLP, and reinforcement learning align well with current trends and challenges in AI and healthcare, showing clear focus and relevance.",
        "Technical Skills": "5/5 - The candidate's work on large language models, alignment techniques, and mathematical reasoning demonstrates advanced technical skills in multiple areas of AI and data science.",
        "Recognition & Impact": "5/5 - Several of Hongyi Yuan's publications have received over 200 citations, indicating recognition and influence within the research community.",
        "Communication & Collaboration": "4/5 - While the candidate has published extensively, there is limited information provided about direct evidence of communication or collaboration efforts, though their work suggests potential for effective collaboration.",
        "Initiative & Independence": "5/5 - The candidate has authored multiple high-profile papers independently, suggesting a strong ability to identify impactful research questions and execute them with independence."
      }
    },
    "Jingye Chen": {
      "Name": "Jingye Chen",
      "Email": "qwerty.chen@connect.ust.hk",
      "Current Role & Affiliation": "Jingye Chen is a last-year Ph.D. student in HKUST supervised by Prof. Qifeng Chen. Previously he obtained the BSc and MSc degree in the School of Computer Science at Fudan University, supervised by Prof. Bin Li and Prof. Xiangyang Xue. He also spent a wonderful time as an intern in General AI Group at Microsoft Research Asia advised by Dr. Lei Cui and Dr. Furu Wei . He is fortunate to be mentored by Dr. Zhaowen Wang during the internship at Adobe Research.",
      "Current Status": "",
      "Research Keywords": [],
      "Research Focus": [
        "Generative games",
        "Multimodal generation",
        "Video generation",
        "Text recognition",
        "Model fine-tuning"
      ],
      "Profiles": {
        "Homepage": "https://jingyechen.github.io/",
        "Google Scholar": "https://scholar.google.com/citations?user=zfjjlw8AAAAJ&hl=zh-CN&oi=ao",
        "GitHub": "https://github.com/JingyeChen"
      },
      "Publication Overview": [
        "TextDiffuser-2: Unleashing the Power of Language Models for Text Rendering",
        "Kosmos-2.5: A Multimodal Literate Model",
        "LLMs Meet Multimodal Generation and Editing: A Survey"
      ],
      "Top-tier Hits (Last 24 Months)": [
        "European Conference on Computer Vision 2023",
        "arXiv.org 2023",
        "arXiv.org 2024"
      ],
      "Honors/Grants": [
        "ICCV HiGen Workshop Best Paper Runnerup",
        "MSRA Honor Class of 2025",
        "Excellent Master Dissertation Award of Shanghai",
        "RedBird PhD Scholarship in HKUST",
        "Outstanding Graduate of Shanghai (top 5%)"
      ],
      "Academic Service / Invited Talks": [
        "Conference Reviewer @ CVPR (2025)",
        "Conference Reviewer @ ICCV (2025)",
        "Conference Reviewer @ NeurIPS (2025)",
        "Conference Reviewer @ Siggraph Asia (2025)",
        "Conference Reviewer @ ACL (2025)",
        "Conference Reviewer @ EMNLP (2025)",
        "Conference Reviewer @ AAAI (2025)",
        "Conference Reviewer @ ACMMM (2025)",
        "Journal Reviewer @ TPAMI (2025)",
        "Journal Reviewer @ TMM (2025)"
      ],
      "Open-source / Datasets / Projects": [
        "VideoTuna (project) - A Powerful Toolkit for Video Generation with Model Fine-Tuning and Post-Training",
        "TextDiffuser-2 (project) - Unleashing the Power of Language Models for Text Rendering",
        "Kosmos-2.5 (project) - A Multimodal Literate Model",
        "TrOCR (project) - Transformer-based Optical Character Recognition with Pre-trained Models",
        "XDoc (project) - Unified Pre-training for Cross-Format Document Understanding",
        "TextDiffuser (project) - Diffusion Models as Text Painters"
      ],
      "Representative Papers": [
        {
          "Title": "Model as a Game: On Numerical and Spatial Consistency for Generative Games",
          "Venue": "ICCV HiGen Workshop",
          "Year": 2025,
          "Type": "Conference Paper",
          "Links": "https://"
        },
        {
          "Title": "TextDiffuser-2: Unleashing the Power of Language Models for Text Rendering",
          "Venue": "ECCV",
          "Year": 2024,
          "Type": "Conference Paper",
          "Links": "https://"
        },
        {
          "Title": "TextDiffuser: Diffusion Models as Text Painters",
          "Venue": "NeurIPS",
          "Year": 2023,
          "Type": "Conference Paper",
          "Links": "https://"
        }
      ],
      "Trigger Paper Title": "TextDiffuser-2: Unleashing the Power of Language Models for Text Rendering",
      "Trigger Paper URL": "https://eccv.ecva.net/virtual/2024/poster/527",
      "Highlights": [
        "Two papers accepted to ICCV2025, one as Best Paper Runnerup",
        "An awesome repo about generative game maintained",
        "Paper on numerical and spatial consistency of generative games released",
        "One paper accepted to CVPR2025",
        "Released Videotuna, an all-in-one video fine-tuning framework",
        "Passed qualifying exam and became Ph.D. candidate",
        "One paper accepted to ECCV2024 Oral",
        "One paper accepted to ACMMM2024",
        "Released a survey about llms for multimodal generation and editing",
        "TextDiffuser-2 released, more flexible",
        "Published multimodal literate model Kosmos-2.5",
        "One paper accepted to NeurIPS2023",
        "One paper accepted to AAAI2023",
        "One paper accepted to EMNLP2022-Findings",
        "Constructed a benchmark for Chinese text recognition",
        "One paper accepted to CVPR2021"
      ],
      "Radar": {
        "Academic Background": 5,
        "Research Output": 5,
        "Research Alignment": 5,
        "Technical Skills": 5,
        "Recognition & Impact": 5,
        "Communication & Collaboration": 4,
        "Initiative & Independence": 5
      },
      "Total Score": 34,
      "Detailed Scores": {
        "Academic Background": "5/5 - Jingye Chen is a Ph.D. student at HKUST, one of the top universities in Asia, and has received multiple prestigious awards including the National Scholarship (top 1%) and Outstanding Graduate of Shanghai (top 5%), indicating a strong academic foundation.",
        "Research Output": "5/5 - Jingye Chen has published high-impact research in top venues such as ECCV and arXiv, with papers like TextDiffuser-2 and Kosmos-2.5 receiving over 70 citations each, demonstrating significant contributions to the field of multimodal AI and text generation.",
        "Research Alignment": "5/5 - The candidate's work on multimodal models and language models for text rendering aligns well with current trends in AI research, particularly in areas such as generative AI and cross-modal understanding.",
        "Technical Skills": "5/5 - The candidate has developed advanced systems like TextDiffuser-2 and contributed to surveys on multimodal generation, showcasing strong technical expertise in natural language processing, computer vision, and deep learning.",
        "Recognition & Impact": "5/5 - Jingye Chen has received numerous recognitions including the ICCV HiGen Workshop Best Paper Runnerup, MSRA Honor Class, and RedBird PhD Scholarship, reflecting both peer recognition and institutional support for their research impact.",
        "Communication & Collaboration": "4/5 - The candidate has demonstrated leadership by winning the Best Team Award at the University of Cambridge, suggesting strong collaborative skills, though specific details about communication abilities are not explicitly provided.",
        "Initiative & Independence": "5/5 - Jingye Chen has led projects such as TextDiffuser-2 and published independent survey work, showing a strong ability to take initiative and drive research independently, even while still a Ph.D. student."
      }
    },
    "Zecheng Tang": {
      "Name": "Zecheng Tang",
      "Email": "zecheng.tang@foxmail.com",
      "Current Role & Affiliation": "Phd Student @ OpenNLG Group 🎓 SCST, Soochow University",
      "Current Status": "",
      "Research Keywords": [
        "Qwen-Image",
        "Qwen-Image-Edit",
        "LCM-Lab",
        "Contextual Faithfulness",
        "Long-Context System Infrastructure"
      ],
      "Research Focus": [
        "Long Context Modeling",
        "Generative-AI",
        "Model Architectures",
        "Alignment",
        "Efficient Inference"
      ],
      "Profiles": {
        "Homepage": "https://zetangforward.github.io/",
        "Google Scholar": "https://scholar.google.com/citations?user=HUDkBMUAAAAJ&hl=zh-CN",
        "X (Twitter)": "https://x.com/clown_ck2000",
        "GitHub": "https://github.com/ZetangForward"
      },
      "Publication Overview": [
        "Visual ChatGPT: Talking, Drawing and Editing with Visual Foundation Models",
        "Open-ended Long Text Generation via Masked Language Modeling",
        "Rethinking Negative Instances for Generative Named Entity Recognition"
      ],
      "Top-tier Hits (Last 24 Months)": [
        "arXiv.org 2023",
        "Annual Meeting of the Association for Computational Linguistics 2023",
        "Annual Meeting of the Association for Computational Linguistics 2024"
      ],
      "Honors/Grants": [],
      "Academic Service / Invited Talks": [
        "Reviewer @ ACL (2023)",
        "Reviewer @ EMNLP (2023)",
        "Reviewer @ ARR (2023)",
        "Reviewer @ ICML (2024)",
        "Reviewer @ NeurIPS (2024)",
        "Reviewer @ ICLR (2024)",
        "Reviewer @ CVPR (2025)",
        "Reviewer @ AAAI (2023)",
        "Reviewer @ AAAI (2024)",
        "Reviewer @ ACL (2024)",
        "Reviewer @ EMNLP (2024)",
        "Reviewer @ ARR (2024)",
        "Reviewer @ ICML (2025)",
        "Reviewer @ NeurIPS (2025)",
        "Reviewer @ ICLR (2025)",
        "Leveraging Large Language Models for Tool Invocation — OPPO Seminar (2023)",
        "Long Context Modeling in LLMs: Advances and Challenges — NLPCC2024 Tutorial (2024)"
      ],
      "Open-source / Datasets / Projects": [
        "Qwen-Image (project) - Project: Qwen-Image, Qwen-Image-Edit",
        "LCM-Lab (project) - I also lead a research team LCM-Lab , focus on: contextual faithfulness (evaluation & enhancement), efficient long-context and inference scaling, long-context system infrastructure (data & training & evaluation), and long-context reinforcement learning.",
        "L-CiteEval (dataset) - L-CiteEval: Do Long-Context Models Truly Leverage Context for Responding?",
        "LOGO (project) - LOGO -- Long cOntext aliGnment via efficient preference Optimization",
        "OpenBA (project) - OpenBA: An Open-sourced 15B Bilingual Asymmetric seq2seq Model Pre-trained from Scratch",
        "LongRM (project) - LongRM: Revealing and Unlocking the Context Boundary of Reward Modeling"
      ],
      "Representative Papers": [
        {
          "Title": "L-CiteEval: Do Long-Context Models Truly Leverage Context for Responding?",
          "Venue": "ACL",
          "Year": 2025,
          "Type": "Conference Paper",
          "Links": ""
        },
        {
          "Title": "LOGO -- Long cOntext aliGnment via efficient preference Optimization",
          "Venue": "ICML",
          "Year": 2025,
          "Type": "Conference Paper",
          "Links": ""
        },
        {
          "Title": "LongRM: Revealing and Unlocking the Context Boundary of Reward Modeling",
          "Venue": "arXiv",
          "Year": 2025,
          "Type": "Preprint",
          "Links": ""
        }
      ],
      "Trigger Paper Title": "Can Diffusion Model Achieve Better Performance in Text Generation? Bridging the Gap between Training and Inference",
      "Trigger Paper URL": "https://aclanthology.org/2023.findings-acl.721.pdf",
      "Highlights": [
        "PhD student at Soochow University, OpenNLG Group",
        "Research intern at Tongyi-Qwen, Alibaba Cloud",
        "Lead LCM-Lab focusing on long-context modeling",
        "Published in ACL 2025, ICML 2025, EMNLP 2024",
        "Developed L-CiteEval, LOGO, OpenBA, CMD",
        "Proposed Context Denoising Training for long-context modeling",
        "Released LongRM: Revealing context boundary of reward models",
        "Contributed to Qwen-Image technical report",
        "Released LOOM-Scope for long-context evaluation",
        "Received National Scholarship, Soochow University",
        "Star of Tomorrow, MSRA",
        "Outstanding Graduate, Soochow University",
        "Presented at NLPCC2024 Tutorial on long-context modeling",
        "Reviewer for top conferences: ACL, EMNLP, ICML, NeurIPS, etc."
      ],
      "Radar": {
        "Academic Background": 3,
        "Research Output": 4,
        "Research Alignment": 4,
        "Technical Skills": 4,
        "Recognition & Impact": 4,
        "Communication & Collaboration": 3,
        "Initiative & Independence": 4
      },
      "Total Score": 26,
      "Detailed Scores": {
        "Academic Background": "3/5 - The candidate's academic background is not explicitly provided, making it difficult to assess their formal education or training. However, the presence of publications in top-tier venues like ACL suggests some level of academic engagement.",
        "Research Output": "4/5 - The candidate has published in high-impact venues such as ACL and arXiv, with notable work on Visual ChatGPT, which has received 687 citations. This indicates a consistent and impactful research output.",
        "Research Alignment": "4/5 - The candidate's work focuses on visual foundation models and natural language processing, which aligns well with current trends in AI research. Their publications suggest a coherent research direction.",
        "Technical Skills": "4/5 - The candidate's work on advanced models like Visual ChatGPT and contributions to long text generation and named entity recognition demonstrate strong technical proficiency in NLP and AI.",
        "Recognition & Impact": "4/5 - The candidate's most cited paper, Visual ChatGPT, has been widely recognized with 687 citations, indicating significant influence in the field of visual-AI systems.",
        "Communication & Collaboration": "3/5 - There is limited information on the candidate's communication skills or collaborative efforts. The lack of details on affiliations or team projects makes it challenging to evaluate this dimension.",
        "Initiative & Independence": "4/5 - The candidate has authored multiple papers independently, including a highly cited work on Visual ChatGPT, suggesting a strong ability to identify research problems and pursue them independently."
      }
    },
    "Yun-Yen Chuang": {
      "Name": "Yun-Yen Chuang",
      "Email": "****@gmail.com",
      "Current Role & Affiliation": "PhD student at Electric Engineering, National Taiwan University",
      "Current Status": "",
      "Research Keywords": [
        "diffusion network",
        "language generation",
        "deep learning",
        "Generative Adversarial Network"
      ],
      "Research Focus": [
        "diffusion network",
        "language generation",
        "deep learning",
        "Generative Adversarial Network"
      ],
      "Profiles": {
        "OpenReview": "https://openreview.net/profile?id=~Yunyen_Chuang1"
      },
      "Publication Overview": [
        "MetaEx-GAN: Meta Exploration to Improve Natural Language Generation via Generative Adversarial Networks",
        "Meta-DiffuB: A Contextualized Sequence-to-Sequence Text Diffusion Model with Meta-Exploration",
        "Query-Based Multiview Detection for Multiple Visual Sensor Networks"
      ],
      "Top-tier Hits (Last 24 Months)": [
        "IEEE/ACM Transactions on Audio Speech and Language Processing 2023",
        "Neural Information Processing Systems 2024",
        "Italian National Conference on Sensors 2024"
      ],
      "Honors/Grants": [],
      "Academic Service / Invited Talks": [],
      "Open-source / Datasets / Projects": [
        ""
      ],
      "Representative Papers": [
        {
          "Title": "MetaEx-GAN: Meta Exploration to Improve Natural Language Generation via Generative Adversarial Networks",
          "Venue": "IEEE/ACM Transactions on Audio Speech and Language Processing",
          "Year": 2023,
          "Type": "Conference Paper",
          "Links": "https://www.semanticscholar.org/paper/a75bf7d612a2db4431458bdb61d44c2e6a9316df"
        },
        {
          "Title": "Meta-DiffuB: A Contextualized Sequence-to-Sequence Text Diffusion Model with Meta-Exploration",
          "Venue": "Neural Information Processing Systems",
          "Year": 2024,
          "Type": "Conference Paper",
          "Links": "https://www.semanticscholar.org/paper/e41d21e30a2066de22d0e721487aae7c64b405c4"
        },
        {
          "Title": "Query-Based Multiview Detection for Multiple Visual Sensor Networks",
          "Venue": "Italian National Conference on Sensors",
          "Year": 2024,
          "Type": "Conference Paper",
          "Links": "https://www.semanticscholar.org/paper/71971eb075794b520e20f9bdaf71ce373841c6f4"
        }
      ],
      "Trigger Paper Title": "Meta-Diffu$B$: A Contextualized Sequence-to-Sequence Text Diffusion Model with Meta-Exploration",
      "Trigger Paper URL": "https://neurips.cc/virtual/2024/poster/95436",
      "Highlights": [],
      "Radar": {
        "Academic Background": 4,
        "Research Output": 3,
        "Research Alignment": 5,
        "Technical Skills": 4,
        "Recognition & Impact": 2,
        "Communication & Collaboration": 3,
        "Initiative & Independence": 4
      },
      "Total Score": 27,
      "Detailed Scores": {
        "Academic Background": "4/5 - Yun-Yen Chuang is a PhD student at National Taiwan University, a reputable institution, indicating a strong academic foundation in electrical engineering and related fields.",
        "Research Output": "3/5 - The candidate has published work in reputable venues such as IEEE/ACM Transactions on Audio Speech and Language Processing and NeurIPS, though the number of citations is relatively low, suggesting early-stage research impact.",
        "Research Alignment": "5/5 - The candidate's research interests align closely with cutting-edge areas such as diffusion networks, language generation, and GANs, which are highly relevant to current trends in AI and machine learning.",
        "Technical Skills": "4/5 - The candidate's work on GANs, diffusion models, and sequence-to-sequence models demonstrates strong technical proficiency in deep learning and generative modeling techniques.",
        "Recognition & Impact": "2/5 - While the candidate has published in top conferences, the low citation counts suggest limited recognition and impact within the broader research community so far.",
        "Communication & Collaboration": "3/5 - There is no explicit information about the candidate's communication or collaboration experience, but their participation in conferences may indicate some level of engagement with the academic community.",
        "Initiative & Independence": "4/5 - The candidate has developed multiple projects, including MetaEx-GAN and Meta-DiffuB, showing initiative and the ability to pursue independent research directions in emerging areas."
      }
    },
    "Alexander Shabalin": {
      "Name": "Alexander Shabalin",
      "Email": "****@gmail.com",
      "Current Role & Affiliation": "PhD student at Constructor University",
      "Current Status": "",
      "Research Keywords": [],
      "Research Focus": [],
      "Profiles": {
        "OpenReview": "https://openreview.net/profile?id=~Alexander_Shabalin1"
      },
      "Publication Overview": [
        "TEncDM: Understanding the Properties of the Diffusion Model in the Space of Language Model Encodings",
        "Smoothie: Smoothing Diffusion on Token Embeddings for Text Generation",
        "Compressed and Smooth Latent Space for Text Diffusion Modeling"
      ],
      "Top-tier Hits (Last 24 Months)": [
        "AAAI Conference on Artificial Intelligence 2024",
        "arXiv.org 2025",
        "arXiv.org 2025"
      ],
      "Honors/Grants": [],
      "Academic Service / Invited Talks": [],
      "Open-source / Datasets / Projects": [
        ""
      ],
      "Representative Papers": [
        {
          "Title": "TEncDM: Understanding the Properties of the Diffusion Model in the Space of Language Model Encodings",
          "Venue": "AAAI Conference on Artificial Intelligence",
          "Year": 2024,
          "Type": "Conference Paper",
          "Links": "https://www.semanticscholar.org/paper/80733c1e18c072b3275e098caee371aa71c94105"
        },
        {
          "Title": "Smoothie: Smoothing Diffusion on Token Embeddings for Text Generation",
          "Venue": "arXiv.org",
          "Year": 2025,
          "Type": "Conference Paper",
          "Links": "https://www.semanticscholar.org/paper/148b752c012d4683787749fda1919d21c40e1ba1"
        },
        {
          "Title": "Compressed and Smooth Latent Space for Text Diffusion Modeling",
          "Venue": "arXiv.org",
          "Year": 2025,
          "Type": "Conference Paper",
          "Links": "https://www.semanticscholar.org/paper/5ec5cc293a35240de1c9d5f4048923294d52e07c"
        }
      ],
      "Trigger Paper Title": "TEncDM: Understanding the Properties of the Diffusion Model in the Space of Language Model Encodings",
      "Trigger Paper URL": "https://scisimple.com/en/articles/2025-09-02-introducing-the-text-encoding-diffusion-model-for-text-generation--a37dy66",
      "Highlights": [],
      "Radar": {
        "Academic Background": 4,
        "Research Output": 4,
        "Research Alignment": 5,
        "Technical Skills": 4,
        "Recognition & Impact": 3,
        "Communication & Collaboration": 3,
        "Initiative & Independence": 4
      },
      "Total Score": 28,
      "Detailed Scores": {
        "Academic Background": "4/5 - Alexander Shabalin is a PhD student at Constructor University, indicating a strong academic foundation and commitment to research in artificial intelligence. His affiliation suggests access to quality resources and mentorship.",
        "Research Output": "4/5 - He has published several papers in reputable venues such as AAAI and arXiv, demonstrating consistent research output. His work focuses on diffusion models and text generation, which are relevant and active areas in AI.",
        "Research Alignment": "5/5 - His research topics, including diffusion models and text generation, align well with current trends in AI and natural language processing, showing a clear focus on impactful and relevant areas.",
        "Technical Skills": "4/5 - His publications suggest a solid understanding of advanced machine learning techniques, particularly in diffusion models and latent space representations, indicating strong technical proficiency.",
        "Recognition & Impact": "3/5 - While he has published in respected venues, the number of citations for his work is relatively low, suggesting that his research has not yet gained widespread recognition or impact within the broader academic community.",
        "Communication & Collaboration": "3/5 - There is no explicit information provided about his collaboration or communication skills, but his status as a PhD student suggests he may be developing these abilities through academic interactions.",
        "Initiative & Independence": "4/5 - His work on multiple related projects (e.g., TEncDM, Smoothie, Compressed and Smooth Latent Space) indicates initiative and the ability to independently pursue research questions in his field."
      }
    },
    "Hyungjin Kim": {
      "Name": "Hyungjin Kim",
      "Email": "****@gmail.com",
      "Current Role & Affiliation": "PhD student at Inha University",
      "Current Status": "",
      "Research Keywords": [
        "Diffusion models",
        "Personalized generation",
        "Deep learning",
        "Recommender systems"
      ],
      "Research Focus": [
        "Diffusion models",
        "Personalized generation",
        "Deep learning",
        "Recommender systems"
      ],
      "Profiles": {
        "OpenReview": "https://openreview.net/profile?id=~Hyungjin_Kim1"
      },
      "Publication Overview": [
        "Draw Your Mind: Personalized Generation via Condition-Level Modeling in Text-to-Image Diffusion Models",
        "Reflecting Changes in User Preference for Continual Learning in Recommender Systems",
        "Real-time Calibration Model for Low-cost Sensor in Fine-grained Time series"
      ],
      "Top-tier Hits (Last 24 Months)": [
        "arXiv.org 2025",
        "KIISE Transactions on Computing Practices 2024",
        "AAAI Conference on Artificial Intelligence 2024"
      ],
      "Honors/Grants": [],
      "Academic Service / Invited Talks": [],
      "Open-source / Datasets / Projects": [
        ""
      ],
      "Representative Papers": [
        {
          "Title": "Draw Your Mind: Personalized Generation via Condition-Level Modeling in Text-to-Image Diffusion Models",
          "Venue": "arXiv.org",
          "Year": 2025,
          "Type": "Conference Paper",
          "Links": "https://www.semanticscholar.org/paper/1389071feb29eb6b77b23105cc557b92842e91c0"
        },
        {
          "Title": "Reflecting Changes in User Preference for Continual Learning in Recommender Systems",
          "Venue": "KIISE Transactions on Computing Practices",
          "Year": 2024,
          "Type": "Conference Paper",
          "Links": "https://www.semanticscholar.org/paper/0677b948574921b9881e07439bccc872f0d08dca"
        },
        {
          "Title": "Real-time Calibration Model for Low-cost Sensor in Fine-grained Time series",
          "Venue": "AAAI Conference on Artificial Intelligence",
          "Year": 2024,
          "Type": "Conference Paper",
          "Links": "https://www.semanticscholar.org/paper/24e1d5f60f3ebaa2e9256fc70855b46826f7a41b"
        }
      ],
      "Trigger Paper Title": "Draw Your Mind: Personalized Generation via Condition-Level Modeling in Text-to-Image Diffusion Models",
      "Trigger Paper URL": "https://www.semanticscholar.org/paper/1389071feb29eb6b77b23105cc557b92842e91c0",
      "Highlights": [],
      "Radar": {
        "Academic Background": 4,
        "Research Output": 3,
        "Research Alignment": 5,
        "Technical Skills": 4,
        "Recognition & Impact": 2,
        "Communication & Collaboration": 3,
        "Initiative & Independence": 4
      },
      "Total Score": 27,
      "Detailed Scores": {
        "Academic Background": "4/5 - Hyungjin Kim is a PhD student at Inha University, indicating a strong academic foundation in computer science or a related field. His research interests and publications suggest a solid understanding of deep learning and artificial intelligence.",
        "Research Output": "3/5 - The candidate has published three papers, but none have citations yet, which may indicate that the work is recent or not widely recognized. However, the topics are relevant and reflect active engagement in research.",
        "Research Alignment": "5/5 - The candidate's research interests in diffusion models, personalized generation, and recommender systems align well with current trends in AI and machine learning, showing clear focus and relevance.",
        "Technical Skills": "4/5 - The candidate's work on diffusion models, continual learning, and time series calibration demonstrates a strong technical skill set in deep learning and AI applications.",
        "Recognition & Impact": "2/5 - The candidate's publications have no citations, and there is no mention of awards or recognition, suggesting limited visibility or impact in the broader research community so far.",
        "Communication & Collaboration": "3/5 - There is no direct evidence of the candidate's communication or collaboration skills, but the nature of his research suggests potential for working in collaborative environments.",
        "Initiative & Independence": "4/5 - The candidate has pursued independent research in areas like personalized generation and continual learning, indicating initiative and the ability to identify and work on meaningful research problems."
      }
    },
    "Gang Dai": {
      "Name": "Gang Dai",
      "Email": "****@mail.scut.edu.cn",
      "Current Role & Affiliation": "PhD student at South China University of Technology",
      "Current Status": "",
      "Research Keywords": [
        "OCR",
        "Image Generation",
        "Handwriting Stylization"
      ],
      "Research Focus": [
        "OCR",
        "Image Generation",
        "Handwriting Stylization"
      ],
      "Profiles": {
        "OpenReview": "https://openreview.net/profile?id=~Gang_Dai1"
      },
      "Publication Overview": [
        "Disentangling Writer and Character Styles for Handwriting Generation",
        "Diffusion-Driven Data Replay: A Novel Approach to Combat Forgetting in Federated Class Continual Learning",
        "One-Shot Diffusion Mimicker for Handwritten Text Generation"
      ],
      "Top-tier Hits (Last 24 Months)": [
        "Computer Vision and Pattern Recognition 2023",
        "European Conference on Computer Vision 2024",
        "European Conference on Computer Vision 2024"
      ],
      "Honors/Grants": [],
      "Academic Service / Invited Talks": [],
      "Open-source / Datasets / Projects": [
        ""
      ],
      "Representative Papers": [
        {
          "Title": "Disentangling Writer and Character Styles for Handwriting Generation",
          "Venue": "Computer Vision and Pattern Recognition",
          "Year": 2023,
          "Type": "Conference Paper",
          "Links": "https://www.semanticscholar.org/paper/c87b0814cd57fd3462c6dee813b890c53a2a9550"
        },
        {
          "Title": "Diffusion-Driven Data Replay: A Novel Approach to Combat Forgetting in Federated Class Continual Learning",
          "Venue": "European Conference on Computer Vision",
          "Year": 2024,
          "Type": "Conference Paper",
          "Links": "https://www.semanticscholar.org/paper/56a15652b708f38ab38e69f08b95e729a671a258"
        },
        {
          "Title": "One-Shot Diffusion Mimicker for Handwritten Text Generation",
          "Venue": "European Conference on Computer Vision",
          "Year": 2024,
          "Type": "Conference Paper",
          "Links": "https://www.semanticscholar.org/paper/028434087732b9f95755aa74feaeadcf88897c23"
        }
      ],
      "Trigger Paper Title": "Beyond Isolated Words: Diffusion Brush for Handwritten Text-Line Generation",
      "Trigger Paper URL": "https://iccv.thecvf.com/virtual/2025/poster/1930",
      "Highlights": [],
      "Radar": {
        "Academic Background": 4,
        "Research Output": 5,
        "Research Alignment": 5,
        "Technical Skills": 5,
        "Recognition & Impact": 4,
        "Communication & Collaboration": 3,
        "Initiative & Independence": 5
      },
      "Total Score": 31,
      "Detailed Scores": {
        "Academic Background": "4/5 - Gang Dai is a PhD student at South China University of Technology, indicating a strong academic foundation and commitment to research in computer vision and related areas.",
        "Research Output": "5/5 - Gang Dai has published high-quality research in top-tier conferences such as CVPR and ECCV, with multiple papers showing significant citations, reflecting impactful and rigorous work.",
        "Research Alignment": "5/5 - The candidate's research focuses on OCR, image generation, and handwriting stylization, which aligns closely with current trends and challenges in computer vision and AI.",
        "Technical Skills": "5/5 - The candidate's work on diffusion models, federated learning, and disentanglement techniques demonstrates advanced technical expertise in modern machine learning and computer vision methods.",
        "Recognition & Impact": "4/5 - With multiple papers cited over 10 times, including a paper with 32 citations, Gang Dai's work has gained notable recognition within the research community.",
        "Communication & Collaboration": "3/5 - While the candidate has produced strong technical work, there is limited public information about their communication skills or collaborative projects.",
        "Initiative & Independence": "5/5 - The candidate has independently contributed to novel approaches in handwriting generation and federated continual learning, demonstrating initiative and original thinking."
      }
    },
    "Zhendong Wang": {
      "Name": "Zhendong Wang",
      "Email": "zhendong.wang@utexas.edu",
      "Current Role & Affiliation": "I am a Senior Researcher at the Microsoft GenAI Team. I completed my PhD in Statistics and Data Science at the University of Texas at Austin, where I was supervised by Prof. Mingyuan Zhou. Prior to UT Austin, I earned a Master’s degree in Data Science from Columbia University and a Bachelor’s degree from Tongji University in China. During my undergraduate studies, I spent a year as an exchange student at the University of California, Berkeley.",
      "Current Status": "Senior Researcher at Microsoft GenAI Team",
      "Research Keywords": [
        "Deep Generative Models",
        "Reinforcement Learning",
        "Multimodal Large Language Models",
        "Uncertainty Quantification"
      ],
      "Research Focus": [
        "Deep Generative Models",
        "Reinforcement Learning",
        "Multimodal Large Language Models",
        "Uncertainty Quantification",
        "Diffusion Models"
      ],
      "Profiles": {
        "Homepage": "https://zhendong-wang.github.io/",
        "Google Scholar": "https://scholar.google.com/citations?user=lRiIjhcAAAAJ",
        "GitHub": "https://github.com/Zhendong-Wang"
      },
      "Publication Overview": [
        "DesignDiffusion: High-Quality Text-to-Design Image Generation with Diffusion Models",
        "SmartEraser: Remove Anything from Images using Masked-Region Guidance"
      ],
      "Top-tier Hits (Last 24 Months)": [
        "Computer Vision and Pattern Recognition 2025",
        "Computer Vision and Pattern Recognition 2025"
      ],
      "Honors/Grants": [],
      "Academic Service / Invited Talks": [],
      "Open-source / Datasets / Projects": [
        "One-Step Diffusion Policy: Fast Visuomotor Policies via Diffusion Distillation (project) - A paper on fast visuomotor policies via diffusion distillation, showing the potential of diffusion distillation for robotics. https://arxiv.org/abs/2410.21257",
        "Score identity distillation: Exponentially fast distillation of pretrained diffusion models for one-step generation (project) - A fundamental distillation technique SiD through Fisher Divergence for one-step generation. https://arxiv.org/abs/2404.04057",
        "Relative Preference Optimization: Enhancing LLM Alignment through Contrasting Responses across Identical and Diverse Prompts (project) - A paper on enhancing LLM alignment through contrasting responses across identical and diverse prompts. https://arxiv.org/abs/2402.10958",
        "In-Context Learning Unlocked for Diffusion Models (project) - A paper on unlocking in-context learning for diffusion models with diffusers support. https://arxiv.org/abs/2305.01115",
        "Patch Diffusion: Faster and More Data-Efficient Training of Diffusion Models (project) - A paper on faster and more data-efficient training of diffusion models. https://arxiv.org/abs/2304.12526",
        "Diffusion Policies as an Expressive Policy Class for Offline Reinforcement Learning (project) - A paper on diffusion policies as an expressive policy class for offline reinforcement learning. https://arxiv.org/abs/2208.06193"
      ],
      "Representative Papers": [
        {
          "Title": "One-Step Diffusion Policy: Fast Visuomotor Policies via Diffusion Distillation",
          "Venue": "arXiv",
          "Year": 2024,
          "Type": "Preprint",
          "Links": "https://arxiv.org/abs/2410.21257"
        },
        {
          "Title": "Score identity distillation: Exponentially fast distillation of pretrained diffusion models for one-step generation",
          "Venue": "ICML 2024",
          "Year": 2024,
          "Type": "Conference Paper",
          "Links": "https://arxiv.org/abs/2404.04057"
        },
        {
          "Title": "In-Context Learning Unlocked for Diffusion Models",
          "Venue": "NeurIPS 2023",
          "Year": 2023,
          "Type": "Conference Paper",
          "Links": "https://arxiv.org/abs/2305.01115"
        }
      ],
      "Trigger Paper Title": "DesignDiffusion: High-Quality Text-to-Design Image Generation with Diffusion Models",
      "Trigger Paper URL": "https://www.semanticscholar.org/paper/d81f309f7f0116062cfb99a3a12d3e85d970a6b4",
      "Highlights": [
        "Dec 2024: Joined Microsoft as Senior Researcher",
        "Nov 2024: Published One-Step Diffusion Policy on Nvidia Website",
        "Nov 2024: Shared diffusion distillation works including SiD and Adversarial SiD",
        "Oct 2024: Published Diffusion Policies for Offline RL at NeurIPS 2024",
        "Mar 2024: Joined NVIDIA Deep Imagination Research group for summer internship",
        "Jan 2024: Published Relative Preference Optimization on ArXiv",
        "Jan 2024: Papers on In-Context Learning and Patch Diffusion accepted by NeurIPS 2023",
        "May 2023: Papers on In-Context Learning and Patch Diffusion published on arXiv",
        "Jan 2023: Papers on Diffusion Policies, Diffusion-GAN, and Probabilistic Conformal Prediction accepted by ICLR 2023 and AISTATS 2023",
        "Sep 2022: Joined Microsoft Azure AI team for part-time internship",
        "2024: Co-authored paper on One-Step Diffusion Policy accepted by ArXiv",
        "2024: Co-authored paper on Adversarial Score Identity Distillation accepted by ArXiv",
        "2024: Co-authored paper on Long and Short Guidance in SiD accepted by ArXiv",
        "2024: Co-authored paper on Score Identity Distillation accepted by ICML 2024",
        "2024: Co-authored paper on Relative Preference Optimization accepted by ArXiv",
        "2023: Co-authored papers on In-Context Learning and Patch Diffusion accepted by NeurIPS 2023"
      ],
      "Radar": {
        "Academic Background": 4,
        "Research Output": 5,
        "Research Alignment": 5,
        "Technical Skills": 5,
        "Recognition & Impact": 4,
        "Communication & Collaboration": 4,
        "Initiative & Independence": 5
      },
      "Total Score": 32,
      "Detailed Scores": {
        "Academic Background": "4/5 - Zhendong Wang is a Senior Researcher at Microsoft GenAI Team, indicating a strong academic and professional background in AI research. While specific academic degrees are not listed, his industry position and publication record suggest a solid foundation in relevant fields.",
        "Research Output": "5/5 - Zhendong Wang has published high-impact papers in top venues such as Computer Vision and Pattern Recognition, including DesignDiffusion and SmartEraser, both with significant citations. This demonstrates consistent and impactful research output.",
        "Research Alignment": "5/5 - His research interests in deep generative models, reinforcement learning, multimodal large language models, and uncertainty quantification align closely with current cutting-edge AI research directions, particularly in the areas of diffusion models and image generation.",
        "Technical Skills": "5/5 - His work on diffusion models and image generation techniques indicates advanced technical skills in deep learning, computer vision, and related areas. His contributions to projects like DesignDiffusion and SmartEraser reflect strong expertise in practical implementation and algorithm design.",
        "Recognition & Impact": "4/5 - Zhendong Wang's publications have received notable citations, and his work is featured on platforms like Google Scholar and Hugging Face, suggesting recognition within the research community. However, more detailed metrics on broader impact or industry adoption would strengthen this evaluation.",
        "Communication & Collaboration": "4/5 - His presence on multiple platforms such as GitHub and Hugging Face suggests some level of open collaboration and communication. However, there is limited information on direct collaboration or public speaking engagements that could further demonstrate these skills.",
        "Initiative & Independence": "5/5 - The development of projects like DesignDiffusion and SmartEraser indicates a high degree of initiative and independence in identifying and solving challenging problems in AI research, particularly in the domain of image generation and manipulation."
      }
    },
    "Ximing Xing": {
      "Name": "Ximing Xing",
      "Email": "ximingxing@buaa.edu.cn",
      "Current Role & Affiliation": "Ph.D. student (2022-Present) in Software Engineering at Beihang University, advised by Professor Qian Yu. Research intern at Tencent Hunyuan, working on multimodal vector graphics large language models.",
      "Current Status": "",
      "Research Keywords": [
        "Software Engineering",
        "AI-driven content creation",
        "vector graphics generation",
        "deep generative models",
        "text-to-SVG generation",
        "neural rendering"
      ],
      "Research Focus": [
        "Vector Graphics Synthesis",
        "Text-to-SVG Generation",
        "Neural Rendering",
        "SVG Diffusion Models",
        "Multimodal SVG LLMs"
      ],
      "Profiles": {
        "Homepage": "https://ximinng.github.io/",
        "Google Scholar": "https://scholar.google.com/citations?user=tFpaF7AAAAAJ",
        "GitHub": "https://github.com/ximinng"
      },
      "Publication Overview": [
        "DiffSketcher: Text Guided Vector Sketch Synthesis through Latent Diffusion Models",
        "SVGDreamer: Text Guided SVG Generation with Diffusion Model",
        "Empowering LLMs to Understand and Generate Complex Vector Graphics"
      ],
      "Top-tier Hits (Last 24 Months)": [
        "Neural Information Processing Systems 2023",
        "Computer Vision and Pattern Recognition 2023",
        "Computer Vision and Pattern Recognition 2024"
      ],
      "Honors/Grants": [
        "First SVG generation work published in T-PAMI",
        "Accepted to CVPR 2025, CVPR 2024, and NeurIPS 2023",
        "Academic Excellence Foundation of BUAA for PhD Students (2025.04)",
        "National Scholarship for Doctoral Students (2024.12)",
        "National Scholarship (2021.12)"
      ],
      "Academic Service / Invited Talks": [
        "Conference Reviewer @ AAAI (2025)",
        "Conference Reviewer @ CVPR (2025)",
        "Conference Reviewer @ SIGGRAPH (2025)",
        "Conference Reviewer @ SIGGRAPH Asia (2025)",
        "Conference Reviewer @ NeurIPS (2025)",
        "Conference Reviewer @ CVPR (2024)",
        "Conference Reviewer @ ECCV (2024)",
        "Conference Reviewer @ NeurIPS (2024)",
        "Conference Reviewer @ ACM MM (2024)",
        "Journal Reviewer @ International Journal of Computer Vision (IJCV)",
        "Journal Reviewer @ IEEE Transactions on Visualization and Computer Graphics (T-VCG)"
      ],
      "Open-source / Datasets / Projects": [
        "LLM4SVG (project) - Introduces learnable SVG Semantic Tokens and a large SVGX-SFT dataset, enabling LLMs to understand and generate complex vector graphics.",
        "Reason-SVG (project) - Introduces the first framework to enhance SVG generation in LLMs through a 'Drawing-with-Thought' (DwT) paradigm—combining explicit design reasoning with code—trained via supervised fine-tuning and HyperReward-driven reinforcement learning.",
        "SVGDreamer++ (project) - An advanced text-to-SVG generator with two core innovations: Hierarchical Image Vectorization (HIVE) - enables semantic object-level and component-level image vectorization, and Adaptive Vector Primitive Control – dynamically assigns the optimal number of vector primitives, capturing fine-grained details without wasting computation.",
        "SVGFusion (project) - Improves text-to-SVG generation by using a VP-VAE to learn a vector representation of SVG elements, and a VS-DiT to generate SVGs from text prompts by performing diffusion within that learned vector space.",
        "PyTorch-SVGRender (library) - A differentiable rendering library for SVG creation. Support: Text-to-SVG, Image-to-SVG and SVG Editing. https://github.com/xingxm/PyTorch-SVGRender",
        "DiffSketcher (project) - Pioneered the use of diffusion models for text-to-vector sketch generation."
      ],
      "Representative Papers": [
        {
          "Title": "Reason-SVG: Hybrid Reward RL for Aha-Moments in Vector Graphics Generation",
          "Venue": "",
          "Year": null,
          "Type": "Conference Paper",
          "Links": ""
        },
        {
          "Title": "Empowering LLMs to Understand and Generate Complex Vector Graphics",
          "Venue": "CVPR'25",
          "Year": 2025,
          "Type": "Conference Paper",
          "Links": "Project | Code | SVGX-SFT-1M Dataset"
        },
        {
          "Title": "SVGDreamer++: Advancing Editability and Diversity in Text-Guided SVG Generation",
          "Venue": "T-PAMI 2025",
          "Year": 2025,
          "Type": "Journal Article",
          "Links": "Project | Code | Blog"
        }
      ],
      "Trigger Paper Title": "SVGDreamer: Text Guided SVG Generation with Diffusion Model",
      "Trigger Paper URL": "https://arxiv.org/abs/2312.16476",
      "Highlights": [
        "LLM4SVG accepted by CVPR 2025",
        "SVGDreamer++ accepted by T-PAMI 2025",
        "SVGDreamer accepted by CVPR 2024",
        "PyTorch-SVGRender released for differentiable SVG rendering",
        "First SVG generation paper in T-PAMI",
        "Research intern at Tencent Hunyuan on multimodal vector graphics LLMs",
        "Published in CVPR, T-PAMI, NeurIPS",
        "Open-source projects with significant GitHub adoption",
        "Invited reviewer for CVPR, NeurIPS, SIGGRAPH",
        "National Scholarship for Doctoral Students",
        "Academic Excellence Foundation of BUAA for PhD Students",
        "Developed SVG diffusion models and text-to-SVG generators",
        "Contributed to CAD-Coder for text-to-CAD generation",
        "Worked on DiffSketcher for text-to-vector sketch synthesis"
      ],
      "Radar": {
        "Academic Background": 5,
        "Research Output": 5,
        "Research Alignment": 5,
        "Technical Skills": 5,
        "Recognition & Impact": 5,
        "Communication & Collaboration": 4,
        "Initiative & Independence": 5
      },
      "Total Score": 34,
      "Detailed Scores": {
        "Academic Background": "5/5 - Ximing Xing is a Ph.D. student in Software Engineering at Beihang University, a prestigious institution in China, and has received multiple academic scholarships, including the National Scholarship for Doctoral Students and the Academic Excellence Foundation of BUAA, indicating strong academic foundation and performance.",
        "Research Output": "5/5 - Ximing Xing has published highly cited works in top-tier venues such as CVPR and NeurIPS, including groundbreaking contributions to SVG generation and diffusion models, demonstrating prolific and impactful research output.",
        "Research Alignment": "5/5 - The candidate's research interests align closely with cutting-edge areas such as AI-driven content creation, deep generative models, and multimodal SVG LLMs, which are relevant to both academic and industrial applications in software engineering and AI.",
        "Technical Skills": "5/5 - The candidate's work on diffusion models, neural rendering, and SVG generation demonstrates advanced technical skills in machine learning, computer vision, and software engineering, particularly in the domain of generative AI.",
        "Recognition & Impact": "5/5 - Ximing Xing has been recognized with prestigious awards such as the National Scholarship for Doctoral Students and has published in high-impact journals and conferences, with several papers receiving over 40 citations, reflecting significant academic recognition and influence.",
        "Communication & Collaboration": "4/5 - While there is no explicit information about collaborative projects or public speaking, the candidate's active presence on platforms like GitHub and Google Scholar suggests engagement with the broader research community, though more evidence would strengthen this assessment.",
        "Initiative & Independence": "5/5 - The candidate has independently developed novel approaches in SVG generation and diffusion models, as evidenced by their publications in top conferences, showing a strong ability to identify research problems and pursue innovative solutions."
      }
    },
    "S. Paliwal": {
      "Name": "S. Paliwal",
      "Email": "****@microsoft.com",
      "Current Role & Affiliation": "PhD student at University of California, Berkeley",
      "Current Status": "",
      "Research Keywords": [
        "Retrieval Augmented Generation",
        "LLMs",
        "Machine Learning",
        "Computer Vision"
      ],
      "Research Focus": [
        "Retrieval Augmented Generation",
        "LLMs",
        "Machine Learning",
        "Computer Vision"
      ],
      "Profiles": {
        "OpenReview": "https://openreview.net/profile?id=~Bhawna_Paliwal1"
      },
      "Publication Overview": [
        "Program Synthesis for Complex QA on Charts via Probabilistic Grammar Based Filtered Iterative Back-Translation",
        "CustomText: Customized Textual Image Generation using Diffusion Models",
        "Multi-Subject Personalization"
      ],
      "Top-tier Hits (Last 24 Months)": [
        "Findings 2023",
        "arXiv.org 2024",
        "arXiv.org 2024"
      ],
      "Honors/Grants": [],
      "Academic Service / Invited Talks": [],
      "Open-source / Datasets / Projects": [
        ""
      ],
      "Representative Papers": [
        {
          "Title": "Program Synthesis for Complex QA on Charts via Probabilistic Grammar Based Filtered Iterative Back-Translation",
          "Venue": "Findings",
          "Year": 2023,
          "Type": "Conference Paper",
          "Links": "https://www.semanticscholar.org/paper/84fa78a36c1173a58ea8e40ce272b4f14727cafd"
        },
        {
          "Title": "CustomText: Customized Textual Image Generation using Diffusion Models",
          "Venue": "arXiv.org",
          "Year": 2024,
          "Type": "Conference Paper",
          "Links": "https://www.semanticscholar.org/paper/3c13f5ab96fdb595f7457ba876ee83d241d6c107"
        },
        {
          "Title": "Multi-Subject Personalization",
          "Venue": "arXiv.org",
          "Year": 2024,
          "Type": "Conference Paper",
          "Links": "https://www.semanticscholar.org/paper/a11d91c87a086e0108b2c11bd5d4e7dcf59013f8"
        }
      ],
      "Trigger Paper Title": "CustomText: Customized Textual Image Generation using Diffusion Models",
      "Trigger Paper URL": "https://www.semanticscholar.org/paper/3c13f5ab96fdb595f7457ba876ee83d241d6c107",
      "Highlights": [],
      "Radar": {
        "Academic Background": 5,
        "Research Output": 4,
        "Research Alignment": 5,
        "Technical Skills": 5,
        "Recognition & Impact": 3,
        "Communication & Collaboration": 3,
        "Initiative & Independence": 4
      },
      "Total Score": 30,
      "Detailed Scores": {
        "Academic Background": "5/5 - S. Paliwal is a PhD student at the University of California, Berkeley, a prestigious institution known for its strong computer science and AI research programs, indicating a solid academic foundation.",
        "Research Output": "4/5 - The candidate has published in reputable venues such as Findings and arXiv, with several papers on topics like program synthesis, text generation, and personalization, showing consistent research output.",
        "Research Alignment": "5/5 - The candidate's research interests in Retrieval Augmented Generation, LLMs, and Machine Learning align well with current trends and challenges in AI, suggesting strong alignment with cutting-edge research directions.",
        "Technical Skills": "5/5 - The candidate's work spans multiple technical areas including diffusion models, probabilistic grammars, and multi-subject personalization, demonstrating a broad and deep set of technical skills.",
        "Recognition & Impact": "3/5 - While the candidate has published in relevant venues, the number of citations per paper is relatively low, suggesting that the impact of their work is still emerging or limited.",
        "Communication & Collaboration": "3/5 - There is no explicit information provided about the candidate's communication or collaboration experience, making it difficult to assess this dimension definitively.",
        "Initiative & Independence": "4/5 - The candidate has pursued independent research in diverse areas such as program synthesis and diffusion models, indicating a capacity for self-directed and innovative work."
      }
    }
  },
  "all_serp": [
    {
      "title": "[2305.10855] TextDiffuser: Diffusion Models as Text Painters",
      "url": "https://arxiv.org/abs/2305.10855",
      "snippet": "Diffusion models have gained increasing attention for their impressive generation abilities but currently struggle with rendering accurate and coherent text. To address this issue, we introduce TextDiffuser, focusing on generating images with visually appealing text that is coherent with backgrounds. TextDiffuser consists of two stages: first, a Transformer model generates the layout of ...",
      "engine": "duckduckgo",
      "authors": [],
      "term": "text generation with diffusion model NeurIPS 2026"
    },
    {
      "title": "TextDiffuser: Diffusion Models as Text Painters - NeurIPS",
      "url": "https://proceedings.neurips.cc/paper_files/paper/2023/hash/1df4afb0b4ebf492a41218ce16b6d8df-Abstract-Conference.html",
      "snippet": "TextDiffuser consists of two stages: first, a Transformer model generates the layout of keywords extracted from text prompts, and then diffusion models generate images conditioned on the text prompt and the generated layout.",
      "engine": "duckduckgo",
      "authors": [],
      "term": "text generation with diffusion model NeurIPS 2026"
    },
    {
      "title": "TextDiffuser: Diffusion Models as Text Painters - GitHub Pages",
      "url": "https://jingyechen.github.io/textdiffuser/",
      "snippet": "TextDiffuser consists of two stages. In the first Layout Generation stage, a Transformer-based encoder-decoder model generates character-level segmentation masks that indicate the layout of keywords in images from text prompts. In the second Image Generation stage, a diffusion model generates images conditioned on noisy features, segmentation masks, feature masks, and masked features (from ...",
      "engine": "duckduckgo",
      "authors": [],
      "term": "text generation with diffusion model NeurIPS 2026"
    },
    {
      "title": "Diffusion models in text generation: a survey - PMC",
      "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC10909201/",
      "snippet": "In this article, we conduct a comprehensive survey on the application of diffusion models in text generation. We divide text generation into three parts (conditional, unconstrained, and multi-mode text generation, respectively) and provide a detailed introduction.",
      "engine": "duckduckgo",
      "authors": [],
      "term": "text generation with diffusion model NeurIPS 2026"
    },
    {
      "title": "Diffusion-LM Improves Controllable Text Generation - OpenReview",
      "url": "https://openreview.net/forum?id=3s9IrEsjLyk",
      "snippet": "We propose a non-autoregressive language model based on continuous diffusions, which demonstrate strong performance in controllable text generation.",
      "engine": "duckduckgo",
      "authors": [],
      "term": "text generation with diffusion model NeurIPS 2026"
    },
    {
      "title": "AR-Diffusion: Auto-Regressive Diffusion Model for Text Generation - NeurIPS",
      "url": "https://proceedings.neurips.cc/paper_files/paper/2023/hash/7d866abba506e5a56335e4644ebe18f9-Abstract-Conference.html",
      "snippet": "Abstract Diffusion models have gained significant attention in the realm of image generation due to their exceptional performance. Their success has been recently expanded to text generation via generating all tokens within a sequence concurrently.",
      "engine": "duckduckgo",
      "authors": [],
      "term": "text generation with diffusion model NeurIPS 2026"
    },
    {
      "title": "Muddit: Liberating Generation Beyond Text-to-Image with a Unified ...",
      "url": "https://arxiv.org/abs/2505.23606",
      "snippet": "Unlike prior unified diffusion models trained from scratch, Muddit integrates strong visual priors from a pretrained text-to-image backbone with a lightweight text decoder, enabling flexible and high-quality multimodal generation under a unified architecture.",
      "engine": "duckduckgo",
      "authors": [],
      "term": "text generation with diffusion model NeurIPS 2026"
    },
    {
      "title": "NeurIPS",
      "url": "https://neurips.cc/",
      "snippet": "About the Conference. The conference was founded in 1987 and is now a multi-track interdisciplinary annual meeting that includes invited talks, demonstrations, symposia, and oral",
      "engine": "duckduckgo",
      "authors": [],
      "term": "text generation with diffusion model NeurIPS 2026"
    },
    {
      "title": "Diffusion Models for Non-autoregressive Text Generation: A Survey",
      "url": "https://arxiv.org/abs/2303.06574",
      "snippet": "Non-autoregressive (NAR) text generation has attracted much attention in the field of natural language processing, which greatly reduces the inference latency but has to sacrifice the generation accuracy. Recently, diffusion models, a class of latent variable generative models, have been introduced into NAR text generation, showing an improved text generation quality. In this survey, we review ...",
      "engine": "duckduckgo",
      "authors": [],
      "term": "text generation with diffusion model NeurIPS 2026"
    },
    {
      "title": "NeurIPS Poster Meta-Diffu$B$: A Contextualized Sequence-to-Sequence ...",
      "url": "https://neurips.cc/virtual/2024/poster/95436",
      "snippet": "Abstract: The diffusion model, a new generative modeling paradigm, has achieved significant success in generating images, audio, video, and text. It has been adapted for sequence-to-sequence text generation (Seq2Seq) through DiffuSeq, termed the S2S-Diffusion model.",
      "engine": "duckduckgo",
      "authors": [],
      "term": "text generation with diffusion model NeurIPS 2026"
    },
    {
      "title": "DiffuSeq: Sequence to Sequence Text Generation with Diffusion Models",
      "url": "https://arxiv.org/abs/2210.08933",
      "snippet": "Recently, diffusion models have emerged as a new paradigm for generative models. Despite the success in domains using continuous signals such as vision and audio, adapting diffusion models to natural language is under-explored due to the discrete nature of texts, especially for conditional generation. We tackle this challenge by proposing DiffuSeq: a diffusion model designed for sequence-to ...",
      "engine": "duckduckgo",
      "authors": [],
      "term": "text generation with diffusion model ICLR 2026"
    },
    {
      "title": "Consistent Diffusion Language Models - OpenReview",
      "url": "https://openreview.net/forum?id=inXScPz1gT",
      "snippet": "Abstract: Diffusion-based language models (DLMs) have emerged as compelling alternatives to sequential autoregressive generation, offering the promise of parallel decoding. Yet existing discrete diffusion models require hundreds of refinement steps for high-quality text, undermining the efficiency gains of parallelism. We introduce the Consistent Diffusion Language Model (CDLM), a new family ...",
      "engine": "duckduckgo",
      "authors": [],
      "term": "text generation with diffusion model ICLR 2026"
    },
    {
      "title": "Energy-Based Diffusion Language Models for Text Generation",
      "url": "https://arxiv.org/abs/2410.21357",
      "snippet": "Our analysis reveals that this degradation is a consequence of an imperfect approximation used by diffusion models. In this work, we propose Energy-based Diffusion Language Model (EDLM), an energy-based model operating at the full sequence level for each diffusion step, introduced to improve the underlying approximation used by diffusion models.",
      "engine": "duckduckgo",
      "authors": [],
      "term": "text generation with diffusion model ICLR 2026"
    },
    {
      "title": "Iclr 2026 Vla 研究现状 - 知乎",
      "url": "https://zhuanlan.zhihu.com/p/1961724511847192399",
      "snippet": "4. 2026年ICLR会议中的VLA研究趋势 在浏览了2026年ICLR会议中大多数含\"VLA\"关键词的提交论文后，我总结出以下VLA研究的关键趋势。 这些类别之间存在显著重叠，许多论文会融合多种理念——例如，将离散扩散与具身推理结合，或将高效架构与新型令牌器结合。",
      "engine": "duckduckgo",
      "authors": [],
      "term": "text generation with diffusion model ICLR 2026"
    },
    {
      "title": "Text Diffusion Model with Encoder-Decoder Transformers for Sequence-to ...",
      "url": "https://aclanthology.org/2024.naacl-long.2/",
      "snippet": "In this work, we propose SeqDiffuSeq, a text diffusion model, to approach sequence-to-sequence text generation with an encoder-decoder Transformer architecture.To improve the generation performance, SeqDiffuSeq is equipped with the self-conditioning technique and our newly proposed adaptive noise schedule technique.",
      "engine": "duckduckgo",
      "authors": [],
      "term": "text generation with diffusion model ICLR 2026"
    },
    {
      "title": "DiffuSeq: Sequence to Sequence Text Generation with Diffusion Models - ICLR",
      "url": "https://iclr.cc/virtual/2023/poster/11561",
      "snippet": "Virtual presentation / poster accept DiffuSeq: Sequence to Sequence Text Generation with Diffusion Models Shansan Gong · Mukai Li · Jiangtao Feng · Zhiyong Wu · Lingpeng Kong Keywords: [ diveristy ] [ text generation ] [ diffusion model ] [ sequence to sequence ] [ Generative models ]",
      "engine": "duckduckgo",
      "authors": [],
      "term": "text generation with diffusion model ICLR 2026"
    },
    {
      "title": "GitHub - Shark-NLP/DiffuSeq: [ICLR'23] DiffuSeq: Sequence to Sequence ...",
      "url": "https://github.com/Shark-NLP/DiffuSeq",
      "snippet": "DiffuSeq Official Codebase for DiffuSeq: Sequence to Sequence Text Generation With Diffusion Models and DiffuSeq-v2: Bridging Discrete and Continuous Text Spaces for Accelerated Seq2Seq Diffusion Models. The diffusion process of our conditional diffusion language model DiffuSeq. The diffusion process of accelerated DiffuSeq.",
      "engine": "duckduckgo",
      "authors": [],
      "term": "text generation with diffusion model ICLR 2026"
    },
    {
      "title": "ICLR Text-to-Model: Text-Conditioned Neural Network Diffusion for Train ...",
      "url": "https://iclr.cc/virtual/2025/33081",
      "snippet": "This research introduces a text-conditioned neural network diffusion approach for personalized model training, enabling efficient customization through a single training process.",
      "engine": "duckduckgo",
      "authors": [],
      "term": "text generation with diffusion model ICLR 2026"
    },
    {
      "title": "PDF Diffusion models in text generation: a survey - PeerJ",
      "url": "https://peerj.com/articles/cs-1905.pdf",
      "snippet": "In this article, we conduct a comprehensive survey on the application of diffusion models in text generation. We divide text generation into three parts (conditional, unconstrained, and multi-mode text generation, respectively) and provide a detailed introduction.",
      "engine": "duckduckgo",
      "authors": [],
      "term": "text generation with diffusion model ICLR 2026"
    },
    {
      "title": "Text generation with diffusion language models | Proceedings of the ...",
      "url": "https://dl.acm.org/doi/10.5555/3618408.3619275",
      "snippet": "In this paper, we introduce a novel dIffusion language modEl pre-training framework for text generation, which we call GENIE. GENIE is a large-scale pre-trained diffusion language model that consists of an encoder and a diffusion-based decoder, which can generate text by gradually transforming a random noise sequence into a coherent text sequence.",
      "engine": "duckduckgo",
      "authors": [],
      "term": "text generation with diffusion model ICLR 2026"
    },
    {
      "title": "PDF Empowering Diffusion Models on the Embedding Space for Text Generation",
      "url": "https://aclanthology.org/2024.naacl-long.261.pdf",
      "snippet": "In this paper, we explore the embedding diffu- sion model from two perspectives separately, i.e., the embedding space and the denoising model, based on which we conduct a thorough study re- spectively. Firstly, for diffusion models on image and audio generation, the ground truth data is sta- tionary during training.",
      "engine": "duckduckgo",
      "authors": [],
      "term": "text generation with diffusion model NAACL 2026"
    },
    {
      "title": "[2212.11685] Text Generation with Diffusion Language Models: A Pre ...",
      "url": "https://arxiv.org/abs/2212.11685",
      "snippet": "In this paper, we introduce a novel dIffusion language modEl pre-training framework for text generation, which we call GENIE. GENIE is a large-scale pretrained diffusion language model that consists of an encoder and a diffusion-based decoder, which can generate text by gradually transforming a random noise sequence into a coherent text sequence. To pre-train GENIE on a large-scale language ...",
      "engine": "duckduckgo",
      "authors": [],
      "term": "text generation with diffusion model NAACL 2026"
    },
    {
      "title": "GitHub - Yuanhy1997/SeqDiffuSeq: Text Diffusion Model with Encoder ...",
      "url": "https://github.com/Yuanhy1997/SeqDiffuSeq",
      "snippet": "About Text Diffusion Model with Encoder-Decoder Transformers for Sequence-to-Sequence Generation [NAACL 2024]",
      "engine": "duckduckgo",
      "authors": [],
      "term": "text generation with diffusion model NAACL 2026"
    },
    {
      "title": "Text Diffusion Model with Encoder-Decoder Transformers for Sequence-to ...",
      "url": "https://openreview.net/pdf?id=2UFmCXQ6zn",
      "snippet": "Abstract The diffusion model, a new generative mod-eling paradigm, has achieved great success in image, audio, and video generation. However, considering the discrete categorical nature of the text, it is not trivial to extend continuous diffusion models to natural language. In this work, we propose SeqDiffuSeq, a text diffu-sion model, to approach sequence-to-sequence text generation with an ...",
      "engine": "duckduckgo",
      "authors": [],
      "term": "text generation with diffusion model NAACL 2026"
    },
    {
      "title": "Text Diffusion Model with Encoder-Decoder Transformers for Sequence-to ...",
      "url": "https://papers.cool/venue/2024.naacl-long.2@ACL",
      "snippet": "The diffusion model, a new generative modeling paradigm, has achieved great success in image, audio, and video generation.However, considering the discrete categorical nature of the text, it is not trivial to extend continuous diffusion models to natural language. In this work, we propose SeqDiffuSeq, a text diffusion model, to approach sequence-to-sequence text generation with an encoder ...",
      "engine": "duckduckgo",
      "authors": [],
      "term": "text generation with diffusion model NAACL 2026"
    },
    {
      "title": "Empowering Diffusion Models on the Embedding Space for Text Generation",
      "url": "https://aclanthology.org/2024.naacl-long.261/",
      "snippet": "Based on the above analysis, we propose Difformer, an embedding diffusion model based on Transformer. Experiments on varieties of seminal text generation tasks show the effectiveness of the proposed methods and the superiority of Difformer over previous state-of-the-art embedding diffusion baselines.",
      "engine": "duckduckgo",
      "authors": [],
      "term": "text generation with diffusion model EMNLP 2026"
    },
    {
      "title": "Diffusion models in text generation: a survey - PubMed",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38435628/",
      "snippet": "Diffusion models are a kind of math-based model that were first applied to image generation. Recently, they have drawn wide interest in natural language generation (NLG), a sub-field of natural language processing (NLP), due to their capability to generate varied and high-quality text outputs. In this article, we conduct a comprehensive survey on the application of diffusion models in text ...",
      "engine": "duckduckgo",
      "authors": [],
      "term": "text generation with diffusion model EMNLP 2026"
    },
    {
      "title": "AoiDragon/Awesome-Text-Diffusion-Models - GitHub",
      "url": "https://github.com/AoiDragon/Awesome-Text-Diffusion-Models",
      "snippet": "A collection of papers related to text diffusion models. The organization of papers refer to our survey 'Diffusion Models for Non-autoregressive Text Generation: A Survey' , which is accepted by IJCAI 2023 survey track. If you find our survey useful for your research, please cite the following paper:",
      "engine": "duckduckgo",
      "authors": [],
      "term": "text generation with diffusion model EMNLP 2026"
    },
    {
      "title": "Table-to-Text Generation With Pretrained Diffusion Models",
      "url": "https://ieeexplore.ieee.org/document/10630478",
      "snippet": "Diffusion models have demonstrated significant potential in achieving state-of-the-art performance across various text generation tasks. In this systematic study, we investigate their application to the table-to-text problem by adapting the diffusion model to the task and conducting an in-depth analysis. Our experiments cover multiple aspects of diffusion models training. We explore sampling ...",
      "engine": "duckduckgo",
      "authors": [],
      "term": "text generation with diffusion model EMNLP 2026"
    },
    {
      "title": "Energy-based Diffusion Language Models for Text Generation",
      "url": "https://research.nvidia.com/labs/genair/publication/xu2024eddm/",
      "snippet": "Energy-based Diffusion Language Models for Text Generation Minkai Xu, Tomas Geffner, Karsten Kreis, Weili Nie, Yilun Xu, Jure Leskovec, Stefano Ermon, Arash Vahdat",
      "engine": "duckduckgo",
      "authors": [],
      "term": "text generation with diffusion model EMNLP 2026"
    },
    {
      "title": "Diffusion Language Models: The New Paradigm - Hugging Face",
      "url": "https://huggingface.co/blog/ProCreations/diffusion-language-model",
      "snippet": "How diffusion transforms language generation Diffusion Language Models fundamentally reimagine text generation through a noise-to-text transformation process rather than sequential token prediction. The approach consists of two complementary phases that mirror the proven success of image diffusion models like DALL-E and Stable Diffusion.",
      "engine": "duckduckgo",
      "authors": [],
      "term": "text generation with diffusion model ICML 2026"
    },
    {
      "title": "ICML Poster Text Generation with Diffusion Language Models: A Pre ...",
      "url": "https://icml.cc/virtual/2023/poster/23708",
      "snippet": "Poster Text Generation with Diffusion Language Models: A Pre-training Approach with Continuous Paragraph Denoise Zhenghao Lin · Yeyun Gong · Yelong Shen · Tong Wu · Zhihao Fan · Chen Lin · Nan Duan · Weizhu Chen",
      "engine": "duckduckgo",
      "authors": [],
      "term": "text generation with diffusion model ICML 2026"
    },
    {
      "title": "PDF Text Generation with Diffusion Language Models: A Pre-training ... - XMUDM",
      "url": "https://xmudm.github.io/files/lin23icml.pdf",
      "snippet": "In this paper, we introduce a novel dIffusion language modEl pre-training framework for text generation, which we call GENIE. GENIE is a large-scale pre-trained diffusion language model that consists of an encoder and a diffusion-based decoder, which can generate text by gradually transforming a random noise sequence into a coherent text sequence.",
      "engine": "duckduckgo",
      "authors": [],
      "term": "text generation with diffusion model ICML 2026"
    },
    {
      "title": "Generative AI Models: Diffusion Models and Text-to-Image Generation 2026",
      "url": "https://johal.in/generative-ai-models-diffusion-models-and-text-to-image-generation-2026/",
      "snippet": "Theoretical Foundation: Understanding Generative AI Models: Diffusion Models and Text-to-Image Generation 2026 At its core, Generative AI Models: Diffusion Models and Text-to-Image Generation 2026 represents a paradigm shift in how we approach artificial intelligence challenges. Unlike traditional methods that rely on [briefly describe traditional approach], this new approach leverages [key ...",
      "engine": "duckduckgo",
      "authors": [],
      "term": "text generation with diffusion model ICML 2026"
    },
    {
      "title": "PDF Can Diffusion Model Achieve Better Performance in Text Generation ...",
      "url": "https://aclanthology.org/2023.findings-acl.721.pdf",
      "snippet": "A typical diffusion-based text generation model contains one reverse process (from noise to data) and one forward process (from data to noise), which is shown in Figure1.",
      "engine": "duckduckgo",
      "authors": [],
      "term": "text generation with diffusion model ICML 2026"
    },
    {
      "title": "[2212.11685] Text Generation with Diffusion Language Models: A Pre ...",
      "url": "https://arxiv.org/abs/2212.11685",
      "snippet": "In this paper, we introduce a novel dIffusion language modEl pre-training framework for text generation, which we call GENIE. GENIE is a large-scale pretrained diffusion language model that consists of an encoder and a diffusion-based decoder, which can generate text by gradually transforming a random noise sequence into a coherent text sequence. To pre-train GENIE on a large-scale language ...",
      "engine": "duckduckgo",
      "authors": [],
      "term": "text generation with diffusion model KDD 2026"
    },
    {
      "title": "PDF Diffusion models in text generation: a survey - PeerJ",
      "url": "https://peerj.com/articles/cs-1905.pdf",
      "snippet": "In this article, we conduct a comprehensive survey on the application of diffusion models in text generation. We divide text generation into three parts (conditional, unconstrained, and multi-mode text generation, respectively) and provide a detailed introduction.",
      "engine": "duckduckgo",
      "authors": [],
      "term": "text generation with diffusion model KDD 2026"
    },
    {
      "title": "AoiDragon/Awesome-Text-Diffusion-Models - GitHub",
      "url": "https://github.com/AoiDragon/Awesome-Text-Diffusion-Models",
      "snippet": "A collection of papers related to text diffusion models. The organization of papers refer to our survey 'Diffusion Models for Non-autoregressive Text Generation: A Survey' , which is accepted by IJCAI 2023 survey track. If you find our survey useful for your research, please cite the following paper:",
      "engine": "duckduckgo",
      "authors": [],
      "term": "text generation with diffusion model KDD 2026"
    },
    {
      "title": "Diffusion models in text generation: a survey - PubMed",
      "url": "https://pubmed.ncbi.nlm.nih.gov/38435628/",
      "snippet": "Diffusion models are a kind of math-based model that were first applied to image generation. Recently, they have drawn wide interest in natural language generation (NLG), a sub-field of natural language processing (NLP), due to their capability to generate varied and high-quality text outputs. In this article, we conduct a comprehensive survey on the application of diffusion models in text ...",
      "engine": "duckduckgo",
      "authors": [],
      "term": "text generation with diffusion model KDD 2026"
    },
    {
      "title": "PDF Can Diffusion Model Achieve Better Performance in Text Generation ...",
      "url": "https://aclanthology.org/2023.findings-acl.721.pdf",
      "snippet": "A typical diffusion-based text generation model contains one reverse process (from noise to data) and one forward process (from data to noise), which is shown in Figure1.",
      "engine": "duckduckgo",
      "authors": [],
      "term": "text generation with diffusion model KDD 2026"
    },
    {
      "title": "Text generation with diffusion language models | Proceedings of the ...",
      "url": "https://dl.acm.org/doi/10.5555/3618408.3619275",
      "snippet": "In this paper, we introduce a novel dIffusion language modEl pre-training framework for text generation, which we call GENIE. GENIE is a large-scale pre-trained diffusion language model that consists of an encoder and a diffusion-based decoder, which can generate text by gradually transforming a random noise sequence into a coherent text sequence.",
      "engine": "duckduckgo",
      "authors": [],
      "term": "text generation with diffusion model KDD 2026"
    },
    {
      "title": "Typographic Text Generation with Off-the-Shelf Diffusion Model",
      "url": "https://arxiv.org/abs/2402.14314",
      "snippet": "Recent diffusion-based generative models show promise in their ability to generate text images, but limitations in specifying the styles of the generated texts render them insufficient in the realm of typographic design. This paper proposes a typographic text generation system to add and modify text on typographic designs while specifying font styles, colors, and text effects. The proposed ...",
      "engine": "duckduckgo",
      "authors": [],
      "term": "text generation with diffusion model KDD 2026"
    },
    {
      "title": "Generative AI Models: Diffusion Models and Text-to-Image Generation 2026",
      "url": "https://johal.in/generative-ai-models-diffusion-models-and-text-to-image-generation-2026/",
      "snippet": "Theoretical Foundation: Understanding Generative AI Models: Diffusion Models and Text-to-Image Generation 2026 At its core, Generative AI Models: Diffusion Models and Text-to-Image Generation 2026 represents a paradigm shift in how we approach artificial intelligence challenges. Unlike traditional methods that rely on [briefly describe traditional approach], this new approach leverages [key ...",
      "engine": "duckduckgo",
      "authors": [],
      "term": "text generation with diffusion model KDD 2026"
    },
    {
      "title": "An Overview of Diffusion Models for Text Generation",
      "url": "https://ieeexplore.ieee.org/document/10159911",
      "snippet": "Given the great success that diffusion models have achieved in generating various types of continuous data, including image, video and audio, there has been a growing interest in the application of these models to text generation. However, the discrete nature of text presents a challenge for diffusion models initially designed for application in a continuous feature space. The two main lines ...",
      "engine": "duckduckgo",
      "authors": [],
      "term": "text generation with diffusion model KDD 2026"
    },
    {
      "title": "PDF Diffusion Models in Text Generation: A Survey - ResearchGate",
      "url": "https://www.researchgate.net/profile/Xiangjie-Kong-2/publication/378458031_Diffusion_models_in_text_generation_a_survey/links/660bc5f3f5a5de0a9ff4b6bd/Diffusion-models-in-text-generation-a-survey.pdf",
      "snippet": "Diffusion models are a kind of math-based model that were first applied to image generation. Recently, they have drawn wide interest in natural language generation (NLG), a sub-field of natural ...",
      "engine": "duckduckgo",
      "authors": [],
      "term": "text generation with diffusion model KDD 2026"
    },
    {
      "title": "ECCV Poster DCDM: Diffusion-Conditioned-Diffusion Model for Scene Text ...",
      "url": "https://eccv.ecva.net/virtual/2024/poster/1027",
      "snippet": "The model is designed to learn the distribution of high-resolution images via two conditions: 1) the low-resolution image and 2) the character-level text embedding generated by a latent diffusion text model.",
      "engine": "duckduckgo",
      "authors": [],
      "term": "text generation with diffusion model ECCV 2026"
    },
    {
      "title": "Text Diffusion Model with Encoder-Decoder Transformers for Sequence-to ...",
      "url": "https://aclanthology.org/2024.naacl-long.2/",
      "snippet": "In this work, we propose SeqDiffuSeq, a text diffusion model, to approach sequence-to-sequence text generation with an encoder-decoder Transformer architecture.To improve the generation performance, SeqDiffuSeq is equipped with the self-conditioning technique and our newly proposed adaptive noise schedule technique.",
      "engine": "duckduckgo",
      "authors": [],
      "term": "text generation with diffusion model ECCV 2026"
    },
    {
      "title": "PDF DesignDiffusion: High-Quality Text-to-Design Image Generation with ...",
      "url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Wang_DesignDiffusion_High-Quality_Text-to-Design_Image_Generation_with_Diffusion_Models_CVPR_2025_paper.pdf",
      "snippet": "We propose DesignDiffusion, an end-to-end, diffusion-based framework for text-to-design image generation, which enables the simultaneous generation of image and visual text elements, eliminating the necessity for prede-fined text regions or traditional two-stage separated text and image creation process.",
      "engine": "duckduckgo",
      "authors": [],
      "term": "text generation with diffusion model ECCV 2026"
    },
    {
      "title": "Diffusion Models for Non-autoregressive Text Generation: A Survey",
      "url": "https://arxiv.org/abs/2303.06574",
      "snippet": "Non-autoregressive (NAR) text generation has attracted much attention in the field of natural language processing, which greatly reduces the inference latency but has to sacrifice the generation accuracy. Recently, diffusion models, a class of latent variable generative models, have been introduced into NAR text generation, showing an improved text generation quality. In this survey, we review ...",
      "engine": "duckduckgo",
      "authors": [],
      "term": "text generation with diffusion model ECCV 2026"
    },
    {
      "title": "Empowering Diffusion Models on the Embedding Space for Text Generation",
      "url": "https://aclanthology.org/2024.naacl-long.261/",
      "snippet": "Based on the above analysis, we propose Difformer, an embedding diffusion model based on Transformer. Experiments on varieties of seminal text generation tasks show the effectiveness of the proposed methods and the superiority of Difformer over previous state-of-the-art embedding diffusion baselines.",
      "engine": "duckduckgo",
      "authors": [],
      "term": "text generation with diffusion model ECCV 2026"
    },
    {
      "title": "DCDM: Diffusion-Conditioned-Diffusion Model for Scene Text Image Super ...",
      "url": "https://dl.acm.org/doi/10.1007/978-3-031-72633-0_17",
      "snippet": "In this paper, we introduce a novel generative model for scene text super-resolution called diffusion-conditioned-diffusion model (DCDM). The model is designed to learn the distribution of high-resolution images via two conditions: 1) the low-resolution image and 2) the character-level text embedding generated by a latent diffusion text model.",
      "engine": "duckduckgo",
      "authors": [],
      "term": "text generation with diffusion model ECCV 2026"
    },
    {
      "title": "ECCV Poster TextDiffuser-2: Unleashing the Power of Language ... - ECVA",
      "url": "https://eccv.ecva.net/virtual/2024/poster/527",
      "snippet": "Abstract: The diffusion model has been proven a powerful generative model in recent years, yet it remains a challenge in generating visual text. Although existing work has endeavored to enhance the accuracy of text rendering, these methods still suffer from several drawbacks, such as (1) limited flexibility and automation, (2) constrained capability of layout prediction, and (3) restricted ...",
      "engine": "duckduckgo",
      "authors": [],
      "term": "text generation with diffusion model ECCV 2026"
    },
    {
      "title": "Text Generation with Diffusion Language Models: A Pre-training Approach ...",
      "url": "https://arxiv.org/pdf/2212.11685",
      "snippet": "In this paper, we introduce a novel dIffusion language modEl pre-training framework for text generation, which we call GENIE. GENIE is a large-scale pretrained diffusion language model that consists of an encoder and a diffusion-based decoder, which can generate text by gradually transforming a random noise sequence into a coherent text sequence.",
      "engine": "duckduckgo",
      "authors": [],
      "term": "text generation with diffusion model ECCV 2026"
    },
    {
      "title": "PDF DCDM:Diffusion-Conditioned-DiffusionModel forSceneTextImageSuper ...",
      "url": "https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/02357.pdf",
      "snippet": "Severe blurring of scene text images, resulting in the loss of critical strokes and textual information, has a profound impact on text readability and recognizability. Therefore, scene text image super-resolution, aiming to enhance text resolution and legibility in low-resolution images, is a crucial task. In this paper, we introduce a novel genera- difusion-conditioned-tive model for scene ...",
      "engine": "duckduckgo",
      "authors": [],
      "term": "text generation with diffusion model ECCV 2026"
    },
    {
      "title": "PDF Enhancing Creative Generation on Stable Diffusion-based Models",
      "url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Han_Enhancing_Creative_Generation_on_Stable_Diffusion-based_Models_CVPR_2025_paper.pdf",
      "snippet": "Stable Diffusion models [17, 20] are widely adopted for their efficient text-to-image generation capabilities, sup-ported by openly accessible checkpoints. Their distilled variants, Turbo [21] and Lightning [12] are optimized specifically for faster sampling.",
      "engine": "duckduckgo",
      "authors": [],
      "term": "text generation with diffusion model CVPR 2026"
    },
    {
      "title": "SVGDreamer: Text Guided SVG Generation with Diffusion Model",
      "url": "https://arxiv.org/abs/2312.16476",
      "snippet": "Recently, text-guided scalable vector graphics (SVGs) synthesis has shown promise in domains such as iconography and sketch. However, existing text-to-SVG generation methods lack editability and struggle with visual quality and result diversity. To address these limitations, we propose a novel text-guided vector graphics synthesis method called SVGDreamer. SVGDreamer incorporates a semantic ...",
      "engine": "duckduckgo",
      "authors": [],
      "term": "text generation with diffusion model CVPR 2026"
    },
    {
      "title": "SVGDreamer: Text Guided SVG Generation with Diffusion Model",
      "url": "https://github.com/ximinng/SVGDreamer",
      "snippet": "This repository contains the official implementation of our CVPR 2024 paper, \"SVGDreamer: Text-Guided SVG Generation with Diffusion Model.\" The method leverages a diffusion-based approach to produce high-quality SVGs guided by text prompts.",
      "engine": "duckduckgo",
      "authors": [],
      "term": "text generation with diffusion model CVPR 2026"
    },
    {
      "title": "CVPR 2025 Open Access Repository",
      "url": "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_DesignDiffusion_High-Quality_Text-to-Design_Image_Generation_with_Diffusion_Models_CVPR_2025_paper.html",
      "snippet": "DesignDiffusion: High-Quality Text-to-Design Image Generation with Diffusion Models Zhendong Wang, Jianmin Bao, Shuyang Gu, Dong Chen, Wengang Zhou, Houqiang Li; Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2025, pp. 20906-20915",
      "engine": "duckduckgo",
      "authors": [],
      "term": "text generation with diffusion model CVPR 2026"
    },
    {
      "title": "CustomText: Customized Textual Image Generation using Diffusion Models",
      "url": "https://arxiv.org/abs/2405.12531",
      "snippet": "Textual image generation spans diverse fields like advertising, education, product packaging, social media, information visualization, and branding. Despite recent strides in language-guided image synthesis using diffusion models, current models excel in image generation but struggle with accurate text rendering and offer limited control over font attributes. In this paper, we aim to enhance ...",
      "engine": "duckduckgo",
      "authors": [],
      "term": "text generation with diffusion model CVPR 2026"
    },
    {
      "title": "Diffusion models for non-autoregressive text generation:",
      "url": "https://dl.acm.org/doi/abs/10.24963/ijcai.2023/750",
      "snippet": "As the background, we first present the general definition of diffusion models and the text diffusion models, and then discuss their merits for NAR generation. As the core content, we further introduce two mainstream diffusion models in existing work of text diffusion, and review the key designs of the diffusion process.",
      "engine": "duckduckgo",
      "authors": [],
      "term": "text generation with diffusion model CVPR 2026"
    },
    {
      "title": "Diffusion models in text generation: a survey - PMC",
      "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC10909201/",
      "snippet": "In this article, we conduct a comprehensive survey on the application of diffusion models in text generation. We divide text generation into three parts (conditional, unconstrained, and multi-mode text generation, respectively) and provide a detailed introduction.",
      "engine": "duckduckgo",
      "authors": [],
      "term": "text generation with diffusion model WWW 2026"
    },
    {
      "title": "Diffusion Language Models: The New Paradigm - Hugging Face",
      "url": "https://huggingface.co/blog/ProCreations/diffusion-language-model",
      "snippet": "Diffusion Language Models represent the most significant architectural innovation in language generation since the introduction of transformers, with Google's Gemini Diffusion achieving the first commercial-grade performance parity with autoregressive models in May 2025. Unlike traditional GPT-style models that generate text sequentially token by token, DLMs employ a revolutionary two-phase ...",
      "engine": "duckduckgo",
      "authors": [],
      "term": "text generation with diffusion model WWW 2026"
    },
    {
      "title": "Gemini Diffusion - Google DeepMind",
      "url": "https://deepmind.google/models/gemini-diffusion/",
      "snippet": "Gemini Diffusion is our state-of-the-art research model exploring what diffusion means for language - and text generation.",
      "engine": "duckduckgo",
      "authors": [],
      "term": "text generation with diffusion model WWW 2026"
    },
    {
      "title": "Introducing the Text Encoding Diffusion Model for Text Generation",
      "url": "https://scisimple.com/en/articles/2025-09-02-introducing-the-text-encoding-diffusion-model-for-text-generation--a37dy66",
      "snippet": "TEncDM improves text generation quality through unique encoding techniques and self-conditioning. In recent years, diffusion models have gained attention for...",
      "engine": "duckduckgo",
      "authors": [],
      "term": "text generation with diffusion model WWW 2026"
    },
    {
      "title": "Muddit: Liberating Generation Beyond Text-to-Image with a Unified ...",
      "url": "https://arxiv.org/abs/2505.23606",
      "snippet": "Unlike prior unified diffusion models trained from scratch, Muddit integrates strong visual priors from a pretrained text-to-image backbone with a lightweight text decoder, enabling flexible and high-quality multimodal generation under a unified architecture.",
      "engine": "duckduckgo",
      "authors": [],
      "term": "text generation with diffusion model WWW 2026"
    },
    {
      "title": "Diffusion models in text generation: a survey [PeerJ]",
      "url": "https://peerj.com/articles/cs-1905/",
      "snippet": "In this article, we conduct a comprehensive survey on the application of diffusion models in text generation. We divide text generation into three parts (conditional, unconstrained, and multi-mode text generation, respectively) and provide a detailed introduction.",
      "engine": "duckduckgo",
      "authors": [],
      "term": "text generation with diffusion model WWW 2026"
    },
    {
      "title": "[ICCV 2025] Decoding Correlation-Induced Misalignment in the ... - GitHub",
      "url": "https://github.com/YunzeTong/DecorrelationDiffusion",
      "snippet": "The fundamental requirement for text-to-image generation is aligning the generated images with the provided text. With large-scale data, pre-trained Stable Diffusion (SD) models have achieved remarkable performance in this task. These models process an input prompt as text control, guiding a vision model to perform denoising operations that recover a clean image from pure noise. However, we ...",
      "engine": "duckduckgo",
      "authors": [],
      "term": "text generation with diffusion model ICCV 2026"
    },
    {
      "title": "Guiding Text-to-Image Diffusion Model Towards Grounded Generation",
      "url": "https://arxiv.org/abs/2301.05221v1",
      "snippet": "The goal of this paper is to augment a pre-trained text-to-image diffusion model with the ability of open-vocabulary objects grounding, i.e., simultaneously generating images and segmentation masks for the corresponding visual entities described in the text prompt. We make the following contributions: (i) we insert a grounding module into the existing diffusion model, that can be trained to ...",
      "engine": "duckduckgo",
      "authors": [],
      "term": "text generation with diffusion model ICCV 2026"
    },
    {
      "title": "ICCV Poster Beyond Isolated Words: Diffusion Brush for Handwritten Text ...",
      "url": "https://iccv.thecvf.com/virtual/2025/poster/1930",
      "snippet": "However, this task poses significant challenges, including the accurate modeling of complex style patterns—encompassing both intra- and inter-word relationships—and maintaining content accuracy across numerous characters. To address these challenges, we propose DiffBrush, a novel diffusion-based model for handwritten text-line generation.",
      "engine": "duckduckgo",
      "authors": [],
      "term": "text generation with diffusion model ICCV 2026"
    },
    {
      "title": "ICCV Poster Draw Your Mind: Personalized Generation via Condition-Level ...",
      "url": "https://iccv.thecvf.com/virtual/2025/poster/1917",
      "snippet": "Personalized generation in T2I diffusion models aims to naturally incorporate individual user preferences into the generation process with minimal user intervention. However, existing studies primarily rely on prompt-level modeling with large-scale models, often leading to inaccurate personalization due to the limited input token capacity of T2I diffusion models. To address these limitations ...",
      "engine": "duckduckgo",
      "authors": [],
      "term": "text generation with diffusion model ICCV 2026"
    },
    {
      "title": "GitHub - Burf/DrUM: Draw Your Mind: Personalized Generation via ...",
      "url": "https://github.com/Burf/DrUM",
      "snippet": "Draw Your Mind: Personalized Generation via Condition-Level Modeling in Text-to-Image Diffusion Models, ICCV 2025 - Burf/DrUM",
      "engine": "duckduckgo",
      "authors": [],
      "term": "text generation with diffusion model ICCV 2026"
    },
    {
      "title": "ICCV Poster Harnessing Text-to-Image Diffusion Models for Point Cloud ...",
      "url": "https://iccv.thecvf.com/virtual/2025/poster/1064",
      "snippet": "Diffusion-based models, widely used in text-to-image generation, have proven effective in 2D representation learning. Recently, this framework has been extended to 3D self-supervised learning by constructing a conditional point generator for enhancing 3D representations.",
      "engine": "duckduckgo",
      "authors": [],
      "term": "text generation with diffusion model ICCV 2026"
    },
    {
      "title": "PDF The Best of Both Worlds: Integrating Language Models and Diffusion ...",
      "url": "https://openaccess.thecvf.com/content/ICCV2025/papers/Yin_The_Best_of_Both_Worlds_Integrating_Language_Models_and_Diffusion_ICCV_2025_paper.pdf",
      "snippet": "Text-to-video (T2V) [6,30,55,60] has made significant progress in recent years, becoming an important research direction in the fields of computer vision and artificial in- telligence. Recent works in T2V models have primarily revolved around two predominant paradigms: autoregres- sive large language model (LLM)-based [30,55] frame- works and diffusion-based architectures [6,60]. However, each ...",
      "engine": "duckduckgo",
      "authors": [],
      "term": "text generation with diffusion model ICCV 2026"
    }
  ],
  "sources": {
    "https://arxiv.org/abs/2305.10855": "SNIPPET: Diffusion models have gained increasing attention for their impressive generation abilities but currently struggle with rendering accurate and coherent text. To address this issue, we introduce TextDiffuser, focusing on generating images with visually appealing text that is coherent with backgrounds. TextDiffuser consists of two stages: first, a Transformer model generates the layout of ...\n\nTITLE: TextDiffuser: Diffusion Models as Text Painters\n\nBODY:\nComputer Science > Computer Vision and Pattern Recognition\n[Submitted on 18 May 2023 (v1), last revised 30 Oct 2023 (this version, v5)]\nTitle:TextDiffuser: Diffusion Models as Text Painters\nView PDFAbstract:Diffusion models have gained increasing attention for their impressive generation abilities but currently struggle with rendering accurate and coherent text. To address this issue, we introduce TextDiffuser, focusing on generating images with visually appealing text that is coherent with backgrounds. TextDiffuser consists of two stages: first, a Transformer model generates the layout of keywords extracted from text prompts, and then diffusion models generate images conditioned on the text prompt and the generated layout. Additionally, we contribute the first large-scale text images dataset with OCR annotations, MARIO-10M, containing 10 million image-text pairs with text recognition, detection, and character-level segmentation annotations. We further collect the MARIO-Eval benchmark to serve as a comprehensive tool for evaluating text rendering quality. Through experiments and user studies, we show that TextDiffuser is flexible and controllable to create high-quality text images using text prompts alone or together with text template images, and conduct text inpainting to reconstruct incomplete images with text. The code, model, and dataset will be available at \\url{this https URL}.\nSubmission history\nFrom: Lei Cui [view email][v1] Thu, 18 May 2023 10:16:19 UTC (48,086 KB)\n[v2] Wed, 24 May 2023 17:57:19 UTC (46,691 KB)\n[v3] Wed, 7 Jun 2023 05:55:26 UTC (46,691 KB)\n[v4] Tue, 13 Jun 2023 11:13:22 UTC (46,690 KB)\n[v5] Mon, 30 Oct 2023 06:33:01 UTC (47,300 KB)\nReferences & Citations\nexport BibTeX citation\nLoading...\nBibliographic and Citation Tools\nBibliographic Explorer (What is the Explorer?)\nConnected Papers (What is Connected Papers?)\nLitmaps (What is Litmaps?)\nscite Smart Citations (What are Smart Citations?)\nCode, Data and Media Associated with this Article\nalphaXiv (What is alphaXiv?)\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\nDagsHub (What is DagsHub?)\nGotit.pub (What is GotitPub?)\nHugging Face (What is Huggingface?)\nPapers with Code (What is Papers with Code?)\nScienceCast (What is ScienceCast?)\nDemos\nRecommenders and Search Tools\nInfluence Flower (What are Influence Flowers?)\nCORE Recommender (What is CORE?)\narXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.\n\nSOURCE: https://arxiv.org/abs/2305.10855",
    "https://arxiv.org/abs/2210.08933": "SNIPPET: Recently, diffusion models have emerged as a new paradigm for generative models. Despite the success in domains using continuous signals such as vision and audio, adapting diffusion models to natural language is under-explored due to the discrete nature of texts, especially for conditional generation. We tackle this challenge by proposing DiffuSeq: a diffusion model designed for sequence-to ...\n\nTITLE: DiffuSeq: Sequence to Sequence Text Generation with Diffusion Models\n\nBODY:\nComputer Science > Computation and Language\n[Submitted on 17 Oct 2022 (v1), last revised 14 Feb 2023 (this version, v3)]\nTitle:DiffuSeq: Sequence to Sequence Text Generation with Diffusion Models\nView PDFAbstract:Recently, diffusion models have emerged as a new paradigm for generative models. Despite the success in domains using continuous signals such as vision and audio, adapting diffusion models to natural language is under-explored due to the discrete nature of texts, especially for conditional generation. We tackle this challenge by proposing DiffuSeq: a diffusion model designed for sequence-to-sequence (Seq2Seq) text generation tasks. Upon extensive evaluation over a wide range of Seq2Seq tasks, we find DiffuSeq achieving comparable or even better performance than six established baselines, including a state-of-the-art model that is based on pre-trained language models. Apart from quality, an intriguing property of DiffuSeq is its high diversity during generation, which is desired in many Seq2Seq tasks. We further include a theoretical analysis revealing the connection between DiffuSeq and autoregressive/non-autoregressive models. Bringing together theoretical analysis and empirical evidence, we demonstrate the great potential of diffusion models in complex conditional language generation tasks. Code is available at \\url{this https URL}\nSubmission history\nFrom: Shansan Gong [view email][v1] Mon, 17 Oct 2022 10:49:08 UTC (1,035 KB)\n[v2] Wed, 26 Oct 2022 07:12:12 UTC (1,035 KB)\n[v3] Tue, 14 Feb 2023 06:45:49 UTC (1,136 KB)\nReferences & Citations\nexport BibTeX citation\nLoading...\nBibliographic and Citation Tools\nBibliographic Explorer (What is the Explorer?)\nConnected Papers (What is Connected Papers?)\nLitmaps (What is Litmaps?)\nscite Smart Citations (What are Smart Citations?)\nCode, Data and Media Associated with this Article\nalphaXiv (What is alphaXiv?)\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\nDagsHub (What is DagsHub?)\nGotit.pub (What is GotitPub?)\nHugging Face (What is Huggingface?)\nPapers with Code (What is Papers with Code?)\nScienceCast (What is ScienceCast?)\nDemos\nRecommenders and Search Tools\nInfluence Flower (What are Influence Flowers?)\nCORE Recommender (What is CORE?)\narXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.\n\nSOURCE: https://arxiv.org/abs/2210.08933",
    "https://arxiv.org/abs/2303.06574": "SNIPPET: Non-autoregressive (NAR) text generation has attracted much attention in the field of natural language processing, which greatly reduces the inference latency but has to sacrifice the generation accuracy. Recently, diffusion models, a class of latent variable generative models, have been introduced into NAR text generation, showing an improved text generation quality. In this survey, we review ...\n\nTITLE: Diffusion Models for Non-autoregressive Text Generation: A Survey\n\nBODY:\nComputer Science > Computation and Language\n[Submitted on 12 Mar 2023 (v1), last revised 13 May 2023 (this version, v2)]\nTitle:Diffusion Models for Non-autoregressive Text Generation: A Survey\nView PDFAbstract:Non-autoregressive (NAR) text generation has attracted much attention in the field of natural language processing, which greatly reduces the inference latency but has to sacrifice the generation accuracy. Recently, diffusion models, a class of latent variable generative models, have been introduced into NAR text generation, showing an improved text generation quality. In this survey, we review the recent progress in diffusion models for NAR text generation. As the background, we first present the general definition of diffusion models and the text diffusion models, and then discuss their merits for NAR generation. As the core content, we further introduce two mainstream diffusion models in existing work of text diffusion, and review the key designs of the diffusion process. Moreover, we discuss the utilization of pre-trained language models (PLMs) for text diffusion models and introduce optimization techniques for text data. Finally, we discuss several promising directions and conclude this paper. Our survey aims to provide researchers with a systematic reference of related research on text diffusion models for NAR generation. We present our collection of text diffusion models at this https URL.\nSubmission history\nFrom: Yifan Li [view email][v1] Sun, 12 Mar 2023 05:11:09 UTC (356 KB)\n[v2] Sat, 13 May 2023 12:42:49 UTC (539 KB)\nReferences & Citations\nexport BibTeX citation\nLoading...\nBibliographic and Citation Tools\nBibliographic Explorer (What is the Explorer?)\nConnected Papers (What is Connected Papers?)\nLitmaps (What is Litmaps?)\nscite Smart Citations (What are Smart Citations?)\nCode, Data and Media Associated with this Article\nalphaXiv (What is alphaXiv?)\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\nDagsHub (What is DagsHub?)\nGotit.pub (What is GotitPub?)\nHugging Face (What is Huggingface?)\nPapers with Code (What is Papers with Code?)\nScienceCast (What is ScienceCast?)\nDemos\nRecommenders and Search Tools\nInfluence Flower (What are Influence Flowers?)\nCORE Recommender (What is CORE?)\narXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.\n\nSOURCE: https://arxiv.org/abs/2303.06574",
    "https://arxiv.org/abs/2505.23606": "SNIPPET: Unlike prior unified diffusion models trained from scratch, Muddit integrates strong visual priors from a pretrained text-to-image backbone with a lightweight text decoder, enabling flexible and high-quality multimodal generation under a unified architecture.\n\nTITLE: Muddit: Liberating Generation Beyond Text-to-Image with a Unified Discrete Diffusion Model\n\nBODY:\nComputer Science > Machine Learning\n[Submitted on 29 May 2025 (v1), last revised 13 Oct 2025 (this version, v2)]\nTitle:Muddit: Liberating Generation Beyond Text-to-Image with a Unified Discrete Diffusion Model\nView PDF HTML (experimental)Abstract:Unified generation models aim to handle diverse tasks across modalities -- such as text generation, image generation, and vision-language reasoning -- within a single architecture and decoding paradigm. Autoregressive unified models suffer from slow inference due to sequential decoding, and non-autoregressive unified models suffer from weak generalization due to limited pretrained backbones. We introduce Muddit, a unified discrete diffusion transformer that enables fast and parallel generation across both text and image modalities. Unlike prior unified diffusion models trained from scratch, Muddit integrates strong visual priors from a pretrained text-to-image backbone with a lightweight text decoder, enabling flexible and high-quality multimodal generation under a unified architecture. Empirical results show that Muddit achieves competitive or superior performance compared to significantly larger autoregressive models in both quality and efficiency. The work highlights the potential of purely discrete diffusion, when equipped with strong visual priors, as a scalable and effective backbone for unified generation.\nSubmission history\nFrom: Jinbin Bai [view email][v1] Thu, 29 May 2025 16:15:48 UTC (3,197 KB)\n[v2] Mon, 13 Oct 2025 09:17:52 UTC (3,185 KB)\nReferences & Citations\nexport BibTeX citation\nLoading...\nBibliographic and Citation Tools\nBibliographic Explorer (What is the Explorer?)\nConnected Papers (What is Connected Papers?)\nLitmaps (What is Litmaps?)\nscite Smart Citations (What are Smart Citations?)\nCode, Data and Media Associated with this Article\nalphaXiv (What is alphaXiv?)\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\nDagsHub (What is DagsHub?)\nGotit.pub (What is GotitPub?)\nHugging Face (What is Huggingface?)\nPapers with Code (What is Papers with Code?)\nScienceCast (What is ScienceCast?)\nDemos\nRecommenders and Search Tools\nInfluence Flower (What are Influence Flowers?)\nCORE Recommender (What is CORE?)\nIArxiv Recommender\n(What is IArxiv?)\narXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.\n\nSOURCE: https://arxiv.org/abs/2505.23606",
    "https://ieeexplore.ieee.org/document/10630478": "SNIPPET: Diffusion models have demonstrated significant potential in achieving state-of-the-art performance across various text generation tasks. In this systematic study, we investigate their application to the table-to-text problem by adapting the diffusion model to the task and conducting an in-depth analysis. Our experiments cover multiple aspects of diffusion models training. We explore sampling ...\n\nTITLE: Table-to-Text Generation With Pretrained Diffusion Models\n\nBODY:\nTable-to-Text Generation With Pretrained Diffusion Models | IEEE Journals & Magazine | IEEE Xplore\n\nSOURCE: https://ieeexplore.ieee.org/document/10630478",
    "https://iclr.cc/virtual/2023/poster/11561": "SNIPPET: Virtual presentation / poster accept DiffuSeq: Sequence to Sequence Text Generation with Diffusion Models Shansan Gong · Mukai Li · Jiangtao Feng · Zhiyong Wu · Lingpeng Kong Keywords: [ diveristy ] [ text generation ] [ diffusion model ] [ sequence to sequence ] [ Generative models ]\n\nTITLE: Main Navigation\n\nBODY:\nVirtual presentation / poster accept\nDiffuSeq: Sequence to Sequence Text Generation with Diffusion Models\nShansan Gong · Mukai Li · Jiangtao Feng · Zhiyong Wu · Lingpeng Kong\nKeywords: [ diveristy ] [ text generation ] [ diffusion model ] [ sequence to sequence ] [ Generative models ]\nRecently, diffusion models have emerged as a new paradigm for generative models. Despite the success in domains using continuous signals such as vision and audio, adapting diffusion models to natural language is under-explored due to the discrete nature of texts, especially for conditional generation. We tackle this challenge by proposing DiffuSeq: a diffusion model designed for sequence-to-sequence (Seq2Seq) text generation tasks. Upon extensive evaluation over a wide range of Seq2Seq tasks, we find DiffuSeq achieving comparable or even better performance than six established baselines, including a state-of-the-art model that is based on pre-trained language models. Apart from quality, an intriguing property of DiffuSeq is its high diversity during generation, which is desired in many Seq2Seq tasks. We further include a theoretical analysis revealing the connection between DiffuSeq and autoregressive/non-autoregressive models. Bringing together theoretical analysis and empirical evidence, we demonstrate the great potential of diffusion models in complex conditional language generation tasks. Code is available at https://github.com/Shark-NLP/DiffuSeq\n\nSOURCE: https://iclr.cc/virtual/2023/poster/11561",
    "https://proceedings.neurips.cc/paper_files/paper/2023/hash/1df4afb0b4ebf492a41218ce16b6d8df-Abstract-Conference.html": "SNIPPET: TextDiffuser consists of two stages: first, a Transformer model generates the layout of keywords extracted from text prompts, and then diffusion models generate images conditioned on the text prompt and the generated layout.\n\nTITLE: TextDiffuser: Diffusion Models as Text Painters\n\nBODY:\nPart of Advances in Neural Information Processing Systems 36 (NeurIPS 2023) Main Conference Track\nJingye Chen, Yupan Huang, Tengchao Lv, Lei Cui, Qifeng Chen, Furu Wei\nDiffusion models have gained increasing attention for their impressive generation abilities but currently struggle with rendering accurate and coherent text. To address this issue, we introduce TextDiffuser, focusing on generating images with visually appealing text that is coherent with backgrounds. TextDiffuser consists of two stages: first, a Transformer model generates the layout of keywords extracted from text prompts, and then diffusion models generate images conditioned on the text prompt and the generated layout. Additionally, we contribute the first large-scale text images dataset with OCR annotations, MARIO-10M, containing 10 million image-text pairs with text recognition, detection, and character-level segmentation annotations. We further collect the MARIO-Eval benchmark to serve as a comprehensive tool for evaluating text rendering quality. Through experiments and user studies, we demonstrate that TextDiffuser is flexible and controllable to create high-quality text images using text prompts alone or together with text template images, and conduct text inpainting to reconstruct incomplete images with text. We will make the code, model and dataset publicly available.\n\nSOURCE: https://proceedings.neurips.cc/paper_files/paper/2023/hash/1df4afb0b4ebf492a41218ce16b6d8df-Abstract-Conference.html",
    "https://openreview.net/forum?id=3s9IrEsjLyk": "SNIPPET: We propose a non-autoregressive language model based on continuous diffusions, which demonstrate strong performance in controllable text generation.\n\nTITLE: Diffusion-LM Improves Controllable Text Generation\n\nBODY:\nKeywords: controllable text generation, controlled generation, infilling, language model, diffusion model\nTL;DR: We propose a non-autoregressive language model based on continuous diffusions, which demonstrate strong performance in controllable text generation.\nAbstract: Controlling the behavior of language models (LMs) without re-training is a major open problem in natural language generation. While recent works have demonstrated successes on controlling simple sentence attributes (e.g., sentiment), there has been little progress on complex, fine-grained controls (e.g., syntactic structure). To address this challenge, we develop a new non-autoregressive language model based on continuous diffusions that we call Diffusion-LM. Building upon the recent successes of diffusion models in continuous domains, Diffusion-LM iteratively denoises a sequence of Gaussian vectors into word vectors, yielding a sequence of intermediate latent variables. The continuous, hierarchical nature of these intermediate variables enables a simple gradient-based algorithm to perform complex, controllable generation tasks. We demonstrate successful control of Diffusion-LM for six challenging fine-grained control tasks, significantly outperforming prior work.\nSupplementary Material: pdf\nCommunity Implementations: [![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/diffusion-lm-improves-controllable-text/code)\n14 Replies\nLoading\n\nSOURCE: https://openreview.net/forum?id=3s9IrEsjLyk",
    "https://neurips.cc/virtual/2024/poster/95436": "SNIPPET: Abstract: The diffusion model, a new generative modeling paradigm, has achieved significant success in generating images, audio, video, and text. It has been adapted for sequence-to-sequence text generation (Seq2Seq) through DiffuSeq, termed the S2S-Diffusion model.\n\nTITLE: Main Navigation\n\nBODY:\nPoster\nMeta-Diffu$B$: A Contextualized Sequence-to-Sequence Text Diffusion Model with Meta-Exploration\nYun-Yen Chuang · Hung-Min Hsu · Kevin Lin · Chen-Sheng Gu · Ling-Zhen Li · Ray-I Chang · Hung-yi Lee\nAbstract:\nThe diffusion model, a new generative modeling paradigm, has achieved significant success in generating images, audio, video, and text. It has been adapted for sequence-to-sequence text generation (Seq2Seq) through DiffuSeq, termed the S2S-Diffusion model. Existing S2S-Diffusion models predominantly rely on fixed or hand-crafted rules to schedule noise during the diffusion and denoising processes. However, these models are limited by non-contextualized noise, which fails to fully consider the characteristics of Seq2Seq tasks. In this paper, we propose the Meta-Diffu$B$ framework—a novel scheduler-exploiter S2S-Diffusion paradigm designed to overcome the limitations of existing S2S-Diffusion models. We employ Meta-Exploration to train an additional scheduler model dedicated to scheduling contextualized noise for each sentence. Our exploiter model, an S2S-Diffusion model, leverages the noise scheduled by our scheduler model for updating and generation. Meta-Diffu$B$ achieves state-of-the-art performance compared to previous S2S-Diffusion models and fine-tuned pre-trained language models (PLMs) across four Seq2Seq benchmark datasets. We further investigate and visualize the impact of Meta-Diffu$B$'s noise scheduling on the generation of sentences with varying difficulties. Additionally, our scheduler model can function as a \"plug-and-play\" model to enhance DiffuSeq without the need for fine-tuning during the inference stage.\nChat is not available.\n\nSOURCE: https://neurips.cc/virtual/2024/poster/95436",
    "https://icml.cc/virtual/2023/poster/23708": "SNIPPET: Poster Text Generation with Diffusion Language Models: A Pre-training Approach with Continuous Paragraph Denoise Zhenghao Lin · Yeyun Gong · Yelong Shen · Tong Wu · Zhihao Fan · Chen Lin · Nan Duan · Weizhu Chen\n\nTITLE: Main Navigation\n\nBODY:\nIn this paper, we introduce a novel dIffusion language modEl pre-training framework for text generation, which we call GENIE. GENIE is a large-scale pre-trained diffusion language model that consists of an encoder and a diffusion-based decoder, which can generate text by gradually transforming a random noise sequence into a coherent text sequence. To pre-train GENIE on a large-scale language corpus, we design a new continuous paragraph denoise objective, which encourages the diffusion-decoder to reconstruct a clean text paragraph from a corrupted version, while preserving the semantic and syntactic coherence. We evaluate GENIE on four downstream text generation benchmarks, namely XSum, CNN/DailyMail, Gigaword, and CommonGen. Our experimental results show that GENIE achieves comparable performance with the state-of-the-art autoregressive models on these benchmarks, and generates more diverse text samples. The code and models of GENIE are available at https://github.com/microsoft/ProphetNet/tree/master/GENIE.\n\nSOURCE: https://icml.cc/virtual/2023/poster/23708",
    "https://arxiv.org/abs/2410.21357": "SNIPPET: Our analysis reveals that this degradation is a consequence of an imperfect approximation used by diffusion models. In this work, we propose Energy-based Diffusion Language Model (EDLM), an energy-based model operating at the full sequence level for each diffusion step, introduced to improve the underlying approximation used by diffusion models.\n\nTITLE: Energy-Based Diffusion Language Models for Text Generation\n\nBODY:\nComputer Science > Computation and Language\n[Submitted on 28 Oct 2024 (v1), last revised 7 Mar 2025 (this version, v4)]\nTitle:Energy-Based Diffusion Language Models for Text Generation\nView PDF HTML (experimental)Abstract:Despite remarkable progress in autoregressive language models, alternative generative paradigms beyond left-to-right generation are still being actively explored. Discrete diffusion models, with the capacity for parallel generation, have recently emerged as a promising alternative. Unfortunately, these models still underperform the autoregressive counterparts, with the performance gap increasing when reducing the number of sampling steps. Our analysis reveals that this degradation is a consequence of an imperfect approximation used by diffusion models. In this work, we propose Energy-based Diffusion Language Model (EDLM), an energy-based model operating at the full sequence level for each diffusion step, introduced to improve the underlying approximation used by diffusion models. More specifically, we introduce an EBM in a residual form, and show that its parameters can be obtained by leveraging a pretrained autoregressive model or by finetuning a bidirectional transformer via noise contrastive estimation. We also propose an efficient generation algorithm via parallel important sampling. Comprehensive experiments on language modeling benchmarks show that our model can consistently outperform state-of-the-art diffusion models by a significant margin, and approaches autoregressive models' perplexity. We further show that, without any generation performance drop, our framework offers a 1.3$\\times$ sampling speedup over existing diffusion models. Reproduced code is available at this https URL.\nSubmission history\nFrom: Minkai Xu [view email][v1] Mon, 28 Oct 2024 17:25:56 UTC (808 KB)\n[v2] Thu, 20 Feb 2025 01:23:30 UTC (813 KB)\n[v3] Fri, 28 Feb 2025 08:41:03 UTC (815 KB)\n[v4] Fri, 7 Mar 2025 04:28:45 UTC (813 KB)\nReferences & Citations\nexport BibTeX citation\nLoading...\nBibliographic and Citation Tools\nBibliographic Explorer (What is the Explorer?)\nConnected Papers (What is Connected Papers?)\nLitmaps (What is Litmaps?)\nscite Smart Citations (What are Smart Citations?)\nCode, Data and Media Associated with this Article\nalphaXiv (What is alphaXiv?)\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\nDagsHub (What is DagsHub?)\nGotit.pub (What is GotitPub?)\nHugging Face (What is Huggingface?)\nPapers with Code (What is Papers with Code?)\nScienceCast (What is ScienceCast?)\nDemos\nRecommenders and Search Tools\nInfluence Flower (What are Influence Flowers?)\nCORE Recommender (What is CORE?)\narXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.\n\nSOURCE: https://arxiv.org/abs/2410.21357",
    "https://iclr.cc/virtual/2025/33081": "SNIPPET: This research introduces a text-conditioned neural network diffusion approach for personalized model training, enabling efficient customization through a single training process.\n\nTITLE: Main Navigation\n\nBODY:\nPoster\nin\nWorkshop: Neural Network Weights as a New Data Modality\nText-to-Model: Text-Conditioned Neural Network Diffusion for Train-Once-for-All Personalization\nZexi Li · Lingzhi Gao · Chao Wu\nKeywords: [ personalization ] [ diffusion ] [ parameter generation ] [ neural network diffusion ]\nAbstract:\nGenerative artificial intelligence (GenAI) has made significant progress in understanding world knowledge and generating content from human languages across various modalities, like text-to-text large language models, text-to-image stable diffusion, and text-to-video Sora. While in this paper, we investigate the capability of GenAI for \\textit{text-to-model} generation, to see whether GenAI can comprehend hyper-level knowledge embedded within AI itself parameters. Specifically, we study a practical scenario termed train-once-for-all personalization, aiming to generate personalized models for diverse end-users and tasks using text prompts. Inspired by the recent emergence of neural network diffusion, we present \\textbf{\\texttt{Tina}}, a text-conditioned neural network diffusion for train-once-for-all personalization. \\texttt{Tina} leverages a diffusion transformer model conditioned on task descriptions embedded using a CLIP model. Despite the astronomical number of potential personalized tasks (e.g., $1.73\\times10^{13}$), by our design, \\texttt{Tina} demonstrates remarkable in-distribution and out-of-distribution generalization even trained on small datasets ($\\sim 1000$). We further verify whether and how \\Tina understands world knowledge by analyzing its capabilities under zero-shot/few-shot image prompts, different numbers of personalized classes, prompts of natural language descriptions, and predicting unseen entities.\nChat is not available.\n\nSOURCE: https://iclr.cc/virtual/2025/33081",
    "https://huggingface.co/blog/ProCreations/diffusion-language-model": "SNIPPET: How diffusion transforms language generation Diffusion Language Models fundamentally reimagine text generation through a noise-to-text transformation process rather than sequential token prediction. The approach consists of two complementary phases that mirror the proven success of image diffusion models like DALL-E and Stable Diffusion.\n\nTITLE: Diffusion Language Models: The New Paradigm\n\nBODY:\nDiffusion Language Models: The New Paradigm\nDiffusion Language Models represent the most significant architectural innovation in language generation since the introduction of transformers, with Google's Gemini Diffusion achieving the first commercial-grade performance parity with autoregressive models in May 2025. Unlike traditional GPT-style models that generate text sequentially token by token, DLMs employ a revolutionary two-phase diffusion process: systematically corrupting clean text through noise injection, then learning to reverse this process through iterative denoising. This paradigm shift enables parallel token generation, bidirectional context modeling, and unprecedented controllability over text generation, addressing fundamental limitations of autoregressive approaches like the reversal curse while opening new possibilities for fine-grained content control.\nHow diffusion transforms language generation\nDiffusion Language Models fundamentally reimagine text generation through a noise-to-text transformation process rather than sequential token prediction. The approach consists of two complementary phases that mirror the proven success of image diffusion models like DALL-E and Stable Diffusion.\nThe forward diffusion process systematically destroys text structure by gradually corrupting clean text over T timesteps. For discrete text tokens, this involves using categorical transition matrices that probabilistically replace original tokens with noise or mask tokens. The most sophisticated approach, discrete denoising diffusion probabilistic models (D3PM), employs transition matrices Q_t where each token can change to other vocabulary items with carefully designed probabilities. Alternative methods map discrete tokens to continuous embedding spaces and apply Gaussian noise following the equation x_t = √(α_t) x_{t-1} + √(1-α_t) ε, though this requires careful handling of the discrete-continuous boundary.\nThe reverse diffusion process represents the core innovation, where neural networks learn to progressively denoise corrupted text back to its original form. Unlike autoregressive models that predict the next token given previous context, diffusion models predict what the original clean text should be at each denoising step. This is mathematically formulated as learning p_θ(x_{t-1} | x_t), where the model must reverse the corruption process step by step.\nRecent breakthroughs like Score Entropy Discrete Diffusion (SEDD) have revolutionized this process by modeling ratios between data distributions rather than absolute probabilities. Instead of directly modeling p_θ(x), SEDD learns concrete scores s_θ(x)_y ≈ p_data(y)/p_data(x), eliminating intractable normalization constants and achieving 25-75% improvements in perplexity over previous diffusion approaches.\nArchitectural innovations enable new capabilities\nModern DLMs leverage transformer architectures with critical modifications for handling the diffusion process. The Diffusion Transformer (DiT) incorporates timestep conditioning into standard transformer blocks through sinusoidal time embeddings and adaptive layer normalization (adaLN). Each layer receives both the corrupted text sequence and the current timestep, allowing the model to adjust its denoising strategy based on the noise level.\nLLaDA (Large Language Diffusion with mAsking), released in February 2025 as the first 8-billion parameter DLM trained from scratch, demonstrates the scalability of diffusion architectures. LLaDA employs a masked diffusion process where the forward process randomly masks tokens at ratio t ~ U[0,1] during pretraining, while the reverse process uses a vanilla transformer to predict all masked tokens simultaneously. This approach achieves competitive performance with LLaMA3 8B while uniquely solving the reversal curse that plagues autoregressive models.\nThe most significant architectural advancement comes from hybrid approaches like HART (Hybrid Autoregressive Transformer), which combines autoregressive modeling for global structure with diffusion refinement for local details. This architecture achieves 4.5-7.7× higher throughput and 3.1-5.9× lower latency compared to pure diffusion models while maintaining quality advantages over purely autoregressive approaches.\nPerformance leaps mark a breakthrough year\nThe period 2024-2025 represents a watershed moment for DLMs, with multiple breakthroughs demonstrating competitive performance with established autoregressive models. Google's Gemini Diffusion, unveiled at Google I/O 2025, achieved the first commercial-grade performance parity with autoregressive models, generating text at 1,479 tokens per second - five times faster than comparable models.\nGemini Diffusion's benchmark performance reveals both strengths and current limitations. The model outperforms Gemini 2.0 Flash-Lite on coding tasks (30.9% vs 28.5% on LiveCodeBench) and demonstrates strong mathematical reasoning capabilities. However, it shows performance gaps on complex reasoning tasks like GPQA Diamond (40.4% vs 56.5%) and general knowledge benchmarks like Global MMLU (69.1% vs 79.0%), indicating areas where sequential reasoning still provides advantages.\nSEDD's ICML 2024 Best Paper Award recognized its fundamental contribution to discrete diffusion theory, while practical implementations demonstrate 6-8× better generative perplexity than GPT-2 with 32× fewer network evaluations. Meanwhile, conversion approaches like DiffuGPT and DiffuLLaMA (accepted to ICLR 2025) show that existing autoregressive models can be successfully adapted to diffusion paradigms using fewer than 200B tokens, opening pathways for leveraging existing model investments.\nFundamental advantages over autoregressive approaches\nDLMs offer compelling advantages that address core limitations of sequential generation models. Parallel token generation allows DLMs to produce entire text blocks simultaneously rather than one token at a time, potentially enabling faster generation for long sequences despite requiring multiple denoising steps.\nBidirectional context modeling represents perhaps the most significant advantage. While autoregressive models can only condition on previous tokens due to causal masking, DLMs can incorporate information from the entire sequence context during generation. This capability proves crucial for tasks requiring global coherence and enables natural support for text infilling and editing applications.\nEnhanced controllability emerges from the iterative refinement process, allowing fine-grained control over generation attributes at each denoising step. Diffusion-LM successfully demonstrated control over six different text attributes simultaneously, while the iterative process provides natural quality knobs - users can trade speed for quality by adjusting the number of denoising steps.\nCritically, DLMs address the reversal curse that affects autoregressive models. While GPT models struggle with tasks requiring reversing learned associations (like generating \"B was trained by A\" when trained on \"A trained B\"), LLaDA demonstrates superior performance on reversal tasks, surpassing GPT-4o on reversal poem completion benchmarks.\nCurrent limitations require continued development\nDespite breakthrough achievements, DLMs face significant challenges that prevent immediate widespread adoption. Computational efficiency remains problematic, with most current implementations requiring 2-10× more compute than optimized autoregressive models despite theoretical advantages in parallel generation.\nTraining complexity exceeds autoregressive approaches, requiring careful tuning of noise schedules, loss weighting, and regularization strategies. The discrete-continuous gap presents ongoing challenges, as applying continuous diffusion mathematics to discrete text tokens requires sophisticated workarounds like score matching or embedding space transformations.\nPerformance gaps persist on complex reasoning tasks, where the sequential nature of logical thinking may inherently favor autoregressive approaches. While DLMs excel at tasks requiring global coherence and controllability, they currently lag behind large autoregressive models (GPT-4, Claude) on multi-step reasoning benchmarks.\nInfrastructure limitations compound deployment challenges, as current ML infrastructure optimizes for autoregressive patterns with techniques like KV-caching that don't directly apply to diffusion models. Production deployment requires specialized serving systems and inference optimization.\nThe competitive landscape is rapidly evolving\nThe diffusion language model landscape has exploded with innovation from both academic institutions and industry labs. Stanford's SEDD established theoretical foundations for discrete diffusion, while University of Hong Kong's DiffuGPT/DiffuLLaMA series demonstrated practical scaling approaches accepted to ICLR 2025.\nGoogle DeepMind leads commercial development with Gemini Diffusion representing the first production-ready DLM, though it remains in experimental testing phase. The model's achievement of performance parity with autoregressive models marks what Principal Scientist Jack Rae called a \"landmark moment\" for the field.\nOpen source developments accelerate research adoption, with multiple models available including SEDD implementations, LLaDA, and the DiffuGPT/DiffuLLaMA series. These releases enable researchers to explore diffusion approaches without massive computational resources required for training from scratch.\nHybrid architectures emerge as a promising middle ground, with models like HART and AR-Diffusion combining autoregressive and diffusion strengths. These approaches achieve better efficiency than pure diffusion while maintaining advantages over purely autoregressive models.\nFuture directions promise expanded capabilities\nThe trajectory of DLM development points toward several transformative directions that could reshape language AI. Multimodal integration represents the most immediate opportunity, with models like VideoLLaMA 2 and SyncFlow demonstrating joint audio-video-text generation capabilities that leverage diffusion's natural support for parallel, coordinated generation across modalities.\nScaling efficiency through techniques like Mixture of Experts (MoE) and state space model integration could address current computational limitations while maintaining diffusion advantages. Flow matching approaches show promise for more efficient training and sampling, with rectified flows reducing the number of required denoising steps while maintaining generation quality.\nScientific applications appear particularly promising, with diffusion models' bidirectional modeling and iterative refinement capabilities aligning well with scientific writing, code generation, and structured content creation tasks. Early results in molecular generation and materials science suggest DLMs could become essential tools for scientific discovery.\nReal-time applications await breakthrough developments in sampling efficiency and specialized hardware acceleration. The development of streaming diffusion algorithms and dedicated inference hardware could enable conversational AI applications that leverage diffusion's controllability advantages.\nConclusion\nDiffusion Language Models have achieved a critical inflection point with Google's Gemini Diffusion demonstrating commercial viability and competitive performance with autoregressive models. The paradigm offers unique advantages in parallel generation, bidirectional context modeling, and fine-grained controllability that address fundamental limitations of sequential approaches.\nWhile challenges remain in computational efficiency, training complexity, and reasoning task performance, the rapid progress in 2024-2025 suggests these limitations are surmountable engineering challenges rather than fundamental barriers. The emergence of hybrid architectures, scaling successes like LLaDA, and theoretical advances like SEDD position DLMs as a complementary and potentially superior approach for specific applications.\nThe field stands at a crossroads where continued investment and development could establish diffusion as the preferred paradigm for controllable, high-quality text generation, while hybrid approaches may ultimately combine the best aspects of both autoregressive and diffusion methods. For practitioners and researchers, DLMs represent not just an alternative to current approaches, but a fundamentally different way of thinking about language generation that opens new possibilities for AI applications requiring sophisticated control, creativity, and coherence.\n\nSOURCE: https://huggingface.co/blog/ProCreations/diffusion-language-model",
    "https://proceedings.neurips.cc/paper_files/paper/2023/hash/7d866abba506e5a56335e4644ebe18f9-Abstract-Conference.html": "SNIPPET: Abstract Diffusion models have gained significant attention in the realm of image generation due to their exceptional performance. Their success has been recently expanded to text generation via generating all tokens within a sequence concurrently.\n\nTITLE: AR-Diffusion: Auto-Regressive Diffusion Model for Text Generation\n\nBODY:\nPart of Advances in Neural Information Processing Systems 36 (NeurIPS 2023) Main Conference Track\nTong Wu, Zhihao Fan, Xiao Liu, Hai-Tao Zheng, Yeyun Gong, yelong shen, Jian Jiao, Juntao Li, zhongyu wei, Jian Guo, Nan Duan, Weizhu Chen\nDiffusion models have gained significant attention in the realm of image generation due to their exceptional performance. Their success has been recently expanded to text generation via generating all tokens within a sequence concurrently. However, natural language exhibits a far more pronounced sequential dependency in comparison to images, and the majority of existing language models are trained with a left-to-right auto-regressive approach.To account for the inherent sequential characteristic of natural language, we introduce Auto-Regressive Diffusion (AR-Diffusion). AR-Diffusion ensures that the generation of tokens on the right depends on the generated ones on the left, a mechanism achieved through employing a dynamic number of denoising steps that vary based on token position. This results in tokens on the left undergoing fewer denoising steps than those on the right, thereby enabling them to generate earlier and subsequently influence the generation of tokens on the right.In a series of experiments on various text generation tasks, including text summarization, machine translation, and common sense generation, AR-Diffusion clearly demonstrated its superiority over existing diffusion language models and that it can be $100\\times\\sim600\\times$ faster when achieving comparable results. Our code is available at https://github.com/microsoft/ProphetNet/tree/master/AR-diffusion.\n\nSOURCE: https://proceedings.neurips.cc/paper_files/paper/2023/hash/7d866abba506e5a56335e4644ebe18f9-Abstract-Conference.html",
    "https://aclanthology.org/2023.findings-acl.721.pdf": "SNIPPET: A typical diffusion-based text generation model contains one reverse process (from noise to data) and one forward process (from data to noise), which is shown in Figure1.\n\nTITLE: (from PDF)\n\nBODY:\n11359\nFindings of the Association for Computational Linguistics: ACL 2023, pages 11359–11386\nJuly 9-14, 2023 ©2023 Association for Computational Linguistics\n\nCanDiffusionModelAchieveBetterPerformanceinTextGeneration?BridgingtheGapbetweenTrainingandInference!ZechengTang∗PinzhengWang∗KeyanZhouJuntaoLi†ZiqiangCaoMinZhangInstituteofComputerScienceandTechnology,SoochowUniversity,China{zctang,pzwang,kyzhou123}@stu.suda.edu.cn;{ljt,zqcao,minzhang}@suda.edu.cnAbstractDiffusionmodelshavebeensuccessfullyadaptedtotextgenerationtasksbymappingthediscretetextintothecontinuousspace.How-ever,thereexistnonnegligiblegapsbetweentrainingandinference,owingtotheabsenceoftheforwardprocessduringinference.Thus,themodelonlypredictsbasedonthepreviouslygeneratedreversenoiseratherthanthenoisecomputedbytheforwardprocess.Besides,thewidely-useddownsamplingstrategyinspeed-inguptheinferencewillcausethemismatchofdiffusiontrajectoriesbetweentrainingandin-ference.Tounderstandandmitigatetheabovetwotypesoftraining-inferencediscrepancies,welaunchathoroughpreliminarystudy.Basedonourobservations,weproposetwosimpleyeteffectivemethodstobridgethegapsmentionedabove,namedDistancePenaltyandAdaptiveDecaySampling.Extensiveexperimentson6generationtasksconfirmthesuperiorityofourmethods,whichcanachieve100×→200×speedupwithbetterperformance.Ourcodeisavailableathttps://github.com/CODINNLG/Bridge_Gap_Diffusion.1IntroductionWiththeprevalenceofAIGC(ArtificialIntelli-genceGeneratedContent)inrecentyears,genera-tivemodels(KingmaandWelling,2013;Goodfel-lowetal.,2020)havebeenreceivingmoreattention.Asoneoftherepresentativegenerativemodels,dif-fusionmodels(Sohl-Dicksteinetal.,2015;Songetal.,2020)haveachievedgreatsuccessonmyri-adsofgenerationtaskswithcontinuousdata,suchasimage(Songetal.,2020;Rameshetal.,2022;Rombachetal.,2022),audiogeneration(Kongetal.,2020),andmoleculegeneration(Hoogeboometal.,2022),byiterativelyrefiningtheinputnoisetomatchadatadistribution.Morerecently,diffu-sionmodelshavebeensuccessfullyadaptedtotext∗Equalcontribution.†CorrespondingAuthor.Figure1:Overviewofdiffusionmodelfortextgenera-tion,whereztdenotestheintermediatenoiseatstept.generation(Lietal.,2022;Gongetal.,2022;Linetal.,2022)byfirstleveraginganextraembeddingmodulethatmapsthediscretedataintothecontin-uousspaceandthenrecoveringthetextfromthecontinuousspacewithroundingstrategy(Lietal.,2022)orlogitsprojection(Strudeletal.,2022).Atypicaldiffusion-basedtextgenerationmodelcontainsonereverseprocess(fromnoisetodata)andoneforwardprocess(fromdatatonoise),whichisshowninFigure1.Moreconcretely,bothofthetwoprocessescanbeviewedasMarkovchains,wheretheforwardprocessgraduallyper-turbsthedataintoGaussianNoisewhilethere-verseprocessrecoverstheoriginaldatastepbystepconditionedonthecorrelatednoisefromthefor-wardprocess.Thetrainingstageinvolvesbothoftheabovetwoprocesses,whiletheinferencestageonlyconsistsofthereverseprocess,i.e.,themodelpredictsbasedonthepreviousnoiseoutputtedbythemodelitselfratherthanthecorrelatedforwardnoise.Suchdiscrepancybetweentrainingandin-ference,alsocalledexposurebias(Ranzatoetal.,2015),leadstoerroraccumulationasthedenois-ingstepsgrowduringtheinferencestage(Huszár,2015;WisemanandRush,2016).Anotherdrawbackofthediffusionmodelisthatitrequiresmultipleiterativedenoisingstepstopro-ducethefinalresultssincethereverseprocessshouldapproximatetheforwardprocess(Hoetal.,2020),whichusuallyinvolvesthousandsofsteps.Numerousiterativereversestepsofdiffusionmod-elsareinevitablytime-consumingfortextgenera-\f11360\n\ntion.Forinstance,adiffusionmodeltakesaround12hoursononesingleNVIDIAA100GPUtofin-ishtheinferenceof10Ksentenceswithalengthof128whiletheCMLM-basednon-autoregressivemodel(Ghazvininejadetal.,2019)onlytakesafewminutes1.Toacceleratetheinferencespeedintextgeneration,downsampling(NicholandDhari-wal,2021)isleveraged(Lietal.,2022;Gaoetal.,2022;Gongetal.,2022),thoughmuchfasterbutatthecostofperformanceowingtothegapbetweenthedownsampledstepsininferenceandthefulldiffusiontrajectoryinthetrainingstage.Toexploretheinsightsandthepotentialimprove-mentoftheaforementionedtraining-inferencegaps,weconductapreliminarystudywithadiffusionmodel(Gongetal.,2022)onthestorygenera-tiontaskandmainlyobservethat:(1)injectingthenoisegeneratedbythemodelitselfintothetrainingstagecanimprovethemodelperformance,and(2)theuniformdownsamplingstrategyintheinferencethattreatseachstepequallyimpairsthemodelperformance,andadaptivesamplingstrategyshouldbeappliedfordifferentgenerationstages.Accordingly,weproposetwosimpleyeteffectivestrategies:DistancePenaltyandAdaptiveDecaySampling,tobridgethetraining-inferencegapsandacceleratetheinferenceprocess.Experimentson6generationtasksof3differentsettings(directed,open-ended,andcontrollable)showthesuperior-ityofourmethodswithoutchangingtheoriginalarchitectureofthediffusionmodeloraddingmoreparameters.Surprisingly,ourmethodscanachieve100×speedupwithperformanceimprovementor200×accelerationwithcompetitiveresults.2Background2.1DiffusionModelDiffusionmodelsareoneoftheprevalentgener-ativemodels(Sohl-Dicksteinetal.,2015;Songetal.,2020;NicholandDhariwal,2021),whichcantransferanarbitrarydatadistributionintotheGaussiannoisewiththeforwardprocessandre-coverthedatafromthepurenoisewiththereverseprocessandbothtwoprocessescanberegardedasaMarkovchain.Specifically,giventhetimestepsT={0,1,···,T}andtheoriginaldatadis-tributionz0attimestept=0,theforwardpro-cessgraduallyperturbsitintotheGaussiannoise1BothdiffusionmodelandCMLMmodelsharethesamebackbonemodel,i.e.,Transformer(Vaswanietal.,2017).zT∼N(0,I)attimestept=T:q(zt|zt−1)=N(zt;p1−βtzt−1,βtI),(1)whereztrepresentstheintermediatenoiseattimesteptandβt∈(0,1)isthescalingfactor,control-lingtheamountofaddednoiseattimestept.Thereversediffusionprocessrecoverstheinitialdatadistributionz0fromtheGaussiannoisezTbypredictingthenoiseofcurrenttimesteptanddenoisingitintothenextreversestatezt−1:pθ(zt−1|zt)=N(zt−1;µθ(zt,t),Σθ(zt,t)),(2)whereµθandΣθcanbeimplementedbyneuralnetworksfθ,e.g.,Transformer2:µθ(zt,t)=1√αt(zt−βt√1−¯αtfθ(zt,t)),(3)whereαt=1−βtand¯αt=Qti=1αi.TrainingThetrainingobjectiveofthediffusionmodelistomaximizethemarginallikelihoodofdatalogpθ(z0),andthesimplifiedtrainingobjec-tivecanbewrittenas(Hoetal.,2020):Lsimple=TXt=1Eq(zt|z0)||µθ(zt,t)−ˆµ(zt,z0)||2,(4)whereˆµ(zt,z0)isthemeanofq(zt−1|z0,zt),anditisworthnotingthateachintermediatenoiseztcanbeobtaineddirectlywithouttheprevioushistoryduringthetrainingstage(Equation12).InferenceTheinferencestageonlyconsistsofthereverseprocess.Tosamplezt−1∼pθ(zt−1|zt)inEquation2,reparameterizationstrategy(KingmaandWelling,2013)isleveraged:zt−1=µθ(zt,t)+σtϵ,(5)whereϵ∼N(0,I),σ2t=βt,andztisinitializedwithpureGaussiannoiseinthebeginning.MoredetailsaboutthetrainingandinferencestagesaswellasthederivationsareshowninAppendixA.2.2DiffusionModelforTextGenerationThecoreofapplyingdiffusionmodelsfortextgen-erationtaskisthetransitionbetweendiscretespaceandcontinuousspace.Existingworksmainlyintro-ducetheembeddingfunction(Lietal.,2022)E(·)tomapthediscretetextw={w1,w2,···,wL}2Σθisoftensetasσ2tI(Hoetal.,2020),whereσ2t=βt.\f11361\n\noflengthLintothecontinuousspaceE(w)={E(w1),E(w2),···,E(wL)}∈RLd.Thus,thediffusionmodelcanhandlediscretetextgenera-tionbyaddinganextraforwardstepbeforet=0,denotedasq(z0|w)=N(E(w),σ0I),andan-otherstepattheendofthereverseprocess,i.e.,pθ(w|z0).MoredetailsaregiveninAppendixB.2.3InferenceSpeedupOnecriticalpointthatpreventstheusabilityofdiffusionmodelsintextgenerationistheirslowsamplingspeedduringinferenceduetothelongreversetrajectory,whichmakeseachdiffusionstepsimpleandeasytoestimate(Sohl-Dicksteinetal.,2015).Toacceleratetheinferencespeedintextgenerationtasks,currentworks(Lietal.,2022;Gaoetal.,2022)applythedownsamplingstrat-egy(NicholandDhariwal,2021)thatpicksthesubsetT′={t′1,t′2,···,t′k}fromthefulldiffu-siontrajectoryandeachintermediatereversestepcanbeobtainedby:z′t−1=µθ(z′t,t′)+σ′tϵ.3GapsbetweenTrainingandInferenceFromtheabovedescriptionofdiffusionmodels,wecansummarizetwogaps:(1)thereverseprocessattimesteptininferenceisconditionedonthepre-dictednoisezt+1bythemodelitselfwhilezt+1canbeobtaineddirectlywiththeforwardcomputationq(zt+1|z0)duringtraining,and(2)thedown-sampledtimesubsetT′ininferenceisinconsistentwiththefulldiffusiontrajectoryTintrainingstagewhenapplyingthedownsamplingmethodforinfer-encespeedup.Tocalibratetheeffectsofthesetwotypesoftraining-inferencegaps,welaunchastudyonthestorygenerationtaskinthissection.3.1StudySettingsWeimplementthediffusionmodelwiththetrans-formermodelandselecttheROCStories(ROC)corpus(Mostafazadehetal.,2016)forthestorygenerationtask.Specifically,giventhepromptorthesourcesentencewxandthereferencewy,weapplythepartiallynoisingstrategy(Gongetal.,2022)fortraining(AppendixA).WeutilizeBLEU(B-2)score(Papinenietal.,2002)toreflectthegenerationprecision(thehigher,thebetter),Lexi-calRepetition(LR-2)score(Shaoetal.,2019)toshowthediversityoftext(thelower,thebetter),ROUGE(R-2)torepresenttherecallofgenerationresult(thehigher,thebetter)andPerplexity(PPL)toreflectsthefluency(thelower,thebetter).More(a)B-2scores.(b)LR-2scores.(c)PPLscores.Figure2:Evaluationresultsofnoiseinjection,wherethenumberinabscissarepresentsγ2andγ1=1−γ2.implementationdetailsareinAppendixC.3.2AnalysisTrainingwithPredictedNoiseTomitigatethetraining-inferencegap,itisnaturaltoinjectpartofthepredictednoisesintothetrainingstagebyreplacingtheforwardnoisezt+1inpθ(zt|zt+1)withthepredictednoisez′t+1fromthe(t+1)-thstepofthereverseprocessorinjectingthepredictednoiseintoztbyreplacing||µθ(zt,t)−ˆµ(zt,z0)||2inEquation4withγ1||µθ(zt,t)−ˆµ(zt,z0)||2+γ2||µθ(zt,t)−ˆµ(z′t,t)||2,wherezt∼q(zt|z0)andz′t∼pθ(zt|z′t+1).Wereporttheevaluationre-sultsinFigure2withdifferentsettingsofγ1andγ2andcanmainlyobservethatreplacingtheforwardnoisewiththepredictednoise(γ2=1,γ1=0)doesmitigatethetraining-inferencegapbyachiev-ingabetterperformancethanthevanillatrainingscheme(γ2=0,γ1=1),andtheinjectingstrat-egyperformsbetterthanthereplacingone.Moredetailsaboutnoisereplacementoperationandeval-uationresultsareshowninAppendixD.1.SamplingStrategyDownsamplingcanacceler-atetheinferencebyuniformlyselectingthesubsetsT′fromthefulldiffusiontrajectoryTbutatthecostofperformance.Suchauniformsamplingstrategytreatseachreversestepequallywhilene-glectingthediscrepanciesamongthemincontri-butiontothefinalresult.Toexplorewhethersuchanequal-stepsamplingstrategybringstheperfor-mancedecrease,wesimplycomparedifferentnon-uniformsamplingschemes.Alongwiththereversesteps,wesplitthereverseprocessintothreestages[κ1,κ2,κ3]anddownsampledifferentnumbersofstepsforeachstagebutkeepthetotaldownsam-pledstepsthesame3.AsshowninFigure3,wecanobservethatwhendownsamplingmorestepsfromκ1(orangecurve),themodelcanachieve3Fortotalnumberofdownsampledsteps20,wecansample{[12,4,4],[4,12,4],[4,4,12],[8,4,8]}stepsas[κ1,κ2,κ3].\f11362\n\n(a)B-2ofnon-uniformsampling.(b)R-2ofnon-uniformsampling.Figure3:Comparisonbetweennon-uniformstepsof[κ1,κ2,κ3]andtheoriginaluniformscheme,wherethex-axisrepresentsthedenoisingsteps,andy-axisillus-tratestheevaluationresultsforeachmetric.Thedotsoneachcurveindicatethenumberofdown-sampledsteps.abetterperformancethanotherdownsamplingschemes(greencurve,redcurve,andpurplecurve)andevenexceedtheoriginalfullreversesteps(bluecurve).Inotherwords,theequal-stepuniformdownsamplingschemedoeslimitthemodelcapa-bility,andthesimplenon-uniformdownsamplingstrategycanmitigatesuchissueandmeanwhileacceleratetheinferencespeed.ExtensiveTrialsAsmentionedabove,thegapbroughtbythedifferentdiffusiontrajectoriesintheinferencestage,i.e.,downsampledreversestepsv.s.thefullreversesteps,furtheraggravatesthetraining-inferencediscrepancy.Inviewthatsim-plyinjectingthepredictedreversenoiseintrainingcaneffectivelynarrowthegapsbetweentrainingandinference,itisalsoappealingtomakesuchastrategyadapttothedownsampleddiffusiontra-jectories,i.e.,introducingthedownsampledre-versenoisesinthetrainingstage.Forinstance,wecaninjectthepredictedreversenoisedownsam-pledfromthereversestepsof(t,t+δ]intothed-th(d∼(t,t+δ])forwardnoisetocomputethet-thstepreversenoise,i.e.,replacingtheforwardnoisezt+1inpθ(zt|zt+1)withzd∼(t,t+δ].Intuitively,addingaperturbationwithareason-ablerangeofvaluesintrainingcanmakethemodelmorerobusttowardstheperturbationduringinfer-ence,whileanunconstrainedperturbationvaluemightriskthemodeltraining,e.g.,thetrainingcollapseinauto-regressivetextgenerationmod-Figure4:Euclideandistancebetweenthepredictedre-versenoiseandtheforwardnoiseofaconvergedmodel.els(Zhangetal.,2019b).Forourpurposes,thediscrepancybeforeandafterinjectingthedown-sampledreversenoiseineachtrainingstepshouldfallinarationalrange,whichmainlydependsonthetimesteptandthechoiceofδ.Toexploremoreinsights,wedepictthediscrepancybetweenpre-dictedreversenoisesandforwardnoisesalongwith200randomlyselectedcontinuoustimestepswiththeEuclideandistance,whichisconsistentwiththetrainingobjectiveinEquation4.Tosimplifythestudyexperiment,wedownsampleatimestepforeverytwentysteps4.AsshowninFigure4,wecanobservethat(1)thediscrepancybetweenpre-dictedreversenoisesandforwardnoisesisgettinglargeralongwiththeincreaseoftimestept(reddiagonalarrow),and(2)thedifferencesbetweentheforwardnoiseattimesteptandthepredictedreversenoisefromttot+δarebecominglargeralongwiththeincreaseoftimestep(yellowhor-izontalarrow).Thus,therangeofdownsampledreversenoisestepsshouldbegraduallynarrowedalongwiththeincreaseoftimestep.3.3PotentialImprovementBasedontheanalysismentionedabove,wecanconcludethat:(1)injectingthepredictedreversenoiseintothetrainingstagecanmitigatethetraining-inferencegaps,(2)theschemeofuniformdownsamplingininferencewhichtreatseachstepequallyharmsthemodelperformance,andanon-uniformadaptivemethodshouldbedesigned,and(3)inspiredby(1)and(2),wecaninjectthedown-sampledreversenoisesintothetrainingstagewhiletherangeofdownsampledstepsshouldbegradu-4Weutilizethediffusionmodeltrainedwith240Ksteps.MoreimplementationdetailsareshowninAppendixD.2\f11363\n\nallynarrowedasthetimestepincreases.4MethodWeproposetwosimpleyeteffectivemethods:Dis-tancePenaltyinthepost-trainingstageandAdap-tiveSparseSamplingininferencetobridgethegapswithoutintroducinganyarchitecturemodifi-cationtodiffusionmodels.Thus,itcanbeflexiblyadaptedtodifferentdiffusionmodelvariants.4.1DistancePenaltyWefirstintroducetheDistancePenaltystrategy,whichinjectstheDownsampledpredictedreversenoiseintothepost-trainingstageofdiffusionmod-elsthatconsistsofTtimesteps.5Forbetterillustra-tion,weutilizenewsymbolsK={0,1,···,K}forthetimestepsinthepost-trainingstagetodis-tinguishfromtheoriginaldiffusiontrajectoryTinthetrainingstage.TheoverviewoftheDistancePenaltystrategyisshowninFigure5.DownsamplingRangeinTrainingToobtainarationalpredictedreversenoiseforeachstepk,i.e.,conductthedownsamplingoperationintherangeRk={k−1,···,k−h},andmitigatethetraining-inferencegaps,weconstrainthetotalamountofnoisesinRkwiththethresholdωkadj:ωkadj=√1−¯αKk′,(6)where√1−¯αKdenotesthescalingfactorthatcontrolsthevarianceofnoiseaccumulatedatstepK(AppendixA),andk′isthenumbero\n...[truncated]",
    "https://openreview.net/pdf?id=2UFmCXQ6zn": "SNIPPET: Abstract The diffusion model, a new generative mod-eling paradigm, has achieved great success in image, audio, and video generation. However, considering the discrete categorical nature of the text, it is not trivial to extend continuous diffusion models to natural language. In this work, we propose SeqDiffuSeq, a text diffu-sion model, to approach sequence-to-sequence text generation with an ...\n\nTITLE: (from PDF)\n\nBODY:\nText Diffusion Model with Encoder-Decoder Transformers for\nSequence-to-Sequence Generation\n\nAnonymous ACL submission\n\n001\n002\n003\n004\n\n005\n006\n007\n008\n009\n010\n\n011\n012\n013\n014\n015\n016\n\n017\n018\n019\n020\n021\n022\n\n023\n\n024\n\n025\n\n026\n\n027\n\n028\n\n029\n\n030\n\n031\n\n032\n\n033\n\n034\n\n035\n\n036\n\n037\n\n038\n\n039\n\n040\n\n041\n\nAbstract\n\nThe diffusion model, a new generative mod-\neling paradigm, has achieved great success in\nimage, audio, and video generation. However,\nconsidering the discrete categorical nature of\nthe text, it is not trivial to extend continuous\ndiffusion models to natural language. In this\nwork, we propose SeqDiffuSeq, a text diffu-\nsion model, to approach sequence-to-sequence\ntext generation with an encoder-decoder Trans-\nformer architecture. To improve the generation\nperformance, SeqDiffuSeq is equipped with\nthe self-conditioning technique and our newly\nproposed adaptive noise schedule technique.\nSelf-conditioning enables SeqDiffuSeq to bet-\nter use the predicted sequence information dur-\ning the generation process. The adaptive noise\nschedule balances the difficulty of denoising\nacross time steps at the token level. Exper-\niment results illustrate the improved perfor-\nmance on five sequence-to-sequence generation\ntasks compared to other diffusion-based models\nregarding text quality and inference time.\n\n1\n\nIntroduction\n\nGenerative modeling is drawing more attention in\nrecent years of machine learning research due to\nthe development of diffusion models (Ho et al.,\n2020). Diffusion models define the forward pro-\ncess and the reverse process where the former grad-\nually diffuses data to random noise while the latter\nrecovers data from random noise iteratively, which\nhave shown superior performance on synthesiz-\ning images (Rombach et al., 2021), audios (Kong\net al., 2020), and videos (Ho et al., 2022) over other\ngenerative methods, such as generative adversar-\nial network (GAN) (Goodfellow et al., 2014) and\nnormalizing flow (Kobyzev et al., 2021).\n\nIt is not trivial to extend diffusion models to the\ngeneration of natural languages. Most of the ex-\nisting diffusion models are applied to continuous\nfeature space (Ho et al., 2020; Nichol and Dhari-\nwal, 2021) while texts are sequences of discrete\n\n1\n\ncategorical tokens. Recently, research has explored\ncategorical diffusion models in discrete space for\ntext generation (Hoogeboom et al., 2021; Austin\net al., 2022). There also exists research such as Dif-\nfusionLM (Li et al., 2022) that applies continuous\ndiffusion models to word embedding. However,\nthese works only focus on unconditional and con-\ntrolled text generation.\n\nSequence-to-sequence text generation is a fun-\ndamental natural language processing setting and\ncovers various practical downstream tasks, such as\ndialogue (Ni et al., 2021) and machine translation\n(Liu et al., 2020). In recent practice, researchers\nresort to auto-regressive (AR) (Dai et al., 2019)\nor non-auto-regressive (NAR) (Gu et al., 2019)\nTransformers for the tasks, and achieve good gen-\neration performance. Using diffusion models, a\nrecent work named DiffuSeq (Gong et al., 2022)\napplies the diffusion-based method for sequence-\nto-sequence text generation. They deploy encoder-\nonly Transformers and partially define diffusion\nand denoising processes on output sequences.\n\nIn this work, we explore diffusion models\nwith encoder-decoder Transformer architecture for\nsequence-to-sequence generation. We propose Se-\nqDiffuSeq which extends the continuous diffusion\nframework proposed in DiffusionLM (Li et al.,\n2022) to sequence-to-sequence settings. We equip\nSeqDiffuSeq with the self-conditioning technique\n(Chen et al., 2022) and our newly proposed adap-\ntive noise schedule. Self-conditioning helps the\nmodel better capture the information from former it-\nerations during the generation. The proposed adap-\ntive noise schedule learns a token-level noise sched-\nule to better control the amount of noise injected\nand information recovered during the forward and\nreverse process (Nichol and Dhariwal, 2021).\n\nWe conduct experiments on five generation tasks.\nResults show that SeqDiffuSeq achieves compet-\nitive performance compared with AR and NAR\nbaselines in terms of generation quality and diver-\n\n042\n\n043\n\n044\n\n045\n\n046\n\n047\n\n048\n\n049\n\n050\n\n051\n\n052\n\n053\n\n054\n\n055\n\n056\n\n057\n\n058\n\n059\n\n060\n\n061\n\n062\n\n063\n\n064\n\n065\n\n066\n\n067\n\n068\n\n069\n\n070\n\n071\n\n072\n\n073\n\n074\n\n075\n\n076\n\n077\n\n078\n\n079\n\n080\n\n081\n\n082\n\n\f083\n\n084\n\n085\n\n086\n\n087\n\n088\n\n089\n\n090\n\n091\n\n092\n\n093\n\n094\n\n095\n\n096\n\n097\n\n098\n\n099\n\n100\n\n101\n\n102\n\n103\n\n104\n\n105\n\n106\n\n107\n\n108\n\n109\n\n110\n\n111\n\n112\n\n113\n\n114\n\n115\n\n116\n\n117\n\n118\n\n119\n\n120\n\n121\n\n122\n\n123\n\n124\n\n125\n\n126\n\n127\n\n128\n\n129\n\n130\n\n131\n\nsity. SeqDiffuSeq also shows improved genera-\ntion performance and inference speed compared\nto text diffison model DiffuSeq. Ablation stud-\nies demonstrate that our model can benefit from\nself-conditioning and adaptive noise schedule tech-\nniques, and both are complementary to each other\nin sequence-to-sequence settings.\n\nTo summarize, the main contributions of this\n\nwork are as follows:\n\n1. We propose SeqDiffuSeq that extends the\ncontinuous text diffusion model to sequence-\nto-sequence text generation with encoder-\ndecoder Transformer architecture.\n\n2. The self-conditioning and newly proposed\nadaptive noise schedule technique can effec-\ntively improve the generation quality of the\ntext diffusion model.\n\n3. Experiments show SeqDiffuSeq achieves\npromising performance with the previous\ndiffusion-based method DiffuSeq as well as\nAR and NAR models on five generation tasks.\n\n2 Related Work\n\nSince the great success of diffusion models in vi-\nsion (Ho et al., 2020; Rombach et al., 2021; Song\net al., 2021b), researchers have explored extend-\ning diffusion models to text generation. Consid-\nering the discrete and categorical nature of texts,\nMultinomial Diffusion (Hoogeboom et al., 2021)\nand D3PM (Austin et al., 2021) are proposed for\nmodeling categorical data. They define discrete\ndiffusion models using discrete categorical transi-\ntions directly on texts. DiffusionBERT (He et al.,\n2022) follows D3PM and introduces pre-trained\nmodels for language modeling. Besides, recent\nresearch also explores converting texts into con-\ntinuous features to adapt to diffusion models. Bit\nDiffusion (Chen et al., 2022) encodes discrete data\nas binary bits and treats these binary bits as real\nnumber features. Yu et al. (2022) is proposed to\nbuild text diffusion models in continuous latent\nspace. DiffusionLM (Li et al., 2022) uses the word\nembedding space for continuous diffusion mod-\nels and introduces auxiliary losses to enable joint\nlearning of embedding and network parameters.\nFollowing DiffusionLM, recent research explores\nimproving text generation quality (Strudel et al.,\n2022), and DiffuSeq (Gong et al., 2022) extends it\nto sequence-to-sequence settings. Compared to Dif-\nfuSeq, we propose a different model architecture\n\nand self-conditioning and adaptive noise schedule\ntechniques to improve sequence-to-sequence gen-\neration performance.\n\nNoise schedules in diffusion models control the\nlevel of noise injected and the level of information\nrecovered in the forward and reverse process re-\nspectively. Previous research in vision and texts\ndemonstrates that appropriate noise schedule de-\nsign can improve the generation quality perfor-\nmance of diffusion models (Nichol and Dhariwal,\n2021; Li et al., 2022). Concurrently, Diffusion-\nBERT (He et al., 2022) proposes a spindle sched-\nule for language modeling, and CDCD (Dieleman\net al., 2022) designs a learned noise schedule for\nlanguage modeling and machine translation. Dif-\nferent from both concurrent works, SeqDiffuSeq\nis proposed with a token-level noise schedule that\nbalances the difficulty of denoising across time\nsteps. Gao et al. (2023) proposes Difformer and is\northogonal to our work.\n\n3 Preliminary\n\nDiffusion model is generally formulated by a de-\nsigned forward diffusion process and a learnt re-\nverse denoising process. In the forward diffusion\nprocess, samples gradually mix with random noise,\nwhile in the reverse denoising process, the random\nnoise is gradually denoised to generate synthetic\nsamples. We adopt the forward and reverse pro-\ncesses proposed in DDPM (Ho et al., 2020).\n\nFor the forward process, given a sample z0 from\na real-world data distribution q(z0). At each time\nstep t ∈ {1, 2, · · · , T }, a noise sample zt is sam-\nαtzt−1, (1 −\npled from zt ∼ q(zt|zt−1) = N (zt;\nαt)I), where αt control the noise added at time\nstep t. In this regard, when T is large enough, a\nreal-world sample will gradually and ultimately\ndiffuse to a standard Gaussian noise distribution.\n\n√\n\nFor the reverse process, the diffusion model\nuses a learnt parameterized denoising distribution\nzt−1 ∼ pθ(zt−1|zt) to gradually recover samples\nfrom noise. The denoising distribution is parame-\nterized by θ and is to fit the posterior distribution\nq(zt−1|zt, z0) of the forward process.\n\nq(zt−1|zt, z0) = N (zt−1; ˜µ(z0, zt), ˜βtI).\n\n(1)\n\nIn this equation,\n\n˜µ(z0, zt) =\n\n√\n\n¯αt−1βt\n1 − ¯αt\n\nz0 +\n\n√\n\nαt(1 − ¯αt−1)\n1 − ¯αt\n\nzt,\n\n(2)\n\n¯αt =\n\nt\n(cid:89)\n\ns=1\n\nαs,\n\nβt = 1 − αt,\n\n˜βt =\n\n1 − ¯αt−1\n1 − ¯αt\n\nβt.\n\n(3)\n\n2\n\n132\n\n133\n\n134\n\n135\n\n136\n\n137\n\n138\n\n139\n\n140\n\n141\n\n142\n\n143\n\n144\n\n145\n\n146\n\n147\n\n148\n\n149\n\n150\n\n151\n\n152\n\n153\n\n154\n\n155\n\n156\n\n157\n\n158\n\n159\n\n160\n\n161\n\n162\n\n163\n\n164\n\n165\n\n166\n\n167\n\n168\n\n169\n\n170\n\n171\n\n172\n\n173\n\n174\n\n175\n\n176\n\n177\n\n178\n\n\f179\n\n180\n\n181\n\n182\n\n183\n\n184\n\n185\n\n186\n\n187\n\n188\n\n189\n\n190\n\n191\n\n192\n\n193\n\n194\n\n195\n\n196\n\n197\n\n198\n\n199\n\n200\n\n201\n\n202\n\n203\n\n204\n\n205\n\n206\n\n207\n\n208\n\n209\n\n210\n\n211\n\n212\n\n213\n\n214\n\n215\n\n216\n\n217\n\n218\n\n219\n\n220\n\n221\n\n222\n\n223\n\n224\n\n225\n\n226\n\n227\n\nWith learnt denoising distribution pθ, a synthetic\nreal-world sample z0 can be generated from pure\nrandom noise zT step-by-step.\n\n4 Approach\n\nIn this section, we present the main design of our\nproposed SeqDiffuSeq for sequence-to-sequence\nlanguage generation. The overview of SeqDiffuSeq\nis depicted in Figure 1. In the following sections,\nthe input and output sequences are denoted as wx\nand wy respectively. For the i-th token in wy, the\ntoken is denoted as wi\ny, where 0 < i ≤ n and\nn represents the maximum output sequence word\nlength. In order to avoid lengthy notations, we omit\nthe indices referring to different data samples.\n\n4.1 Diffusion Model\n\nForward Process To fit diffusion models to\nsequence-to-sequence settings, we extend the text\ndiffusion model, DiffusionLM (Li et al., 2022).\n\nIn the sequence-to-sequence setting, the forward\nprocess gradually changes the target output se-\nquence wy to random noise. Diffusing wy to pure\nrandom noise is independent of the input sequence\nwx. For the sequence wy, we use an embedding\nfunction gϕ to map the word tokens wi\ny to con-\ny) ∈ Rd, where\ntinuous word embedding gϕ(wi\nd represents the dimension of embedding and ϕ\nrepresents the parameters of the word embedding\nfunction. The embedding for the sequence wy is\ndefined by stacking the tokens’ embedding and is\ndenoted as gϕ(wy) ∈ Rn×d. At the beginning of\nthe forward process, a Markovian transition pa-\nrameterized by qϕ(z0|wy) = N (z0; gϕ(wy), β0I)\nis added. Extended by qϕ(z0|wy), the forward pro-\ncess can continue to diffuse continuous features of\nz0 iteratively. For each time step t, we apply the\ndiffusion distribution q(zt|zt−1) to get noisier sam-\nples. Ultimately, the output sequence wy becomes\nzT which is nearly pure random noise following\nstandard Gaussian distribution.\n\nReverse Process Diffusion models generate the\nsynthetic samples by successively sampling the de-\nnoising distribution in the reverse process. For each\ntime step t in the reverse process, a learnt denoising\ndistribution pθ parameterized by θ generates sam-\nples zt−1 conditioned on the former noisier sam-\nples zt. In the sequence-to-sequence setting, the\ngenerated sequences correlate to input sequences.\nTherefore, the denoising distribution is addition-\nally conditioned on the input sequence wx, and\n\npθ = pθ(zt−1|zt, wx). After the reverse denoising\nprocess reaches T = 0, we round each column of\nthe generated ˆz0 to its nearest word in the embed-\nding space by the rounding distribution ˜pϕ(wy|ˆz0)\nto generate the final word sequences.\n\nTraining Objective We optimize θ and embed-\nding parameters by minimizing the variational\nbound of the data log-likelihood:\n\nLV B = Eqϕ(z0:T ,wx,wy)[log\n\n+\n\nT\n(cid:88)\n\nt=2\n\nlog\n\nq(zt−1|z0, zt)\npθ(zt−1|zt, wx)\n\nq(zT |z0)\np(zT )\n\n− log pθ(z0|z1, wx)\n\n+ log qϕ(z0|wy) − log ˜pϕ(wy|z0)],\n\n(4)\n\nThe training objective is to narrow down the dis-\ncrepancy between pθ(zt−1|zt, wx) and the poste-\nrior q(zt−1|zt, z0) in the forward process. Since\nq(zt−1|zt, z0) follows the form of Gaussian dis-\ntribution, we parameterize the denoising distribu-\ntion following Gaussian distribution family and\npθ(zt−1|zt, wx) = N (zt−1; ˜µθ(zt, wx, t), ˜βtI),\nwhere\n\n˜µθ(zt, wx, t) =\n√\n\n¯αt−1βt\n1 − ¯αt\n\nz0\nθ (zt, wx, t) +\n\n√\n\nαt(1 − ¯αt−1)\n1 − ¯αt\n\nzt. (5)\n\nz0\nθ (zt, wx, t) is named the denoising function and\npredicts the estimated output embedding sequences\nat each reverse step t. Then according to density\nfunctions q and pθ following Gaussian distribution,\nthe objective can be further simplified as:\n\nLsimple =\n\nEqϕ(z0,wx,wy)[\n\nT\n(cid:88)\n\nt=2\n\nEq(zt|z0)∥z0\n\nθ (zt, wx, t) − z0∥2\n\n+ ∥˜µ(zT , z0)∥2 + ∥z0\n− log ˜pϕ(wy|z0)],\n\nθ (z1, wx, 1) − gϕ(wy)∥2\n\n(6)\n\n√\n\n¯αtz0, (1− ¯αt)I) for effi-\nwhere q(zt|z0) = N (zt;\ncient sampling of zt during training, and µT (z0) =\n√\n¯αT z0. We leave the detailed derivation to Ap-\npendix B. The training objective becomes to fit\ngϕ(wy) and the denoising function z0\nθ (zt, wx, t),\nwhich we can model with encoder-decoder Trans-\nformers architectures. During training, the sam-\npling distribution qϕ contains trainable parame-\nters of word embedding. We can backpropagate\nthrough this with reparameterization trick (Kingma\nand Welling, 2013).\n\n3\n\n228\n\n229\n\n230\n\n231\n\n232\n\n233\n\n234\n\n235\n\n236\n\n237\n\n238\n\n239\n\n240\n\n241\n\n242\n\n243\n\n244\n\n245\n\n246\n\n247\n\n248\n\n249\n\n250\n\n251\n\n252\n\n253\n\n254\n\n255\n\n256\n\n257\n\n258\n\n259\n\n260\n\n261\n\n262\n\n263\n\n264\n\n265\n\n266\n\n267\n\n268\n\n\fFigure 1: The overview of SeqDiffuSeq with an encoder-decoder Transformers architecture.\n\n269\n\n270\n\n271\n\n272\n\n273\n\n274\n\n275\n\n276\n\n277\n\n278\n\n279\n\n280\n\n281\n\n282\n\n283\n\n284\n\n285\n\n286\n\n287\n\n288\n\n289\n\n290\n\n291\n\n292\n\n293\n\n294\n\n295\n\n296\n\n297\n\n298\n\n299\n\n300\n\n301\n\n302\n\n303\n\n304\n\n305\n\nDenoising with Encoder-Decoder Framework\nUnlike DiffuSeq (Gong et al., 2022) using encoder-\nonly Transformer architecture, we propose using\nan encoder-decoder Transformers architecture to\nmodel the input and output text sequences. For\nz\n...[truncated]",
    "https://dl.acm.org/doi/abs/10.24963/ijcai.2023/750": "SNIPPET: As the background, we first present the general definition of diffusion models and the text diffusion models, and then discuss their merits for NAR generation. As the core content, we further introduce two mainstream diffusion models in existing work of text diffusion, and review the key designs of the diffusion process.\n\n[FetchError] HTTP 403 for https://dl.acm.org/doi/abs/10.24963/ijcai.2023/750\n\nSOURCE: https://dl.acm.org/doi/abs/10.24963/ijcai.2023/750",
    "https://dl.acm.org/doi/10.1007/978-3-031-72633-0_17": "SNIPPET: In this paper, we introduce a novel generative model for scene text super-resolution called diffusion-conditioned-diffusion model (DCDM). The model is designed to learn the distribution of high-resolution images via two conditions: 1) the low-resolution image and 2) the character-level text embedding generated by a latent diffusion text model.\n\n[FetchError] HTTP 403 for https://dl.acm.org/doi/10.1007/978-3-031-72633-0_17\n\nSOURCE: https://dl.acm.org/doi/10.1007/978-3-031-72633-0_17",
    "https://dl.acm.org/doi/10.5555/3618408.3619275": "SNIPPET: In this paper, we introduce a novel dIffusion language modEl pre-training framework for text generation, which we call GENIE. GENIE is a large-scale pre-trained diffusion language model that consists of an encoder and a diffusion-based decoder, which can generate text by gradually transforming a random noise sequence into a coherent text sequence.\n\n[FetchError] HTTP 403 for https://dl.acm.org/doi/10.5555/3618408.3619275\n\nSOURCE: https://dl.acm.org/doi/10.5555/3618408.3619275",
    "https://scisimple.com/en/articles/2025-09-02-introducing-the-text-encoding-diffusion-model-for-text-generation--a37dy66": "SNIPPET: TEncDM improves text generation quality through unique encoding techniques and self-conditioning. In recent years, diffusion models have gained attention for...\n\nTITLE: Introducing the Text Encoding Diffusion Model for Text Generation\n\nBODY:\nIntroducing the Text Encoding Diffusion Model for Text Generation\nTEncDM improves text generation quality through unique encoding techniques and self-conditioning.\n― 6 min read\nTable of Contents\nIn recent years, Diffusion Models have gained attention for their ability to generate data in fields like images, audio, and video. However, adapting these models for text generation remains a challenge. In this article, we will discuss the features of diffusion models in relation to text data, introduce a new approach called the Text Encoding Diffusion Model (TEncDM), and explore its components, benefits, and performance compared to other methods.\nIntroduction to Text Generation\nText generation is the process of producing coherent and meaningful text from existing data. This can be done with or without certain conditions. Unconditional text generation refers to creating text without specific guidelines, while conditional text generation involves generating text based on certain criteria or prompts.\nTraditional approaches to text generation often rely on autoregressive (AR) models. These models generate text one token at a time, considering the previously generated tokens for context. While they produce high-quality outputs, they also have limitations. If an error is made early in the generation, it may affect the rest of the text. Additionally, the process can be slow, as each token requires evaluation through the model.\nThe Rise of Diffusion Models\nDiffusion models have emerged as a promising alternative, particularly in image generation. These models work by gradually transforming random noise into a structured output. This ability to handle data in this way makes them attractive for generating text as well.\nThe key feature of non-autoregressive (NAR) diffusion models is their capability to generate all tokens simultaneously. This means they can adjust any part of the text during the generation process, providing flexibility. Moreover, the speed of these models is often better than AR models since the evaluation time depends on the number of iterations rather than the length of the text.\nChallenges in Text Diffusion\nThe challenge with text diffusion models lies in the discrete nature of text. Unlike images or audio, text is made up of distinct tokens that do not translate well into a continuous space. Traditional methods have often used token embeddings, which map tokens into a continuous latent space. However, this approach can lose contextual information and leads to difficulties in generating meaningful text.\nSeveral models have been proposed to improve the text diffusion process, including those that introduce different noise types or alter how outputs are decoded back into text. Yet, no method has emerged as the standard solution.\nIntroducing the Text Encoding Diffusion Model\nTo address the challenges mentioned, TEncDM was developed. This new model trains in the latent space of language model encodings, such as those provided by models like BERT. By focusing on this encoding space instead of just token embeddings, TEncDM captures more contextual information, improving the generation quality.\nThe TEncDM framework consists of three main components: a diffusion encoder, a diffusion model, and a Decoder.\nDiffusion Encoder: This part takes the input text and converts it into encodings using a pre-trained language model. This helps retain the contextual information that traditional token embeddings might miss. Special token embeddings are also modified to enhance the model's performance.\nDiffusion Model: This component is central to the TEncDM framework. It consists of layers designed to reconstruct the original data from noisy inputs. It uses a specific noise schedule that is adapted for text, ensuring that the training process is challenging enough without being overly complex.\nDecoder: The decoder's role is to convert the generated latents back into coherent text. The design of the decoder is crucial, as it needs to handle the inaccuracies that might arise during the generation process effectively.\nSelf-conditioning in TEncDM\nOne interesting aspect of TEncDM is the incorporation of self-conditioning. This technique allows the model to use its previous outputs to improve the current predictions. By doing so, it increases the confidence of the predictions at each step, reducing the number of steps needed for generating high-quality text.\nThe Role of Noise Schedulers\nIn diffusion models, noise scheduling is essential as it determines how much noise is added during the generation process. For TEncDM, a unique noise schedule called the tan-d noise scheduler was created. This approach ensures that the model faces enough challenges during training without overwhelming it, resulting in better performance during inference.\nEvaluation of TEncDM\nTo assess the effectiveness of TEncDM, we conducted tests on datasets designed for different text generation tasks, including paraphrasing and summarization. The results indicated that TEncDM outperformed both traditional autoregressive models and existing non-autoregressive models consistently.\nWe used popular metrics such as Perplexity, diversity, and memorization rate to evaluate the model's quality. The findings confirmed TEncDM's superiority across all metrics compared to its counterparts.\nDetailed Analysis of Components\nImportance of Encodings\nUsing encodings rather than embeddings has proven to be a significant advantage. Encodings from pre-trained language models carry rich contextual information, leading to a more effective training process. As a result, TEncDM can produce more coherent and contextually relevant text compared to models relying solely on embeddings.\nDecoder Architecture\nThe architecture of the decoder also plays a critical role in the overall performance of TEncDM. By employing a transformer-based architecture, the decoder can effectively contextually analyze the generated latents, improving the final output's quality.\nEffect of Self-Conditioning\nSelf-conditioning has shown to have a positive influence on the model's performance. It improves the text quality and reduces the number of steps needed to generate satisfactory text. This finding is crucial, as it suggests that models can be made more efficient while maintaining output quality.\nNoise Scheduling Comparisons\nComparisons between different noise schedules highlighted the effectiveness of the tan-d noise scheduler. This innovative approach optimizes the training process, ensuring that all timesteps contribute effectively to the model's learning.\nConclusion\nThe development of the Text Encoding Diffusion Model marks an important step in the evolution of text generation techniques. By leveraging language model encodings and incorporating self-conditioning, TEncDM sets a new standard for non-autoregressive text generation.\nThe evaluation results demonstrate its potential across various tasks, achieving improved performance over traditional methods. As research continues in this area, TEncDM offers valuable insights for future developments in natural language processing, particularly in the realm of text generation.\nThrough ongoing exploration of noise scheduling, encoder-decoder architectures, and other components, we expect to see even greater advancements in the quality and efficiency of text generation models.\nTitle: TEncDM: Understanding the Properties of the Diffusion Model in the Space of Language Model Encodings\nAbstract: This paper presents the Text Encoding Diffusion Model (TEncDM), a novel approach to diffusion modeling that operates in the space of pre-trained language model encodings. In contrast to traditionally used embeddings, encodings integrate contextual information. In our approach, we also employ a transformer-based decoder, specifically designed to incorporate context in the token prediction process. We conduct a comprehensive examination of the influence of the encoder, decoder, noise scheduler, and self-conditioning on zero-shot generation. Furthermore, we compare TEncDM with previous approaches on three conditional text generation tasks: QQP, XSum, and Wiki-Auto. The results show that TEncDM exhibits superior performance compared to existing non-autoregressive diffusion models. Our code is available at https://github.com/M0RJIQUE/tencdm.\nAuthors: Alexander Shabalin, Viacheslav Meshchaninov, Egor Chimbulatov, Vladislav Lapikov, Roman Kim, Grigory Bartosh, Dmitry Molchanov, Sergey Markov, Dmitry Vetrov\nLast Update: 2024-12-18 00:00:00\nLanguage: English\nSource URL: https://arxiv.org/abs/2402.19097\nSource PDF: https://arxiv.org/pdf/2402.19097\nLicence: https://creativecommons.org/licenses/by/4.0/\nChanges: This summary was created with assistance from AI and may have inaccuracies. For accurate information, please refer to the original source documents linked here.\nThank you to arxiv for use of its open access interoperability.\n\nSOURCE: https://scisimple.com/en/articles/2025-09-02-introducing-the-text-encoding-diffusion-model-for-text-generation--a37dy66",
    "https://arxiv.org/abs/2212.11685": "SNIPPET: In this paper, we introduce a novel dIffusion language modEl pre-training framework for text generation, which we call GENIE. GENIE is a large-scale pretrained diffusion language model that consists of an encoder and a diffusion-based decoder, which can generate text by gradually transforming a random noise sequence into a coherent text sequence. To pre-train GENIE on a large-scale language ...\n\nTITLE: Text Generation with Diffusion Language Models: A Pre-training Approach with Continuous Paragraph Denoise\n\nBODY:\nComputer Science > Computation and Language\n[Submitted on 22 Dec 2022 (v1), last revised 17 Feb 2023 (this version, v2)]\nTitle:Text Generation with Diffusion Language Models: A Pre-training Approach with Continuous Paragraph Denoise\nView PDFAbstract:In this paper, we introduce a novel dIffusion language modEl pre-training framework for text generation, which we call GENIE. GENIE is a large-scale pretrained diffusion language model that consists of an encoder and a diffusion-based decoder, which can generate text by gradually transforming a random noise sequence into a coherent text sequence. To pre-train GENIE on a large-scale language corpus, we design a new continuous paragraph denoise objective, which encourages the diffusion-decoder to reconstruct a clean text paragraph from a corrupted version, while preserving the semantic and syntactic coherence. We evaluate GENIE on four downstream text generation benchmarks, namely XSum, CNN/DailyMail, Gigaword, and CommonGen. Our experimental results show that GENIE achieves comparable performance with the state-of-the-art autoregressive models on these benchmarks, and generates more diverse text samples. The code and models of GENIE are available at this https URL.\nSubmission history\nFrom: Zhenghao Lin [view email][v1] Thu, 22 Dec 2022 13:17:11 UTC (489 KB)\n[v2] Fri, 17 Feb 2023 17:14:52 UTC (624 KB)\nReferences & Citations\nexport BibTeX citation\nLoading...\nBibliographic and Citation Tools\nBibliographic Explorer (What is the Explorer?)\nConnected Papers (What is Connected Papers?)\nLitmaps (What is Litmaps?)\nscite Smart Citations (What are Smart Citations?)\nCode, Data and Media Associated with this Article\nalphaXiv (What is alphaXiv?)\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\nDagsHub (What is DagsHub?)\nGotit.pub (What is GotitPub?)\nHugging Face (What is Huggingface?)\nPapers with Code (What is Papers with Code?)\nScienceCast (What is ScienceCast?)\nDemos\nRecommenders and Search Tools\nInfluence Flower (What are Influence Flowers?)\nCORE Recommender (What is CORE?)\narXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.\n\nSOURCE: https://arxiv.org/abs/2212.11685",
    "https://arxiv.org/abs/2312.16476": "SNIPPET: Recently, text-guided scalable vector graphics (SVGs) synthesis has shown promise in domains such as iconography and sketch. However, existing text-to-SVG generation methods lack editability and struggle with visual quality and result diversity. To address these limitations, we propose a novel text-guided vector graphics synthesis method called SVGDreamer. SVGDreamer incorporates a semantic ...\n\nTITLE: SVGDreamer: Text Guided SVG Generation with Diffusion Model\n\nBODY:\nComputer Science > Computer Vision and Pattern Recognition\n[Submitted on 27 Dec 2023 (v1), last revised 17 Dec 2024 (this version, v6)]\nTitle:SVGDreamer: Text Guided SVG Generation with Diffusion Model\nView PDF HTML (experimental)Abstract:Recently, text-guided scalable vector graphics (SVGs) synthesis has shown promise in domains such as iconography and sketch. However, existing text-to-SVG generation methods lack editability and struggle with visual quality and result diversity. To address these limitations, we propose a novel text-guided vector graphics synthesis method called SVGDreamer. SVGDreamer incorporates a semantic-driven image vectorization (SIVE) process that enables the decomposition of synthesis into foreground objects and background, thereby enhancing editability. Specifically, the SIVE process introduces attention-based primitive control and an attention-mask loss function for effective control and manipulation of individual elements. Additionally, we propose a Vectorized Particle-based Score Distillation (VPSD) approach to address issues of shape over-smoothing, color over-saturation, limited diversity, and slow convergence of the existing text-to-SVG generation methods by modeling SVGs as distributions of control points and colors. Furthermore, VPSD leverages a reward model to re-weight vector particles, which improves aesthetic appeal and accelerates convergence. Extensive experiments are conducted to validate the effectiveness of SVGDreamer, demonstrating its superiority over baseline methods in terms of editability, visual quality, and diversity. Project page: this https URL\nSubmission history\nFrom: XiMing Xing [view email][v1] Wed, 27 Dec 2023 08:50:01 UTC (93,615 KB)\n[v2] Wed, 3 Jan 2024 14:40:49 UTC (93,617 KB)\n[v3] Sun, 17 Mar 2024 09:12:58 UTC (48,269 KB)\n[v4] Mon, 25 Mar 2024 11:24:45 UTC (45,667 KB)\n[v5] Tue, 2 Apr 2024 13:25:04 UTC (45,663 KB)\n[v6] Tue, 17 Dec 2024 12:55:57 UTC (47,243 KB)\nReferences & Citations\nexport BibTeX citation\nLoading...\nBibliographic and Citation Tools\nBibliographic Explorer (What is the Explorer?)\nConnected Papers (What is Connected Papers?)\nLitmaps (What is Litmaps?)\nscite Smart Citations (What are Smart Citations?)\nCode, Data and Media Associated with this Article\nalphaXiv (What is alphaXiv?)\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\nDagsHub (What is DagsHub?)\nGotit.pub (What is GotitPub?)\nHugging Face (What is Huggingface?)\nPapers with Code (What is Papers with Code?)\nScienceCast (What is ScienceCast?)\nDemos\nRecommenders and Search Tools\nInfluence Flower (What are Influence Flowers?)\nCORE Recommender (What is CORE?)\narXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.\n\nSOURCE: https://arxiv.org/abs/2312.16476",
    "https://arxiv.org/abs/2301.05221v1": "SNIPPET: The goal of this paper is to augment a pre-trained text-to-image diffusion model with the ability of open-vocabulary objects grounding, i.e., simultaneously generating images and segmentation masks for the corresponding visual entities described in the text prompt. We make the following contributions: (i) we insert a grounding module into the existing diffusion model, that can be trained to ...\n\nTITLE: Guiding Text-to-Image Diffusion Model Towards Grounded Generation\n\nBODY:\nComputer Science > Computer Vision and Pattern Recognition\n[Submitted on 12 Jan 2023 (this version), latest version 10 Aug 2023 (v2)]\nTitle:Guiding Text-to-Image Diffusion Model Towards Grounded Generation\nView PDFAbstract:The goal of this paper is to augment a pre-trained text-to-image diffusion model with the ability of open-vocabulary objects grounding, i.e., simultaneously generating images and segmentation masks for the corresponding visual entities described in the text prompt. We make the following contributions: (i) we insert a grounding module into the existing diffusion model, that can be trained to align the visual and textual embedding space of the diffusion model with only a small number of object categories; (ii) we propose an automatic pipeline for constructing a dataset, that consists of {image, segmentation mask, text prompt} triplets, to train the proposed grounding module; (iii) we evaluate the performance of open-vocabulary grounding on images generated from the text-to-image diffusion model and show that the module can well segment the objects of categories beyond seen ones at training time; (iv) we adopt the guided diffusion model to build a synthetic semantic segmentation dataset, and show that training a standard segmentation model on such dataset demonstrates competitive performance on zero-shot segmentation(ZS3) benchmark, which opens up new opportunities for adopting the powerful diffusion model for discriminative tasks.\nSubmission history\nFrom: Ziyi Li [view email][v1] Thu, 12 Jan 2023 18:59:08 UTC (44,585 KB)\n[v2] Thu, 10 Aug 2023 16:05:14 UTC (45,126 KB)\nReferences & Citations\nexport BibTeX citation\nLoading...\nBibliographic and Citation Tools\nBibliographic Explorer (What is the Explorer?)\nConnected Papers (What is Connected Papers?)\nLitmaps (What is Litmaps?)\nscite Smart Citations (What are Smart Citations?)\nCode, Data and Media Associated with this Article\nalphaXiv (What is alphaXiv?)\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\nDagsHub (What is DagsHub?)\nGotit.pub (What is GotitPub?)\nHugging Face (What is Huggingface?)\nPapers with Code (What is Papers with Code?)\nScienceCast (What is ScienceCast?)\nDemos\nRecommenders and Search Tools\nInfluence Flower (What are Influence Flowers?)\nCORE Recommender (What is CORE?)\narXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.\n\nSOURCE: https://arxiv.org/abs/2301.05221v1",
    "https://arxiv.org/abs/2405.12531": "SNIPPET: Textual image generation spans diverse fields like advertising, education, product packaging, social media, information visualization, and branding. Despite recent strides in language-guided image synthesis using diffusion models, current models excel in image generation but struggle with accurate text rendering and offer limited control over font attributes. In this paper, we aim to enhance ...\n\nTITLE: CustomText: Customized Textual Image Generation using Diffusion Models\n\nBODY:\nComputer Science > Computer Vision and Pattern Recognition\n[Submitted on 21 May 2024]\nTitle:CustomText: Customized Textual Image Generation using Diffusion Models\nView PDF HTML (experimental)Abstract:Textual image generation spans diverse fields like advertising, education, product packaging, social media, information visualization, and branding. Despite recent strides in language-guided image synthesis using diffusion models, current models excel in image generation but struggle with accurate text rendering and offer limited control over font attributes. In this paper, we aim to enhance the synthesis of high-quality images with precise text customization, thereby contributing to the advancement of image generation models. We call our proposed method CustomText. Our implementation leverages a pre-trained TextDiffuser model to enable control over font color, background, and types. Additionally, to address the challenge of accurately rendering small-sized fonts, we train the ControlNet model for a consistency decoder, significantly enhancing text-generation performance. We assess the performance of CustomText in comparison to previous methods of textual image generation on the publicly available CTW-1500 dataset and a self-curated dataset for small-text generation, showcasing superior results.\nSubmission history\nFrom: Shubham Paliwal [view email][v1] Tue, 21 May 2024 06:43:03 UTC (10,056 KB)\nReferences & Citations\nexport BibTeX citation\nLoading...\nBibliographic and Citation Tools\nBibliographic Explorer (What is the Explorer?)\nConnected Papers (What is Connected Papers?)\nLitmaps (What is Litmaps?)\nscite Smart Citations (What are Smart Citations?)\nCode, Data and Media Associated with this Article\nalphaXiv (What is alphaXiv?)\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\nDagsHub (What is DagsHub?)\nGotit.pub (What is GotitPub?)\nHugging Face (What is Huggingface?)\nPapers with Code (What is Papers with Code?)\nScienceCast (What is ScienceCast?)\nDemos\nRecommenders and Search Tools\nInfluence Flower (What are Influence Flowers?)\nCORE Recommender (What is CORE?)\narXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.\n\nSOURCE: https://arxiv.org/abs/2405.12531",
    "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_DesignDiffusion_High-Quality_Text-to-Design_Image_Generation_with_Diffusion_Models_CVPR_2025_paper.html": "SNIPPET: DesignDiffusion: High-Quality Text-to-Design Image Generation with Diffusion Models Zhendong Wang, Jianmin Bao, Shuyang Gu, Dong Chen, Wengang Zhou, Houqiang Li; Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2025, pp. 20906-20915\n\nTITLE: CVPR 2025 Open Access Repository\n\nBODY:\n-\n[pdf]\n[supp]\n[arXiv]\n[bibtex]@InProceedings{Wang_2025_CVPR, author = {Wang, Zhendong and Bao, Jianmin and Gu, Shuyang and Chen, Dong and Zhou, Wengang and Li, Houqiang}, title = {DesignDiffusion: High-Quality Text-to-Design Image Generation with Diffusion Models}, booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, month = {June}, year = {2025}, pages = {20906-20915} }\nDesignDiffusion: High-Quality Text-to-Design Image Generation with Diffusion Models\nAbstract\nIn this paper, we present DesignDiffusion, a simple yet effective framework for the novel task of synthesizing design images from textual descriptions. A primary challenge lies in generating accurate and style-consistent textual and visual content. Existing works in a related task of visual text generation often focus on generating text within given specific regions, which limits the creativity of generation models, resulting in style or color inconsistencies between textual and visual elements if applied to design image generation. To address this issue, we propose an end-to-end, one-stage diffusion-based framework that avoids intricate components like position and layout modeling. Specifically, the proposed framework directly synthesizes textual and visual design elements from user prompts. It utilizes a distinctive character embedding derived from the visual text to enhance the input prompt, along with a character localization loss for enhanced supervision during text generation. Furthermore, we employ a self-play Direct Preference Optimization fine-tuning strategy to improve the quality and accuracy of the synthesized visual text. Extensive experiments demonstrate that DesignDiffusion achieves state-of-the-art performance in design image generation.\nRelated Material\n\nSOURCE: https://openaccess.thecvf.com/content/CVPR2025/html/Wang_DesignDiffusion_High-Quality_Text-to-Design_Image_Generation_with_Diffusion_Models_CVPR_2025_paper.html",
    "https://peerj.com/articles/cs-1905.pdf": "SNIPPET: In this article, we conduct a comprehensive survey on the application of diffusion models in text generation. We divide text generation into three parts (conditional, unconstrained, and multi-mode text generation, respectively) and provide a detailed introduction.\n\n[FetchError] HTTP 400 for https://peerj.com/articles/cs-1905.pdf\n\nSOURCE: https://peerj.com/articles/cs-1905.pdf",
    "https://arxiv.org/pdf/2212.11685": "SNIPPET: In this paper, we introduce a novel dIffusion language modEl pre-training framework for text generation, which we call GENIE. GENIE is a large-scale pretrained diffusion language model that consists of an encoder and a diffusion-based decoder, which can generate text by gradually transforming a random noise sequence into a coherent text sequence.\n\nTITLE: (from PDF)\n\nBODY:\nText Generation with Diffusion Language Models: A Pre-training Approach\nwith Continuous Paragraph Denoise\n\nZhenghao Lin 1 2 Yeyun Gong 3 Yelong Shen 4 Tong Wu 5 2 Zhihao Fan 6 2\nChen Lin 1 Nan Duan 3 Weizhu Chen 4\n\n3\n2\n0\n2\n\nb\ne\nF\n7\n1\n\n]\nL\nC\n.\ns\nc\n[\n\n2\nv\n5\n8\n6\n1\n1\n.\n2\n1\n2\n2\n:\nv\ni\nX\nr\na\n\nAbstract\n\nIn this paper, we introduce a novel dIffusion\nlanguage modEl pre-training framework for text\ngeneration, which we call GENIE. GENIE is a\nlarge-scale pretrained diffusion language model\nthat consists of an encoder and a diffusion-based\ndecoder, which can generate text by gradually\ntransforming a random noise sequence into a\ncoherent text sequence. To pre-train GENIE\non a large-scale language corpus, we design a\nnew continuous paragraph denoise objective,\nwhich encourages the diffusion-decoder\nto\nreconstruct a clean text paragraph from a\ncorrupted version, while preserving the semantic\nand syntactic coherence. We evaluate GENIE\non four downstream text generation benchmarks,\nnamely XSUM, CNN/DAILYMAIL, GIGA-\nWORD, and COMMONGEN. Our experimental\nresults show that GENIE achieves comparable\nperformance with the state-of-the-art autore-\ngressive models on these benchmarks, and\ngenerates more diverse text samples.\nThe\ncode and models of GENIE are available\nhttps://github.com/microsoft/\nat\nProphetNet/tree/master/GENIE.\n\n1. Introduction\n\nText generation is a crucial task in natural language process-\ning, which aims to produce ﬂuent and coherent texts for var-\nious applications. Previous text generation methods mainly\nrelied on recurrent neural networks (RNNs) (Pawade et al.,\n2018; Song et al., 2018; Gu et al., 2016a; Qi et al., 2021),\nwhich generate texts sequentially from left to right. How-\never, RNNs suffer from issues such as long-term dependency\nand exposure bias. Recently, Transformer (Vaswani et al.,\n\n1Xiamen University 2This work was done during an internship\nin MSRA 3Microsoft Research Asia 4Microsoft 5Tsinghua Uni-\nversity 6Fudan University. Correspondence to: Chen Lin <chen-\nlin@xmu.edu.cn>.\n\n2017b), a self-attention based neural network, has emerged\nas the dominant paradigm for text generation, thanks to its\nability to capture global dependencies and leverage large-\nscale pre-trained language models (Qi et al., 2020; Lewis\net al., 2019; Raffel et al., 2020a). Transformer-based meth-\nods typically adopt an encoder-decoder architecture, where\nthe encoder maps the input text to a sequence of hidden\nvectors, and the decoder generates the output text either\nautoregressively (AR) or non-autoregressively (NAR). AR\ndecoding is more accurate but slower, as it predicts each\nword conditioned on the previous ones. NAR decoding is\nfaster but less precise, as it predicts all words simultaneously\nwithout modeling the dependencies among them.\n\nIn this paper, we present a new text generation approach,\ncalled GENIE, that integrates the diffusion model and\nTransformer-based method. The diffusion model is a genera-\ntive model that reverses a stochastic process of adding noise\nto the data, and has shown promising results in image (Ho\net al., 2020; Song et al., 2020), molecule (Hoogeboom et al.,\n2022), video (Ho et al., 2022), and text (Li et al., 2022b;\nGong et al., 2022; Strudel et al., 2022; Reid et al., 2022)\ngeneration. GENIE follows the encoder-decoder architec-\nture, where the encoder transforms the input text to hidden\nvectors, and the diffusion model restores the output text\nfrom a random Gaussian noise, guided by the encoder hid-\nden vectors. The diffusion model iterates over multiple time\nsteps, and gradually denoises the output text at each step.\n\nTo leverage the large-scale unlabeled text data, we also pro-\npose an end-to-end pre-training method for GENIE. Unlike\nthe existing pre-training tasks that involve masking or split-\nting tokens or texts (Qi et al., 2020; Lewis et al., 2019; Raffel\net al., 2020a), we design a novel pre-training task, called\ncontinuous paragraph denoise (CPD). CPD requires the\nmodel to predict the noise added to continuous paragraphs\nin the current time step, given the paragraph context and the\nnoisy paragraph information.\n\nWe evaluate GENIE on four popular text generation bench-\nmarks: XSum (Narayan et al., 2018), CNN/DailyMail (Her-\nmann et al., 2015), Gigaword (Rush et al., 2015), and Com-\nmonGen (Lin et al., 2019). The experimental results demon-\nstrate that GENIE achieves competitive performance with\n\n \n \n \n \n \n \n\fText Generation with Diffusion Language Models: A Pre-training Approach with Continuous Paragraph Denoise\n\nTransformer-based AR methods, and that the proposed pre-\ntraining method can effectively improve the performance.\nWe notice that GENIE has achieved signiﬁcant improve-\nments in diversity metric. To evaluate the multiple outputs\nof the generation model, we design an automatic annotation\nmethod based on large language model. We also conduct\nablation studies to analyze the impact of the diffusion steps\nand pre-training steps.\n\nThe main contributions of this work are summarized as\nfollows:\n\n• We propose GENIE, the ﬁrst large-scale language\npre-trained model based on the diffusion framework,\nwhich can generate high-quality texts for sequence-to-\nsequence tasks.\n\n• We introduce a novel CPD loss as the pre-training\nobjective, which can enhance the model’s ability to\ndenoise noisy texts and capture the paragraph-level\ncoherence.\n\n• We validate the effectiveness of the pre-trained diffu-\nsion model on downstream tasks, and design a new\nautomatic annotation method for the evaluation based\non large language model. We also provide extensive\nanalyses on the model’s behavior and properties.\n\n2. Preliminary\n\n2.1. Task Deﬁnition\n\nIn the classical sequence-to-sequence task, given a source\ntext s = {ws\n2, . . . , ws\n1, ws\nn} with n tokens, it generates tar-\nget text sequence y = {wy\n1 , wy\nn}. A sequence gen-\neration model can achieve this by modeling the conditional\nprobability: p (y | s).\n\n2 , . . . , wy\n\n2.2. Diffusion model\n\nIn the diffusion model, the diffusion process can be regarded\nas a discrete-time Markov process. The diffusion process\nstarts with initial state x0 at time step t = 0, where x0 is\nthe Gaussian distribution of the original data. It gradually\nadds Gaussian noises to x0 in the forward diffusion process\naccording to a variance schedule β1, ..., βT . At the time step\nt + 1, the latent variable xt+1 is only determined by the xt\nat time t, expressed as:\n\nq (xt+1 | xt) = N\n\n(cid:16)\n\nxt+1; (cid:112)1 − βt+1xt, βt+1I\n\n(cid:17)\n\n(1)\n\nAs t increases, xt becomes closer to standard Gaussian\nnoise N (xT ; 0, I).\n\nThe diffusion model learns to perform the inverse diffusion\nprocess during generation, which predicts the noise given\nthe current state xt at time step t. The previous state xt−1\n\ncan be reconstructed by subtracting the noise and rescaling\nthe mean. Thus, the distribution of xt−1 given xt is a\nGaussian with mean µt−1\n\nand variance σt−1\n\n2\n\nθ\n\n:\n(cid:1)\n\nθ\n, σt−1\nθ\n\np (xt−1 | xt) = N (cid:0)xt−1; µt−1\n\nθ\n\nµt−1\n\nθ =\n\n(cid:18)\n\nxt −\n\n1\n√\nαt\n\nβt√\n\n1 − ¯αt\n\n(cid:19)\n\nzθ (xt, t)\n\n2\n\nσt−1\nθ\n\n=\n\n1 − ¯αt−1\n1 − ¯αt\n\n· βt\n\n(2)\n\n(3)\n\n(4)\n\nwhere αt = 1 − βt, ¯αt = (cid:81)t\ni=1 αi and zθ is predicted by\na neural network parameterized by θ. The diffusion model\nis trained by minimizing the mean squared error between\nµt−1\nand the true mean ˆµt−1, which is computed from the\nθ\nreverse conditional distribution q(xt−1|xt, x0):\n\nq (xt−1 | xt, x0) = N\n√\n\nˆµt−1\n\nθ =\n\n¯αt−1βt\n1 − ¯αt\n\nx0 +\n\n(cid:16)\n\n(cid:17)\nxt−1; ˆµt−1, ˆβt−1I\n√\n\nαt (1 − ¯αt−1)\n1 − ¯αt\n\nxt\n\n(5)\n\n(6)\n\nFollowing the variational lower bound (VLB) approach (Ho\net al., 2020), the diffusion model can be trained by minimiz-\ning the loss function:\n\nLdiff =\n\nT\n(cid:88)\n\nt=1\n\nE\nq(xt|x0)\n\n(cid:13)\n(cid:13)µt−1\n\nθ − ˆµt−1\n\n(cid:13)\n2\n(cid:13)\n\n(7)\n\n3. Model\n\nGENIE is the proposed diffusion language model for pre-\ntraining, it adopts the sequence-to-sequence framework\nas illustrated in Figure 1. GENIE could generate a high-\nquality text sequence y given a source text s, such as pro-\nducing y : Messi’s performance from s : In the World\nCup 2022, [MASK] won people’s praise.. To achieve this,\nGENIE leverages two components: a bidirectional encoder\nmodel and a cross-attention diffusion model. The encoder\nmodel encodes the source text s into a set of hidden vec-\ntors Hs = Encoder(s), which indicates the distributed\nrepresentation of s. The diffusion model takes Hs and a\nGaussian noise as inputs, and iteratively reﬁnes the data by\napplying a sequence of denoising operations. In contrast\nto the traditional autoregressive text generation paradigm,\nwhich generates one token at a time, the diffusion model in\nGENIE outputs the sequence of embeddings in parallel at\neach denoising step, making GENIE a non-autoregressive\ngeneration (NAR) model.\n\nEncoder The encoder in GENIE is a 6-layer transformer\nmodel which takes the source text s as input with bidi-\nrectional self-attention. Speciﬁcally, given a source text\nsequence s = {ws\nn} with n tokens, the encoder\nmodel computes the vector hi for each token wi. Thus,\nthe source text s can be represented as Hs by the encoder\nmodel:\n\n2, . . . , ws\n\n1, ws\n\nHs = {h1, h2, ..., hn} = Encoder(s)\n\n(8)\n\n\fText Generation with Diffusion Language Models: A Pre-training Approach with Continuous Paragraph Denoise\n\nFigure 1. The framework of GENIE. We take the masked source sequence s as the input of the Encoder to obtain the hidden information\nHs, and interact with Language Diffusion Model through cross attention. Language Diffusion Model restores the randomly initial\nGaussian noise to the output text y through the iterative denoising and grounding process.\n\nLanguage Diffusion Model The diffusion model\nin\nGENIE is a 6-layer transformer with cross-attention on the\nsource text representation Hs. It learns to predict Gaussian\nnoise zθ (xt, t, Hs) conditioned on the current diffusion\nstep t and the state xt, where xt is the continuous latent\nrepresentation of the target text. We use an embedding func-\ntion and a clamping trick to ground the continuous state xt\nwith discrete target tokens, which will be elaborated in the\nfollowing section.\n\nInference Phase To generate text from the diffusion\nmodel, we start from the ﬁnal step t = T and sample a\nstate xT from a standard Gaussian distribution. Then we\niteratively generate the noise for the previous step using\nequations 3 and 4, and subtract it from the current state to\nobtain xt−1. After arriving at t = 0, we apply the clamp-\ning trick (Li et al., 2022b) to replace the values of x0 with\nits closest word embeddings, and then decode the discrete\ntokens from x0.\n\nTraining Phase To train the diffusion model for sequence-\nto-sequence tasks, we ﬁrst convert the target sequence\ny = {wy\nn} into a continuous state x0 using\nthe embedding function with a additional Gaussian noise\npermutation, which can be expressed as:\n\n2 , . . . , wy\n\n1 , wy\n\nq(x0|y) = N (x0; Emb(y), β0I)\n\n(9)\n\nat any step t as a function of x0, as shown in equation:\n\nq(xt|x0) = N (cid:0)xt;\n\n√\n\n¯αtx0,\n\n√\n\n1 − ¯αtI(cid:1)\n\n(10)\n\nwhere ¯αt = (cid:81)t\ni=1 αi. In the training phase, we sample\na random step t to calculate xt, and then use the denois-\ning architecture to predict the noise for that step, based\non the cross-attention with the source representation Hs.\nThe mean and variance of the predicted noise are given by\nequations 11:\n\nµt−1\n\nθ =\n\n(cid:18)\n\nxt −\n\n1\n√\nαt\n\nβt√\n\n1 − ¯αt\n\n(cid:19)\n\nzθ (xt, t, Hs)\n\n(11)\n\nwhere zθ is the output of the denoising architecture and θ\nare its parameters. The training objective is to minimize\nthe squared error between the predicted and true noise, as\nwell as the reconstruction error between x0 and the target\nembeddings, as expressed in equation 12:\n\nLs2s =\n\nE\n[\nq(x0:T |y)\n\nT\n(cid:88)\n\nt=1\n\n(cid:13)\n(cid:13)µt−1\n\nθ − ˆµt−1\n\n(cid:13)\n2\n(cid:13)\n\n(12)\n\n+ (cid:13)\n\n(cid:13)Emb(y) − µ0\nθ\n\n(cid:13)\n2\n(cid:13)\n\n− log pθ(y|x0)]\n\nwhere pθ(y|x0) = (cid:81)n\ni=1 pθ(wy\ni |x0), represents mapping\nthe continuous latent variable x0 into the discrete space\ntoken wy\ni .\n\nwhere Emb(·) is embedding function, β0 represents the\nscaling of variance at time step t = 0. Then we apply the\nforward diffusion process (equation 1) to obtain the state xt\n\n3.1. Pre-training GENIE\n\nDiffusion models have great potential for natural language\ngeneration (NLG) due to their ability to produce diverse\n\nEncoder𝑤(cid:2869)𝑤(cid:2870)𝑀𝑤(cid:2869)(cid:2868)𝑤(cid:2869)(cid:2869)𝑥(cid:3021)Initial Gaussian Noise𝑦(cid:2871)𝑦(cid:2872)𝑦(cid:2873)outputs𝑦(cid:2870)𝑦(cid:2869)𝑦(cid:2874)Language Diffusion ModelEncoder Inputsℎ(cid:2869)ℎ(cid:2870)ℎ(cid:2871)ℎ(cid:2872)ℎ(cid:2873)𝐻(cid:3046)Decoder/GroundingCross AttentionStatus 𝑥(cid:3047), time step 𝑡𝐻(cid:3046)Transformer blocks𝑥(cid:3047)(cid:2879)(cid:2869)denoise\fText Generation with Diffusion Language Models: A Pre-training Approach with Continuous Paragraph Denoise\n\noutputs. However, they have been largely overlooked in\nNLG because of their slow convergence and low quality\ncompared to autoregressive models. In this section, we ad-\ndress these challenges by pre-training a diffusion language\nmodel and introducing a novel pre-training task tailored for\nit. The novel pre-training task we propose is called con-\ntinuous paragraph denoise (CPD). CPD aims to train the\nmodel to predict the noise added to a continuous paragraph\nin the current diffusion step, given the paragraph and its\nsurrounding context.\n\n1, wd\n\n1, wp\n\n1 , wd(cid:48)\n\n2, . . . , wp\n\n2 , . . . , [MASK], . . . , wd(cid:48)\n\nSpeciﬁcally, given a document d = {wd\n2, . . . , wd\nl }\nwith l words, we randomly select a paragraph p =\n{wp\nm} from d, where m = (cid:98)γ ∗ l(cid:99) is\nthe paragraph length and γ is a predeﬁned ratio. We\nmask the paragraph in the document with a special to-\nken ([MASK]), and feed the masked document d(cid:48) =\n{wd(cid:48)\nl−m} to the GENIE en-\ncoder. We also apply the forward diffusion process to the\nparagraph p and obtain a noisy version xt at a random step\nt, and feed it to the GENIE denoising architecture. The\ndenoising architecture then uses the cross-attention with the\nsource representation Hs to predict the noise for the current\nstep, using equations 11. In summary, the pre-training ob-\njective of CPD is to minimize the same loss as in equation\n12, except that y is replaced by p and x0 is the embedded\nparagraph with noise.\n\nThrough this pre-training task, the diffusion model can en-\nhance its semantic understanding of the continuous text and\nits denoising ability at each diffusion step. Moreover, the\nCPD task is self-supervised and does not rely on external\nlabelled data sources, so it can fully exploit the information\nin the\n...[truncated]",
    "https://ieeexplore.ieee.org/document/10159911": "SNIPPET: Given the great success that diffusion models have achieved in generating various types of continuous data, including image, video and audio, there has been a growing interest in the application of these models to text generation. However, the discrete nature of text presents a challenge for diffusion models initially designed for application in a continuous feature space. The two main lines ...\n\nTITLE: An Overview of Diffusion Models for Text Generation\n\nBODY:\nAn Overview of Diffusion Models for Text Generation | IEEE Conference Publication | IEEE Xplore\n\nSOURCE: https://ieeexplore.ieee.org/document/10159911",
    "https://eccv.ecva.net/virtual/2024/poster/527": "SNIPPET: Abstract: The diffusion model has been proven a powerful generative model in recent years, yet it remains a challenge in generating visual text. Although existing work has endeavored to enhance the accuracy of text rendering, these methods still suffer from several drawbacks, such as (1) limited flexibility and automation, (2) constrained capability of layout prediction, and (3) restricted ...\n\nTITLE: Main Navigation\n\nBODY:\nPoster\nTextDiffuser-2: Unleashing the Power of Language Models for Text Rendering\nJingye Chen · Yupan Huang · Tengchao Lv · Lei Cui · Qifeng Chen · Furu Wei\nThe diffusion model has been proven a powerful generative model in recent years, yet it remains a challenge in generating visual text. Although existing work has endeavored to enhance the accuracy of text rendering, these methods still suffer from several drawbacks, such as (1) limited flexibility and automation, (2) constrained capability of layout prediction, and (3) restricted diversity. In this paper, we present TextDiffuser-2, aiming to unleash the power of language models for text rendering while taking these three aspects into account. Firstly, we fine-tune a large language model for layout planning. The large language model is capable of automatically generating keywords and placing the text in optimal positions for text rendering. Secondly, we utilize the language model within the diffusion model to encode the position and content of keywords at the line level. Unlike previous methods that employed tight character-level guidance, our approach generates more diverse text images. We conduct extensive experiments and incorporate user studies involving human participants and GPT-4V, validating TextDiffuser-2's capacity to achieve a more rational text layout and generation with enhanced diversity. Furthermore, the proposed methods are compatible with existing text rendering techniques, such as TextDiffuser and GlyphControl, serving to enhance automation and diversity, as well as augment the rendering accuracy. For instance, by using the proposed layout planner, TextDiffuser is capable of rendering text with more aesthetically pleasing line breaks and alignment, meanwhile obviating the need for explicit keyword specification. Furthermore, GlyphControl can leverage the layout planner to achieve diverse layouts without the necessity for user-specified glyph images, and the rendering F-measure can be boosted by 6.51\\% when using the proposed layout encoding training technique. The code and model will be available to promote future research.\n\nSOURCE: https://eccv.ecva.net/virtual/2024/poster/527",
    "https://iccv.thecvf.com/virtual/2025/poster/1917": "SNIPPET: Personalized generation in T2I diffusion models aims to naturally incorporate individual user preferences into the generation process with minimal user intervention. However, existing studies primarily rely on prompt-level modeling with large-scale models, often leading to inaccurate personalization due to the limited input token capacity of T2I diffusion models. To address these limitations ...\n\nTITLE: Main Navigation\n\nBODY:\nPoster\nDraw Your Mind: Personalized Generation via Condition-Level Modeling in Text-to-Image Diffusion Models\nHyungjin Kim · Seokho Ahn · Young-Duk Seo\nExhibit Hall I #1583\nPersonalized generation in T2I diffusion models aims to naturally incorporate individual user preferences into the generation process with minimal user intervention. However, existing studies primarily rely on prompt-level modeling with large-scale models, often leading to inaccurate personalization due to the limited input token capacity of T2I diffusion models. To address these limitations, we propose DrUM, a novel method that integrates user profiling with a transformer-based adapter to enable personalized generation through condition-level modeling in the latent space. DrUM demonstrates strong performance on large-scale datasets and seamlessly integrates with open-source text encoders, making it compatible with widely used foundation T2I models without requiring additional fine-tuning.\nLive content is unavailable. Log in and register to view live content\n\nSOURCE: https://iccv.thecvf.com/virtual/2025/poster/1917",
    "https://iccv.thecvf.com/virtual/2025/poster/1930": "SNIPPET: However, this task poses significant challenges, including the accurate modeling of complex style patterns—encompassing both intra- and inter-word relationships—and maintaining content accuracy across numerous characters. To address these challenges, we propose DiffBrush, a novel diffusion-based model for handwritten text-line generation.\n\nTITLE: Main Navigation\n\nBODY:\nPoster\nBeyond Isolated Words: Diffusion Brush for Handwritten Text-Line Generation\nGang Dai · Yifan Zhang · Yutao Qin · Qiangya Guo · Shuangping Huang · Shuicheng YAN\nExhibit Hall I #1765\nExisting handwritten text generation methods primarily focus on isolated words. However, realistic handwritten text demands attention not only to individual words but also to the relationships between them, such as vertical alignment and horizontal spacing. Therefore, generating entire text line emerges as a more promising and comprehensive task. However, this task poses significant challenges, including the accurate modeling of complex style patterns—encompassing both intra- and inter-word relationships—and maintaining content accuracy across numerous characters. To address these challenges, we propose DiffBrush, a novel diffusion-based model for handwritten text-line generation. Unlike existing methods, DiffBrush excels in both style imitation and content accuracy through two key strategies: (1) content-decoupled style learning, which disentangles style from content to better capture intra-word and inter-word style patterns by using column- and row-wise masking; and (2) multi-scale content learning, which employs line and word discriminators to ensure global coherence and local accuracy of textual content. Extensive experiments show that DiffBrush excels in generating high-quality text-lines, particularly in style reproduction and content preservation. Our source code will be made publicly available.\nLive content is unavailable. Log in and register to view live content\n\nSOURCE: https://iccv.thecvf.com/virtual/2025/poster/1930",
    "https://eccv.ecva.net/virtual/2024/poster/1027": "SNIPPET: The model is designed to learn the distribution of high-resolution images via two conditions: 1) the low-resolution image and 2) the character-level text embedding generated by a latent diffusion text model.\n\nTITLE: Main Navigation\n\nBODY:\nPoster\nDCDM: Diffusion-Conditioned-Diffusion Model for Scene Text Image Super-Resolution\nShrey Singh · Prateek Keserwani · Masakazu Iwamura · Partha Pratim Roy\nSevere blurring of scene text images, resulting in the loss of critical strokes and textual information, has a profound impact on text readability and recognizability. Therefore, scene text image super-resolution, aiming to enhance text resolution and legibility in low-resolution images, is a crucial task. In this paper, we introduce a novel generative model for scene text super-resolution called ``\\textit{Diffusion-Conditioned-Diffusion Model} (DCDM).'' The model is designed to learn the distribution of high-resolution images via two conditions: 1) the low-resolution image and 2) the character-level text embedding generated by a latent diffusion text model. The latent diffusion text module is specifically designed to generate character-level text embedding space from the latent space of low-resolution images. Additionally, the character-level CLIP module has been used to align the high-resolution character-level text embeddings with low-resolution embeddings. This ensures visual alignment with the semantics of scene text image characters. Our experiments on the TextZoom dataset demonstrate the superiority of the proposed method to state-of-the-art methods.\n\nSOURCE: https://eccv.ecva.net/virtual/2024/poster/1027",
    "https://arxiv.org/abs/2402.14314": "SNIPPET: Recent diffusion-based generative models show promise in their ability to generate text images, but limitations in specifying the styles of the generated texts render them insufficient in the realm of typographic design. This paper proposes a typographic text generation system to add and modify text on typographic designs while specifying font styles, colors, and text effects. The proposed ...\n\nTITLE: Typographic Text Generation with Off-the-Shelf Diffusion Model\n\nBODY:\nComputer Science > Computer Vision and Pattern Recognition\n[Submitted on 22 Feb 2024]\nTitle:Typographic Text Generation with Off-the-Shelf Diffusion Model\nView PDF HTML (experimental)Abstract:Recent diffusion-based generative models show promise in their ability to generate text images, but limitations in specifying the styles of the generated texts render them insufficient in the realm of typographic design. This paper proposes a typographic text generation system to add and modify text on typographic designs while specifying font styles, colors, and text effects. The proposed system is a novel combination of two off-the-shelf methods for diffusion models, ControlNet and Blended Latent Diffusion. The former functions to generate text images under the guidance of edge conditions specifying stroke contours. The latter blends latent noise in Latent Diffusion Models (LDM) to add typographic text naturally onto an existing background. We first show that given appropriate text edges, ControlNet can generate texts in specified fonts while incorporating effects described by prompts. We further introduce text edge manipulation as an intuitive and customizable way to produce texts with complex effects such as ``shadows'' and ``reflections''. Finally, with the proposed system, we successfully add and modify texts on a predefined background while preserving its overall coherence.\nReferences & Citations\nexport BibTeX citation\nLoading...\nBibliographic and Citation Tools\nBibliographic Explorer (What is the Explorer?)\nConnected Papers (What is Connected Papers?)\nLitmaps (What is Litmaps?)\nscite Smart Citations (What are Smart Citations?)\nCode, Data and Media Associated with this Article\nalphaXiv (What is alphaXiv?)\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\nDagsHub (What is DagsHub?)\nGotit.pub (What is GotitPub?)\nHugging Face (What is Huggingface?)\nPapers with Code (What is Papers with Code?)\nScienceCast (What is ScienceCast?)\nDemos\nRecommenders and Search Tools\nInfluence Flower (What are Influence Flowers?)\nCORE Recommender (What is CORE?)\narXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.\n\nSOURCE: https://arxiv.org/abs/2402.14314",
    "https://openaccess.thecvf.com/content/CVPR2025/papers/Wang_DesignDiffusion_High-Quality_Text-to-Design_Image_Generation_with_Diffusion_Models_CVPR_2025_paper.pdf": "SNIPPET: We propose DesignDiffusion, an end-to-end, diffusion-based framework for text-to-design image generation, which enables the simultaneous generation of image and visual text elements, eliminating the necessity for prede-fined text regions or traditional two-stage separated text and image creation process.\n\nTITLE: (from PDF)\n\nBODY:\nDesignDiffusion: High-Quality Text-to-Design Image Generation\nwith Diffusion Models\n\nZhendong Wang1, Jianmin Bao2,\n\n⇤\n\n,\n\n†, Shuyang Gu2, Dong Chen2, Wengang Zhou1,\n\n⇤, Houqiang Li1\n\n1 University of Science and Technology of China\n2 Microsoft Research Asia\n\n⇤corresponding author\n\n†project lead\n\nFigure 1. Images generated by our DesignDiffusion model, which only requires a simple text prompt as input and can generate diverse,\nhigh-quality design images with accurate textual and vivid visual content.\n\nAbstract\n\nIn this paper, we present DesignDiffusion, a simple yet ef-\nfective framework for the novel task of synthesizing design\nimages from textual descriptions. A primary challenge lies\nin generating accurate and style-consistent textual and vi-\nsual content. Existing works in a related task of visual text\ngeneration often focus on generating text within given spe-\nciﬁc regions, which limits the creativity of generation models,\nresulting in style or color inconsistencies between textual\nand visual elements if applied to design image generation.\nTo address this issue, we propose an end-to-end, one-stage\ndiffusion-based framework that avoids intricate components\nlike position and layout modeling. Speciﬁcally, the proposed\nframework directly synthesizes textual and visual design\nelements from user prompts. It utilizes a distinctive charac-\nter embedding derived from the visual text to enhance the\ninput prompt, along with a character localization loss for\nenhanced supervision during text generation. Furthermore,\nwe employ a self-play Direct Preference Optimization ﬁne-\n\ntuning strategy to improve the quality and accuracy of the\nsynthesized visual text. Extensive experiments demonstrate\nthat DesignDiffusion achieves state-of-the-art performance\nin design image generation.\n\n1. Introduction\n\nDesign images play a pivotal role in various applications\nsuch as graphic design, advertising, and scientiﬁc visualiza-\ntion, where the integration of visual and textual elements\nmust be ﬂawless.\n\nRecently many fascinating and wonderful applications,\nsuch as art image creation or ﬁlm production, have shown\nremarkable progress due to the rapid advance of text-to-\nimage generation techniques [37–40, 47]. Despite these ad-\nvancements, the precise generation of design images poses a\ndistinct challenge, largely due to the limited capabilities of\nexisting image generation models in complex layout plan-\nning and visual text generation.\n\nTo address the critical issue of visual text generation, re-\n\nThisCVPRpaperistheOpenAccessversion,providedbytheComputerVisionFoundation.Exceptforthiswatermark,itisidenticaltotheacceptedversion;thefinalpublishedversionoftheproceedingsisavailableonIEEEXplore.20906\fcent studies [27, 41] have made signiﬁcant contributions. A\ncommon approach involves a two-step process: ﬁrst gen-\nerating an image from textual input and then identifying a\nsuitable location within the image to insert text. However,\nthis method presents several challenges. Firstly, a generated\nimage may not have a suitable region for adding text, re-\nsulting in an overlay of text on visual elements. Secondly,\neven with a plausible location, the rendered text may lack\nconsistency with the visual elements presented in the image,\ncompromising overall coherence and professionalism.\n\nAn alternative research direction in visual text rendering,\nas suggested by [5, 6], focuses on learning the layout of\ntext regions from textual prompts in advance. Then, this\nlearned layout serves as a condition to guide text-to-image\nmodels in generating text within designated regions of the\nimage. However, the dependence on predeﬁned text regions\nmight constrain the creative breadth of the generated images.\nMoreover, these predetermined text regions may not always\nadhere to natural design principles, potentially leading to\nvisually unattractive or conceptually incoherent outputs.\n\nTo address the aforementioned issues, we propose Design-\nDiffusion, an end-to-end diffusion-based framework specif-\nically developed for the novel task of generating design\nimages from text. The holistic learning approach presents\nmultiple beneﬁts: (1) it eliminates the need to predetermine\ntext regions in the generative images, thereby preserving the\ncreative freedom of the generative models; (2) it enables the\nsmooth incorporation of generated text into images, yielding\ncohesive designs that accurately reﬂect input prompts.\n\nIn our framework, we employ three essential techniques\nwhen ﬁne-tuning text-to-image diffusion models: a) Recog-\nnizing that a conventional CLIP text encoder might overlook\nthe speciﬁc textual content designated for rendering in the\ngenerated image, we propose a character-level decomposi-\ntion of words. This approach provides the CLIP text encoder\nhints to render each character in the image precisely. b)\nTo encourage the model to pay more attention to newly\nintroduced character embeddings, we propose a character\nlocalization loss. This loss function compels each character\nembedding to concentrate on its respective area within the\nimage, ensuring an accurate visual representation of each\ncharacter. c) We introduce a new training strategy called\nSelf-Play Direct Preference Optimization (SP-DPO) for ﬁne-\ntuning the model to enhance its capability of generating\naccurate and high-ﬁdelity text. SP-DPO is based on the\nhypothesis that ground truth data are typically preferred by\nhumans over model-generated samples. Based on this, we\nﬁne-tune the model to align with these human preferences.\nExtensive experiments demonstrate the superior capabil-\nities of DesignDiffusion for design image generation. It\ngenerates creative design images with prompts, as shown\nin Fig. 1. Quantitative and qualitative analysis shows that\nour approach signiﬁcantly surpasses current state-of-the-art\n\ntext-to-image and visual text rendering models in terms of\nboth image quality and text accuracy.\n\nIn summary, our contributions are presented as follows:\n• We propose DesignDiffusion, an end-to-end, diffusion-\nbased framework for text-to-design image generation,\nwhich enables the simultaneous generation of image and\nvisual text elements, eliminating the necessity for prede-\nﬁned text regions or traditional two-stage separated text\nand image creation process.\n\n• We explore prompt enhancement for CLIP text encoding\nalong with a character localization loss, to improve the ac-\ncuracy of text placement and the ﬁdelity of text rendering.\n• By adopting an SP-DPO mechanism, the generative model\nis further ﬁne-tuned, achieving remarkable improvements\nin visual text accuracy and overall image quality.\n\n2. Related Work\n\nVisual Text Rendering. Despite the remarkable success of\nexisting diffusion models [37, 38, 40] in text-to-image gen-\neration, visual text rendering remains a challenging task. Re-\ncently, researchers have recognized this issue and proposed\nseveral approaches to address it, such as GlyphDraw [29],\nDiffUTE [4], GlyphControl [45], AnyText [41], TextDiffuser\nseries [5, 6], and [26]. While these efforts have improved\ntext rendering accuracy, it is worth noting that these models\nobtain limited image quality due to the need of pre-giving\ntext rendering areas during the generation process or inpaint-\ning text into a well-generated image. Separating text and\nvisual content generation away seriously limits the overall\nquality of generated images with visual text rendered, which\nis incompatible with the goal of design image generation.\nCompositional Image Generation. To improve the ability\nof text-to-image models to create complex compositions,\nresearchers have integrated new components into diffusion\nmodels, enabling the generation of images with multiple\nfeatures. Examples include T2I-Adapter [30] and Control-\nNet [47], which incorporate semantic details like layout and\npose to inﬂuence the structure of images [48]. In a similar\nvein, GLIGEN [21] utilizes adapters with diffusion mod-\nels to achieve image generation that is conditioned on spe-\nciﬁc spatial arrangements. However, these methods cannot\ngenerate images directly from prompts and require substan-\ntial human effort to deﬁne the image layout. On the other\nhand, some studies have investigated methods that do not\nrequire additional training to prompt diffusion models to\ncreate images based on spatial conditions during the infer-\nence phase. Recent advancements [22, 32, 44] have seen\nlarge language models aiding diffusion models in generating\nimages with intricate or predeﬁned layouts. Yet, these ap-\nproaches often fall short in generating design-centric images\nthat seamlessly integrate both textual and visual components\nin a well-organized manner.\nAligning Diffusion Models. Replicating the success of re-\n\n20907\fCharacter Localization Loss\n\n!\n\nDiffusion\nModel\n\n!\n\nText \nEncoder\n\ns\ns\no\nL\nO\nP\nD\n\nWin data\n\nLose data\n\nImage Captioning\n\nText prompt: “Mardi Gras Carnival” \nwritten in colorful, stylized letters with a \nfestive background featuring starbursts \nand leaves.\n\nt\nn\ne\nm\ne\nc\nn\na\nh\nn\nE\n\nt\np\nm\no\nr\nP\n\n(a) Fine-tuning with prompt enhancement and character localization loss\n\n(b) Fine-tuning with self-play DPO loss\n\nFigure 2. Overview of the DesignDiffusion framework. DesignDiffusion is based on enhanced text prompts, with trainable CLIP text\nencoder and UNet, and does not require additional complex conditions (glyphs, positions). Character localization loss is added as extra\nsupervision at cross-attention maps to force the UNet to attend more to the visual character regions. To further improve the quality of visual\ntext generation, we incorporate a self-play DPO strategy into the ﬁne-tuning process. Diffusion denoise loss is omitted here.\n\ninforcement learning in large language models [31, 35] to\ntext-to-image diffusion models remains a signiﬁcant chal-\nlenge. Recent works have proposed various methods to\naddress this problem. For example, EMU [7] identiﬁes that\na few high-quality samples are crucial when ﬁne-tuning the\nmodel to generate visually appealing images. DPOK [12]\nand DDPO [3] use RL-based approaches to maximize re-\nwards on a limited vocabulary set, performing well when\nthe number of train/test prompts is small. Another work Dif-\nfusionDPO [42] applies DPO based on a human-annotated\npreference dataset. Our DesignDiffusion differs by applying\na self-play DPO strategy, which does not require human an-\nnotation and lacks of exploration in design image generation.\n\n3. DesignDiffusion\n\nThe generative community has witnessed the strong gen-\nerative capability of diffusion models [9, 16, 34, 38, 48].\nHowever, generating design images remains a great chal-\nlenge, particularly in the context of exquisite layout and text\nrendering. Previous approaches [5, 6] in a related task (vi-\nsual text rendering) attempt to inpaint text after generating\nan image, or to plan text regions using large language models\nsuch as Vicuna [49] and GPT [1] before generating an image.\nWe argue that they do not consider the integration of text\nand image content. Note that, the text and image contexts in\ndesign images are often intertwined and inseparable, such\n\nthat they are incompatible with design image generation.\n\nIn contrast, we introduce a straightforward, one-stage ap-\nproach for text-to-design image generation, relying solely on\nthe user prompt. It is based on an aesthetic model, i.e., Stable\nDiffusion XL (SDXL) model [34], incorporating three core\ncomponents that enable the successful generation of text-rich\ndesign images. An overview of the DesignDiffusion frame-\nwork is provided in Fig. 2. The DesignDiffusion training\ninvolves a two-stage ﬁne-tuning strategy. In the ﬁrst stage,\nwe introduce prompt enhancement and character localization\nloss into the framework to generate high-quality images with\naccurate text. In the second stage, we propose SP-DPO to\nfurther improve the text rendering accuracy. These compo-\nnents are detailed in the following subsections.\n\n3.1. Prompt Enhancement for CLIP Text Encoding\n\nCurrently, available text-to-image diffusion models struggle\nto generate text-rich design images based solely on an input\ntext prompt. This challenge arises from two issues: 1) Visu-\nally rendered text must be letter-by-letter correct, however,\nduring tokenization, most words are tokenized as a single\ntoken, ignoring each letter within the word thus complicating\nthe learning of individual letters; 2) Visual text in images\nis often diverse in style, size, and layout, increasing the\ncomplexity of visual text modeling.\n\nTo address the ﬁrst issue, we introduce a novel approach\n\n20908 \n \n\fAlgorithm 1 Self-play DPO ﬁne-tuning, given a base diffusion model ✏ref, a trainable copy ✏✓, a prompt-image paired\ndesign-image dataset D =\n\nN\ni=1, in which xi and yi represent image and prompt, respectively.\n\nxi, yi}\n{\n,N } do\n\nfor all i in {1,\n\n· · ·\nGenerate K candidate samples\nLosing data: xl\nWinning data: xw\n\ni = xi\nn\n\nUpdate ✏✓ by: min\n✏✓\ni=0\n✏ref(xw\ni,t, yi, ti)\nP\n\n(log\u0000(\n\n\u0000\n\n✏w\ni \u0000\nk\nend for\nreturn ✏✓\n\ni = gik, in which gik gets the worst text rendering accuracy\n\ngik}\n{\n\nK\nk=1 using ✏ref with input yi\n\n\u0000T! (\u0000t))(\n\n✏✓(xw\n\n✏w\ni \u0000\nk\n✏✓(xl\ni,t, yi, ti)\n\ni,t, yi, ti)\n\n2\n2 \u0000 k\n\nk\n\n✏l\ni \u0000\n\n2\n2\u0000\nk\n✏ref(xl\n\n2\n2 \u0000\n\n(\n\n✏l\ni \u0000\nk\n\nk\n\ni,t, yi, ti)\n\n2\n2)))\nk\n\nby embedding each letter from prompts directly into CLIP\ntext encoders within latent diffusion models [34, 38]. Specif-\nically, the input prompt of the CLIP text encoders is en-\nhanced by appending an extra text description to the original\nprompt. The template for the text description is “Rendered\ncharacters: <|startofchar|>\n<|endofchar|>\". For exam-\n· · ·\nple, as illustrated in Fig. 2, if the desired rendered text is\n“Mardi Gras Carnival”, the appended text description would\nbe “Rendered characters:<|startofchar|> <|M|> <|a|> <|r|>\n<|d|> <|i|><|endofchar|><|startofchar|> <|G|> <|r|> <|a|> <|s|>\n<|endofchar|> <|startofchar|> <|C|> <|a|> <|r|> <|n|> <|i|>\n<|v|> <|a|> <|l|><|endofchar|>”. A character surrounded by\n‘|’ denotes a newly introduced trainable token.\n\nWe introduce a total of 97 new tokens, encompassing\n26 uppercase letters, 26 lowercase letters, 10 numbers, 32\npunctuation marks, a space, a start ﬂag, and an end ﬂag. The\nenhanced prompt enables the CLIP text encoder to be trained\nspeciﬁcally to learn hints for rendering text. By augmenting\nthe prompt with detailed rendered text information, the UNet\nlearns to generate images that include the speciﬁed visual\ntext content seamlessly integrated into the image, without\nrelying on predeﬁned text glyphs or positions. This approach\naddresses the challenge of integrating text and image content\nin a tightly coupled manner, distinguishing it from previous\napproach\n...[truncated]",
    "https://openaccess.thecvf.com/content/ICCV2025/papers/Yin_The_Best_of_Both_Worlds_Integrating_Language_Models_and_Diffusion_ICCV_2025_paper.pdf": "SNIPPET: Text-to-video (T2V) [6,30,55,60] has made significant progress in recent years, becoming an important research direction in the fields of computer vision and artificial in- telligence. Recent works in T2V models have primarily revolved around two predominant paradigms: autoregres- sive large language model (LLM)-based [30,55] frame- works and diffusion-based architectures [6,60]. However, each ...\n\nTITLE: (from PDF)\n\nBODY:\nThe Best of Both Worlds: Integrating Language Models and Diffusion Models\nfor Video Generation\n\nAoxiong Yin1*, Xu Tan2* †, Kai Shen2*, Yichong Leng2 , Xinyu Zhou2 , Juncheng Li1†, Siliang Tang1\n1Zhejiang University ,2Moonshot AI\nyinaoxiong@zju.edu.cn, tanxu2012@gmail.com, junchengli@zju.edu.cn\n\nAbstract\n\nRecent advancements in text-to-video (T2V) generation\nhave been driven by two competing paradigms: autore-\ngressive language models and diffusion models. However,\neach paradigm has intrinsic limitations: language models\nstruggle with visual quality and error accumulation, while\ndiffusion models lack semantic understanding and causal\nIn this work, we propose LanDiff, a hybrid\nmodeling.\nframework that synergizes the strengths of both paradigms\nthrough coarse-to-fine generation. Our architecture in-\ntroduces three key innovations: (1) a semantic tokenizer\nthat compresses 3D visual features into compact 1D dis-\ncrete representations through efficient semantic compres-\nsion, achieving a ∼14,000× compression ratio; (2) a lan-\nguage model that generates semantic tokens with high-level\nsemantic relationships; (3) a streaming diffusion model that\nrefines coarse semantics into high-fidelity videos. Experi-\nments show that LanDiff, a 5B model, achieves a score of\n85.43 on the VBench T2V benchmark, surpassing the state-\nof-the-art open-source models Hunyuan Video (13B) and\nother commercial models such as Sora, Kling, and Hailuo.\nFurthermore, our model also achieves state-of-the-art per-\nformance in long video generation, surpassing other open-\nsource models in this field. Our demo can be viewed at\nhttps://landiff.github.io/.\n\n1. Introduction\n\nText-to-video (T2V) [6, 30, 55, 60] has made significant\nprogress in recent years, becoming an important research\ndirection in the fields of computer vision and artificial in-\ntelligence. Recent works in T2V models have primarily\nrevolved around two predominant paradigms: autoregres-\nsive large language model (LLM)-based [30, 55] frame-\nworks and diffusion-based architectures [6, 60]. However,\neach paradigm has their own advantages and limitations, as\n\n*Equal contribution.\n†Corresponding author.\n\nFigure 1. The rate-distortion curve illustrates how visual distortion\ndecreases as the number of transmitted bits increases. With just a\nsmall number of bits representing high-level semantic features, we\ncan already achieve relatively low visual distortion. Building on\nthis information-theoretic insight, LanDiff combines the strengths\nof both paradigms: LLMs efficiently generate compact semantic\nfeatures in the first stage, followed by diffusion models that add\nperceptual details in the second stage, before final decoding to pix-\nels via VAE. Data from Ho et al. [20], illustration is conceptual.\n\nshown in Table 1.\n\nFrom a representation perspective, LLM-based meth-\nods leverage discrete tokenization to explicitly encode high-\nlevel semantics through vector quantization, effectively pri-\noritizing conceptual abstraction and narrative coherence.\nHowever, this discretization inherently sacrifices low-level\nvisual fidelity due to information compression, resulting in\nlow reconstruction quality. In contrast, diffusion-based ap-\nproaches employ continuous latent representations to pre-\nserve much more perceptual details, enabling superior re-\nconstruction quality at the cost of diluted semantic inter-\npretability, as hierarchical features remain entangled in the\nlatent space. From a generative modeling perspective,\nLLM-based systems adopt autoregressive modeling to en-\nforce causal dependencies between video frames, ensuring\nstrong temporal coherence. However, the autoregressive\ngeneration inherently risks error propagation across time\nsteps, where inaccuracies in early predictions amplify dur-\n\nPerceptual  FeatureVideoSemantic FeatureLittle boy blowing outbirthday candlesOur Video Generation ProcessLow RateHigh Ratebits/dimHigh DistortionLow DistortionRMSELLMDiffusionVAEThisICCVpaperistheOpenAccessversion,providedbytheComputerVisionFoundation.Exceptforthiswatermark,itisidenticaltotheacceptedversion;thefinalpublishedversionoftheproceedingsisavailableonIEEEXplore.15604\fTable 1. The comparison between LLM based and diffusion based\nT2V systems. The advantages and disadvantages are marked by\n\nand\n\n, respectively.\n\nTable 2. The comparison between LanDiff and previous large-\nscale T2V systems. The compression rates of LLM-based and\ndiffusion-based models are illustrated using VideoPoet [30] and\nCogVideoX [60] as examples, respectively.\n\nMethods\n\nRepresentations\n\nLLM\n\nDiffusion\n\nDiscrete Tokens\nLow Reconstruction Quality\nHighlight Semantic Information\n\nContinuous Vectors\nHigh Reconstruction Quality\nLack Semantic Information\n\nModeling\n\nAutoregressvie\nNo Refinement\nCausal Modeling\n\nNon-Autoregressvie\nProgressive Refinement\nNon-causal Modeling\n\ning decoding. In contrast, diffusion-based methods employ\nnon-autoregressive generation, refining outputs in parallel\nthrough iterative denoising steps. Although this design mit-\nigates error accumulation and enhances generation flexibil-\nity, the absence of explicit causal constraints often leads to\ntemporal inconsistencies or semantic hallucinations.\n\nIn this work, we propose a hybrid architecture that syner-\ngizes the strengths of both Language models and Diffusion\nmodels, named LanDiff, through a coarse-to-fine generation\nparadigm. As shown in Figure 1, inspired by the human\ncreation of video which will generate the high-level story-\nline first and then add low-level visual details based on the\nstoryline to form the video, we design a two-stage video\ngeneration process with the number of bits gradually in-\ncreasing and carefully design the autoregressive model and\nthe diffusion model to be responsible for different stages of\nT2V generation, so as to play to their strengths and avoid\ntheir weaknesses. In detail, 1) at the low-bit position “se-\nmantic feature”, the low-bit information ensures that the to-\nken sequence is not too long, and the high-level information\nmakes it easier for the model to capture the overall seman-\ntic entity motion of the video, so as to play to the strengths\nof the autoregressive model and avoid its weaknesses. Thus\nwe use LLM to generate a coarse-grained video in the first\nstage; 2) at the high-bit position “perceptual feature”, since\nwe have already obtained the coarse-grained with rich se-\nmantic and time-serial information, we only need to focus\non how to add details to the coarse-grained video. Thus\nwe apply a diffusion model in the second stage. Finally, a\nVAE decoder transforms the generated “perceptual feature”\ninto the final RGB video output. By unifying these com-\nplementary mechanisms, we demonstrate that hybrid archi-\ntectures can overcome the inherent limitations of isolated\napproaches, enabling coherent, semantically faithful, and\nvisually compelling video generation from textual descrip-\ntions, as shown in Table 2.\n\nWith this design, the ideal semantic feature should con-\ntain high-level semantic information and motion informa-\ntion and only require a few bits to represent. We achieve\nthis goal by performing extreme compression on video rep-\nresentations rich in high-level semantics. For video repre-\nsentation, we select the Theia model [46] as our visual rep-\n\nLLM Diffusion LanDiff\n\nModels\nHighlight Semantic Information? ✓\n✗\nProgressive Refinement?\n✓\nCausal Modeling?\n✗\nHigh Visual Quality?\n\n✗\n✓\n✗\n✓\n\n✓\n✓\n✓\n✓\n\nCompression Ratio ↑\nLong video Generation?\n\n∼256 ∼1024\n✗\n\n✗\n\n∼14000\n✓\n\nresentation backbone, which has been distilled from multi-\nple visual understanding and self-supervised representation\nmodels, ensuring the encoded features contain rich semantic\ninformation. To achieve extreme compression and reduce\nthe number of bits, we design an efficient tokenizer to com-\npress 3D visual features into 1D discrete representations.\nThe tokenizer is based on the Transformer [54] structure,\nuses query embedding to aggregate visual features, and has\na higher compression rate than CNN-based structures [63].\nTo further compress the video by fully utilizing the tempo-\nral redundancy of the video, inspired by the MP4 video en-\ncoding algorithm [32], we divide the video into keyframes\nand non-keyframes, and set more numbers of tokens for\nkeyframes. The detailed design is shown in subsection 3.1.\nFor the diffusion model, we use generated semantic tokens\nas conditions and generate the target video by gradually re-\nmoving the noises. To better support the long-video gen-\neration, we train a chunk-wise streaming diffusion model\nthat only uses a limited number of historical frames as con-\nditions, thereby greatly reducing the computational cost of\ntraining and inference.\n\nThanks to these designs, our LanDiff has made signifi-\ncant progress in spatial relationship compliance, action co-\nherence, visual quality, etc. Specifically, 1) for short video\ngeneration, our 5B model achieved a score of 85.43 on\nthe VBench T2V benchmark, surpassing the state-of-the-art\nopen-source models Hunyuan Video (13B) and other com-\nmercial models such as Sora, Kling, and Hailuo; 2) for long\nvideo generation, after testing by the VBench T2V bench-\nmark, our model also achieved state-of-the-art performance,\nsurpassing other open-source models in this field. Our\nvideo examples can be viewed at https://landiff.\ngithub.io/.\n\n2. Related Work\n\nVideo Tokenization. Video tokenization plays a crucial\nrole in video understanding and generation tasks. Since\na video can be represented as a sequence of continuous\nframes, some works directly use image tokenizers to pro-\ncess videos frame by frame. For example, Wang et al.\n[55] directly use SBER-MoVQGAN as the video tokenizer.\n\n15605\fHowever, this method ignores the temporal redundancy in\nvideos, resulting in a low compression rate. To reduce tem-\nporal redundancy, some works [58, 62] try to extend im-\nage tokenizers based on 2D convolution to 3D convolution,\nwhich can process both temporal and spatial information si-\nmultaneously. These methods encode videos in the original\nRGB space and perform video reconstruction tasks, which\nare more about perceptual compression. In addition, some\nworks [15, 25] try to train video tokenizers on features ex-\ntracted from pre-trained visual encoders. These tokenizers\ncan achieve good performance in understanding and gener-\nation tasks while maintaining a high compression rate. The\nvideo tokenizer we propose belongs to this category. Un-\nlike previous feature-based tokenizers that achieved limited\ncompression rates and primarily focused on image process-\ning, our video tokenizer delivers significantly higher com-\npression while handling both images and videos in a unified\nframework.\n\nLLM based Video Generation. LLM-based video gen-\neration methods are usually based on the Transformer [54]\nstructure, learning the mapping from text to video through\nnext token prediction. TATS [14] uses VQ-GAN as the to-\nkenizer and predicts video tokens using a GPT-like model\nstructure. VideoGPT [58] uses 3D convolution to extract\nfeatures and quantize them, and predicts the quantized\nvideo discrete tokens using a GPT-like model. Recently,\nVideoPoet [62] uses MagViT2 [62] as the tokenizer and uni-\nfies multiple modalities as input to a large language model\n(LLM) to conditionally generate video tokens. In addition,\nEmu3 [55] uses SBER-MoVQGAN as the tokenizer to per-\nform video understanding and generation by predicting the\nnext token. These works all use LLMs to directly generate\n“perceptual features” that contain rich visual details with\nhigh bit rates. Recently, ARLON [33] attempts to discretize\nVAE features into a small number of tokens to reduce the bit\nrate required for LLMs’ prediction. In this way, the tokens\nretain low-frequency visual information such as blurry con-\ntours rather than high-level semantic information. In con-\ntrast, our method employs tokens containing high-level se-\nmantic information as prediction targets for LLMs, which\nenables us to fully leverage LLMs’ advantages in causal\nmodeling to precisely generate high-quality videos.\n\nDiffusion based Video Generation. Diffusion-based\nmethods have achieved great success in image generation,\nand recently many people [7, 21, 22, 49, 69] have tried\nto apply them to video generation tasks. VDM [22] ex-\ntends the 3D U-Net structure for video generation. Wang\net al. [56] propose to generate high-quality and aesthetically\npleasing videos in a cascaded manner. Benefiting from the\nsuccess of the text-to-image (T2I) field, some works such\nas Animatediff [16], SVD [6], and PixelDance [65] try to\nuse pre-trained T2I models as initialization, and then add\nmodules for temporal modeling to capture motion informa-\n\ntion for video generation. Ma et al. [39] explore the gen-\neration capabilities of multiple different structures of latent\ndiffusion transformer. After the release of SORA, a series\nof video generation methods based on the DiT [41] model\nhave been proposed, including OpenSora [68], OpenSora-\nPlan [34], Cogvideox [60], Hunyuan Video [31], Mira [27]\nand STIV [35] etc. These methods can only generate short\nvideos of a few seconds. Recently, StreamingT2V [18] gen-\nerates long videos by block-wise generation on a pre-trained\nshort video generation model, and then uniformly performs\nmixed augmentation. In addition, some works improve the\nconsistency of long video generation by leveraging noise\nrescheduling techniques [37, 43]. Our method employs dif-\nfusion models as renderers for semantic features, enabling\nus to leverage their superior visual generation quality while\ncircumventing their limitations in causal modeling.\n\n3. Method\n\nIn this work, we propose a novel text-to-video generation\nframework that synergistically integrates the strengths of\nautoregressive modeling and diffusion processes while cir-\ncumventing their respective limitations. The framework\nmainly consists of the following components: 1) an efficient\ntokenizer that transforms 3D visual features into compact\n1D discrete representations while preserving and enhanc-\ning their semantic information; 2) a language model that\nperforms temporal sequence modeling to generate semantic\ntokens representing video blueprints from textual descrip-\ntions; 3) a streaming diffusion model that progressively re-\nfines coarse semantic videos by adding fine-grained details\nto produce high-quality VAE features; and 4) a VAE de-\ncoder that reconstructs the final video frames from the re-\nfined VAE features.\n\n3.1. Video Semantic Tokenizer\n\nIn this section, we introduce a novel video se\n...[truncated]",
    "https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/02357.pdf": "SNIPPET: Severe blurring of scene text images, resulting in the loss of critical strokes and textual information, has a profound impact on text readability and recognizability. Therefore, scene text image super-resolution, aiming to enhance text resolution and legibility in low-resolution images, is a crucial task. In this paper, we introduce a novel genera- difusion-conditioned-tive model for scene ...\n\nTITLE: (from PDF)\n\nBODY:\nDCDM: Diffusion-Conditioned-Diffusion Model\nfor Scene Text Image Super-Resolution\n\nShrey Singh1 , Prateek Keserwani1 , Masakazu Iwamura2 , and Partha\nPratim Roy1\n\n1 Indian Institute of Technology, Roorkee, Uttarakhand-247667, India\n{ssingh19, pkeserwani, partha}@cs.iitr.ac.in\n2 Osaka Metropolitan University, Sakai, Osaka 599-8531, Japan\nmasa.i@omu.ac.jp\n\nAbstract. Severe blurring of scene text images, resulting in the loss\nof critical strokes and textual information, has a profound impact on\ntext readability and recognizability. Therefore, scene text image super-\nresolution, aiming to enhance text resolution and legibility in low-resolution\nimages, is a crucial task. In this paper, we introduce a novel genera-\ntive model for scene text super-resolution called diffusion-conditioned-\ndiffusion model (DCDM). The model is designed to learn the distribu-\ntion of high-resolution images via two conditions: 1) the low-resolution\nimage and 2) the character-level text embedding generated by a latent\ndiffusion text model. The latent diffusion text module is specifically de-\nsigned to generate character-level text embedding space from the latent\nspace of low-resolution images. Additionally, the character-level CLIP\nmodule has been used to align the high-resolution character-level text\nembeddings with low-resolution embeddings. This ensures visual align-\nment with the semantics of scene text image characters. Our experi-\nments on the TextZoom and Real-CE datasets demonstrate the supe-\nriority of the proposed method to state-of-the-art methods. The source\ncodes and other resources will be available through the project page:\nhttps://github.com/shreygithub/DCDM.\n\nKeywords: Scene Text Image Super-Resolution · Diffusion-Conditioned-\nDiffusion Model · Character-Level CLIP\n\n1\n\nIntroduction\n\nScene text understanding has remained an important area of research in com-\nputer vision for over a decade. This field encompasses various tasks, including\nscene text recognition [33], scene text retrieval [37], and scene text visual question\nanswering [1]. A major challenge in these tasks is image degradation, particu-\nlarly due to low resolution. Additionally, these texts are optically degraded in\nthe form of blur and noise, which makes the reading the text difficult. Improving\n\n\f2\n\nS. Singh et al.\n\nFig. 1: Quality enhancement of low-resolution (LR) image using different methods and\ntheir optical character recognition (OCR) results. (a) Generated image by a generic\nsuper resolution method (SRCNN). (b) Generated image by a latent image diffusion\nmodel. (c) Generated image by the proposed method (diffusion-conditioned-diffusion)\nthat combines the diffusion method based text prior generation and used as a condition\nfor image diffusion model for scene text image super resolution.\n\nthe quality of the text strokes along with removing the noise and blur in the im-\nage is termed as scene text image super-resolution (STISR). A good solution is\nneeded to solve various text understanding tasks in a low-resolution constraint.\nIn past, various attempts have been made to enhance scene text resolution\nalong with removing blur and noise for reading the text efficiently by off-the-shelf\nOCR. The resolution of scene text images was improved by applying super-\nresolution (SR) techniques (illustrated in Fig. 1(a)), such as [42]. Afterward,\ntextual properties were used to enhance the super-resolution results [40]. A sig-\nnificant limitation of general-purpose image super-resolution techniques is their\ninability to emphasize text strokes compared to background pixels, which is\nneeded to improve the visual quality of scene text.\n\nFor emphasizing more on text than background pixels, various approaches\nhave been explored. They can be broadly grouped into the following categories:\ntext recognition loss [19,40], sequential information [3,39], text mask [49], stroke-\naware loss [20, 39], and text prior [16, 18, 48]. Their common feature is to utilize\nsome additional information to enhance the low-resolution (LR) text images.\nAmong those, the text-prior-based methods [16,18,48] uplifted most of the scene\ntext super-resolution performance. In these methods, a text recognizer is applied\nto the LR image during inference and the recognition result is used as a noisy\ntext prior with recognition errors. It can hinder the performance of the scene\ntext image super-resolution.\n\nFollowing the impact of diffusion models on generic image super-resolution [31,\n38], an image diffusion-based method trained on synthesized data [23] has been\n\ncEImageDiffusionTextDiffusionEGeneric Super-Resolution(a) Generic super resolution methodOCRHorc(b) Latent image diffusion model(c) Diffusion-conditioned-diffusion (Proposed method)NoiseNoisecImageDiffusionNoiseOCRHereOCRHoreGenerated ImageGenerated ImageLR ImageLR ImageLR ImageGenerated Image\fDCDM: Diffusion-Conditioned-Diffusion Model for STISR\n\n3\n\nrecently proposed, which is based on the method illustrated on Fig. 1(b). This\nmethod also relies on a text prior incorporating a text recognizer. However, a\nfundamental question arises; that is, “Is a text recognizer necessary for STISR\nin inference?” To seek an answer to this question, this work explores the possi-\nbility of utilizing latent text diffusion models as a generator of text prior to the\nsucceeding image diffusion model (as illustrated in Fig. 1(c)). In other words, we\nuse two diffusion models at once: the latent text diffusion and the image diffu-\nsion models. We call the proposed method diffusion-conditioned-diffusion model\n(DCDM). Like a text recognizer generates a text prior in the text-prior-based\nmethods, the latent text diffusion model takes an LR image and outputs the\ntext prior that conditions the succeeding image diffusion model. A character-\nlevel CLIP (CL-CLIP) model is used to train the latent text diffusion model.\nThis diffusion-conditioned-diffusion method is good for super-resolution and re-\nmoving blur impact due to optical degradation while grabbing the text image.\nThe most similar work to the proposed method is StableCascade (SC) [25],\nour concurrent work, which introduced two-stage latent diffusion models (LDMs)\nfor generating images and image embeddings. The key difference between our\nproposed method and SC lies in introducing an image-to-text LDM, which gen-\nerates a text before the succeeding text-to-image LDM.\n\nThe major contributions of the presented work are as follows:\n\n1. A diffusion-conditioned-diffusion model has been proposed which has utilized\n\nthe text characteristics for the image super-resolution for text.\n\n2. We introduce the latent text diffusion model to generate character-level\ntext embedding from a given low-resolution latent space. It incorporated a\ncharacter-level CLIP model (called CL-CLIP) to obtain linguistic and visual\nconnections.\n\n3. Through detailed experiments, we demonstrate the impact of the diffusion-\n\nconditioned-diffusion model on the STISR.\n\n2 Related Work\n\n2.1 Single Image Super-Resolution (SISR)\n\nThe SISR is a task for estimating a high resolution (HR) image from its cor-\nresponding LR image. The ill-posed nature of the SISR problem adds more\nchallenges to the problem. In the past, the prior information is used in the form\nof a distribution/energy function to aggregate the constraints of the SR image.\nAdaptive high-dimensional nonlocal total variation-based adaptive geometric du-\nality prior [29] and sparse regression and statistical image priors [12] are some\nimportant works on reconstruction-based techniques. These hand-crafted-based\nmethods work well in reducing the virtual artifacts but are still not enough to\nfulfill the requirements of the SISR. In recent years, convolutional neural net-\nworks (CNNs) have been frequently used and accomplish leading performance\nfor the SISR. The SRCNN pioneers CNN to learn the mapping function between\nLR and HR images. In later works, the CNN architectures are designed deeper\n\n\f4\n\nS. Singh et al.\n\nand with more sophistication to elevate the performance of SISR, for example,\nLaplacian pyramid [13], dense connections based [35], residual block, and chan-\nnel attention mechanism [46]. In recent work, prior information has been utilized\nto boost the performance of CNN architectures for SISR [9].\n\n2.2 Scene Text Image Super-Resolution (STISR)\n\nThe general-purpose SISR focuses on natural scene images. The STISR is a spe-\ncial case of SISR. Unlike general-purpose SISR, the objective of STISR is not\nonly to scale up the resolution of the text image but also to focus on improving\ntext readability. The preliminary methods for STISR adopted CNN architectures\nfrom general-purpose SISR and directly attempted to extend it for the text im-\nages. For ICDAR 2015 competition [26], Dong et al . [7] extended the SRCNN [6]\nto text images to achieve the best result in the competition. In [24], three SR\nframeworks are designed to accomplish SR on binary document images. The\nperformances of these initial methods are not good on the text images because\nthese methods directly utilize the generic SR frameworks and ignore text-centric\nproperties such as word or character-level layout details. PlugNet [19] utilized\na pluggable SR unit for a designing multi-task framework to perform SR and\nrecognition hand in hand.\n\nWang et al . [39] built a real-world dataset named TextZoom for STISR im-\nages. They also proposed a text super-resolution network (TSRN) to address\nthe STISR problem on real-world text images. The sequential residual block\n(SRB) is the main building block of TSRN. The sequential notion of SRB is\ncovered by using the horizontal and vertical bidirectional long short-term mem-\nory (BLSTM) blocks. Apart from BLSTM blocks (used to capture sequential\ninformation such as text), the TSRN is not doing much for text-related fea-\ntures. [3] proposed transformer-based super-resolution network (TBSRN) uses\na self-attention module to process sequential information. The perceptual text\nlosses as position and content awareness on a character level are applied to help\nthe text recognition. In [16], embed the text prior (guided by HR) into the STISR\nmodel for better reconstruction of text in the HR scene text images. The meth-\nods for STISR discussed above embed text-prior information to the SR module\nto help reconstruct the text in HR images. The embedding of only text prior to\nthe SR module is insufficient for the STISR. The prior low-resolution features\nneed to be boosted with the guides of ground truth in training. We aim to design\na module to boost the low-resolution features to support the reconstruction of\nthe HR text image and achieve better recognition in this paper.\n\n3 Proposed Method\n\nAs shown in Fig. 2, we introduce a novel Diffusion Conditional Diffusion Model\n(DCDM) consisting of two specialized diffusion-based modules. As shown in the\ntop part of Fig. 2, the first module, the Latent Text Diffusion Module, is de-\nsigned to learn the joint distribution between low-resolution images and text\n\n\fDCDM: Diffusion-Conditioned-Diffusion Model for STISR\n\n5\n\nFig. 2: The detailed diagram of the proposed method of diffusion-condition-diffusion.\nIt consists of two forward passes, one for the latent text diffusion model and the other\nfor the latent image diffusion model. The latent text diffusion model consists of a\ncharacter-level CLIP model for alignment between characters and the structural part\nof an image. The latent text diffusion model acts as a conditioning module for the\nlatent image diffusion model. The dotted line flow is only used during a training phase.\n\npriors. This module excels in discerning complex dependencies between latent\nimages and textual information, providing a comprehensive understanding of\ntheir interplay. Complementing this, as shown in the bottom part of Fig. 2, the\nsecond module, the Image Diffusion Model, is strategically designed for hybrid\nconditioning, considering both textual elements (text prior) and visual com-\nponents (low-resolution images). This dual consideration allows the model to\ncapture synergistic effects of text and images in a unified manner, enhancing its\nability to discern nuanced patterns and relationships within the data.\n\n3.1\n\nImage Diffusion Model\n\nIn our proposed work, we introduce a novel generative model, rooted in the prin-\nciples of the Diffusion Model (DM) [21], which is shown in the bottom part of\nFig. 2. This model’s primary objective is to acquire a high-resolution image, de-\nnoted as I HR, while considering specific conditioning information denoted as I LR\nand additional conditions represented as C. I HR is used only in the training time\nas the ground truth. Our approach fundamentally revolves around the concept\nof shaping the distribution of the high-resolution images, I HR. This is achieved\nthrough a gradual denoising process, effectively mirroring the reverse dynam-\nics of a predefined fixed-length Markov Chain. Let us denote the predefined\nfixed-length as T . Conceptually, our model can be envisioned as an ensemble of\n\nあLatent text denoising Unet (LTD Unet)(Reverse diffusion, repeating T times)C tImage denoising Unet (IDUnet)(Reverse diffusion, repeating T times)Text Prior Module (OCR)ImageEncoderHR Image(Used only in training)LR ImageForward diffusioncEnhanced image via super-resolution Character-level CLIP (CL-CLIP)HR Image(Used only in training)LR ImageForward diffusionQcKcVcQcKcVcQcKcVcQcKcVcQKVQKVQKVQKVLatent Text Diffusion ModelImage Diffusion ModelCItIHRILRZLRQKVCross AttentionFrozen WeightsFeature ConcatenationDenoising StepSkip Connectionc\f6\n\nS. Singh et al.\n\ndenoising autoencoders, each with a distinct role in the diffusion process. These\ndenoising autoencoders, referred to as Image Denoising UNet (IDUnet), are se-\n, I LR, C, t), with t iterating from\nquentially arranged and represented as ϵθ(I HR\n1 to T . I HR\nis full\nt\nof noise and I HR\nis expected to be close enough to I HR. The primary objective\nfunction under consideration is formulated as\n\nis the intermediate denoised HR image at the t-th step. I HR\n\nT\n\n1\n\nt\n\nLI LR→I HR = EI HR,I LR,C,ϵ∼N (0,1),t\n\n(cid:2)|ϵ − ϵθ(I HR\n\nt\n\n, I LR, C, t)|2\n2\n\n(cid:3) ,\n\n(1)\n\nwhere t = [1, 2, . . . , T ] is a uniform distribution function. The central element of\nthis objective function is the UNet denoising function, denoted as ϵθ. This func-\ntion is intricately conditioned by multiple components, encompassing I HR and\nI LR signifying image information, C representing text embedding (see Sec. 3.2),\nand ϵ following a standard normal distribution, N (0, 1) [31]. Our objective func-\ntion aims to\n...[truncated]"
  },
  "all_scored_papers": {
    "https://arxiv.org/abs/2410.21357": {
      "url": "https://arxiv.org/abs/2410.21357",
      "title": "Energy-Based Diffusion Language Models for Text Generation",
      "abstract": "Our analysis reveals that this degradation is a consequence of an imperfect approximation used by diffusion models. In this work, we propose Energy-based Diffusion Language Model (EDLM), an energy-based model operating at the full sequence level for each diffusion step, introduced to improve the underlying approximation used by diffusion models.",
      "score": 9,
      "relevant_tier": 1,
      "completeness_score": 6.0,
      "associated_candidates": []
    },
    "https://openreview.net/pdf?id=2UFmCXQ6zn": {
      "url": "https://openreview.net/pdf?id=2UFmCXQ6zn",
      "title": "Text Diffusion Model with Encoder-Decoder Transformers for Sequence-to ...",
      "abstract": "Abstract The diffusion model, a new generative mod-eling paradigm, has achieved great success in image, audio, and video generation. However, considering the discrete categorical nature of the text, it is not trivial to extend continuous diffusion models to natural language. In this work, we propose SeqDiffuSeq, a text diffu-sion model, to approach sequence-to-sequence text generation with an ...",
      "score": 9,
      "relevant_tier": 1,
      "completeness_score": 6.0,
      "associated_candidates": [
        "Hongyi Yuan"
      ]
    },
    "https://proceedings.neurips.cc/paper_files/paper/2023/hash/1df4afb0b4ebf492a41218ce16b6d8df-Abstract-Conference.html": {
      "url": "https://proceedings.neurips.cc/paper_files/paper/2023/hash/1df4afb0b4ebf492a41218ce16b6d8df-Abstract-Conference.html",
      "title": "TextDiffuser: Diffusion Models as Text Painters - NeurIPS",
      "abstract": "TextDiffuser consists of two stages: first, a Transformer model generates the layout of keywords extracted from text prompts, and then diffusion models generate images conditioned on the text prompt and the generated layout.",
      "score": 9,
      "relevant_tier": 1,
      "completeness_score": 5.0,
      "associated_candidates": []
    },
    "https://proceedings.neurips.cc/paper_files/paper/2023/hash/7d866abba506e5a56335e4644ebe18f9-Abstract-Conference.html": {
      "url": "https://proceedings.neurips.cc/paper_files/paper/2023/hash/7d866abba506e5a56335e4644ebe18f9-Abstract-Conference.html",
      "title": "AR-Diffusion: Auto-Regressive Diffusion Model for Text Generation - NeurIPS",
      "abstract": "Abstract Diffusion models have gained significant attention in the realm of image generation due to their exceptional performance. Their success has been recently expanded to text generation via generating all tokens within a sequence concurrently.",
      "score": 9,
      "relevant_tier": 1,
      "completeness_score": 5.0,
      "associated_candidates": [
        "Tong Wu"
      ]
    },
    "https://huggingface.co/blog/ProCreations/diffusion-language-model": {
      "url": "https://huggingface.co/blog/ProCreations/diffusion-language-model",
      "title": "Diffusion Language Models: The New Paradigm - Hugging Face",
      "abstract": "How diffusion transforms language generation Diffusion Language Models fundamentally reimagine text generation through a noise-to-text transformation process rather than sequential token prediction. The approach consists of two complementary phases that mirror the proven success of image diffusion models like DALL-E and Stable Diffusion.",
      "score": 9,
      "relevant_tier": 1,
      "completeness_score": 5.0,
      "associated_candidates": []
    },
    "https://openreview.net/forum?id=3s9IrEsjLyk": {
      "url": "https://openreview.net/forum?id=3s9IrEsjLyk",
      "title": "Diffusion-LM Improves Controllable Text Generation - OpenReview",
      "abstract": "We propose a non-autoregressive language model based on continuous diffusions, which demonstrate strong performance in controllable text generation.",
      "score": 9,
      "relevant_tier": 1,
      "completeness_score": 4.0,
      "associated_candidates": [
        "Xiang Lisa Li"
      ]
    },
    "https://iclr.cc/virtual/2023/poster/11561": {
      "url": "https://iclr.cc/virtual/2023/poster/11561",
      "title": "DiffuSeq: Sequence to Sequence Text Generation with Diffusion Models - ICLR",
      "abstract": "Virtual presentation / poster accept DiffuSeq: Sequence to Sequence Text Generation with Diffusion Models Shansan Gong · Mukai Li · Jiangtao Feng · Zhiyong Wu · Lingpeng Kong Keywords: [ diveristy ] [ text generation ] [ diffusion model ] [ sequence to sequence ] [ Generative models ]",
      "score": 9,
      "relevant_tier": 1,
      "completeness_score": 4.0,
      "associated_candidates": [
        "Shansan Gong"
      ]
    },
    "https://iclr.cc/virtual/2025/33081": {
      "url": "https://iclr.cc/virtual/2025/33081",
      "title": "ICLR Text-to-Model: Text-Conditioned Neural Network Diffusion for Train ...",
      "abstract": "This research introduces a text-conditioned neural network diffusion approach for personalized model training, enabling efficient customization through a single training process.",
      "score": 9,
      "relevant_tier": 1,
      "completeness_score": 4.0,
      "associated_candidates": []
    },
    "https://icml.cc/virtual/2023/poster/23708": {
      "url": "https://icml.cc/virtual/2023/poster/23708",
      "title": "ICML Poster Text Generation with Diffusion Language Models: A Pre ...",
      "abstract": "Poster Text Generation with Diffusion Language Models: A Pre-training Approach with Continuous Paragraph Denoise Zhenghao Lin · Yeyun Gong · Yelong Shen · Tong Wu · Zhihao Fan · Chen Lin · Nan Duan · Weizhu Chen",
      "score": 9,
      "relevant_tier": 1,
      "completeness_score": 4.0,
      "associated_candidates": []
    },
    "https://arxiv.org/abs/2305.10855": {
      "url": "https://arxiv.org/abs/2305.10855",
      "title": "[2305.10855] TextDiffuser: Diffusion Models as Text Painters",
      "abstract": "Diffusion models have gained increasing attention for their impressive generation abilities but currently struggle with rendering accurate and coherent text. To address this issue, we introduce TextDiffuser, focusing on generating images with visually appealing text that is coherent with backgrounds. TextDiffuser consists of two stages: first, a Transformer model generates the layout of ...",
      "score": 8,
      "relevant_tier": 0,
      "completeness_score": 6.0,
      "associated_candidates": [
        "Jingye Chen"
      ]
    },
    "https://arxiv.org/abs/2303.06574": {
      "url": "https://arxiv.org/abs/2303.06574",
      "title": "Diffusion Models for Non-autoregressive Text Generation: A Survey",
      "abstract": "Non-autoregressive (NAR) text generation has attracted much attention in the field of natural language processing, which greatly reduces the inference latency but has to sacrifice the generation accuracy. Recently, diffusion models, a class of latent variable generative models, have been introduced into NAR text generation, showing an improved text generation quality. In this survey, we review ...",
      "score": 8,
      "relevant_tier": 0,
      "completeness_score": 6.0,
      "associated_candidates": []
    },
    "https://arxiv.org/abs/2210.08933": {
      "url": "https://arxiv.org/abs/2210.08933",
      "title": "DiffuSeq: Sequence to Sequence Text Generation with Diffusion Models",
      "abstract": "Recently, diffusion models have emerged as a new paradigm for generative models. Despite the success in domains using continuous signals such as vision and audio, adapting diffusion models to natural language is under-explored due to the discrete nature of texts, especially for conditional generation. We tackle this challenge by proposing DiffuSeq: a diffusion model designed for sequence-to ...",
      "score": 8,
      "relevant_tier": 0,
      "completeness_score": 6.0,
      "associated_candidates": []
    },
    "https://arxiv.org/abs/2505.23606": {
      "url": "https://arxiv.org/abs/2505.23606",
      "title": "Muddit: Liberating Generation Beyond Text-to-Image with a Unified ...",
      "abstract": "Unlike prior unified diffusion models trained from scratch, Muddit integrates strong visual priors from a pretrained text-to-image backbone with a lightweight text decoder, enabling flexible and high-quality multimodal generation under a unified architecture.",
      "score": 8,
      "relevant_tier": 0,
      "completeness_score": 5.0,
      "associated_candidates": []
    },
    "https://ieeexplore.ieee.org/document/10630478": {
      "url": "https://ieeexplore.ieee.org/document/10630478",
      "title": "Table-to-Text Generation With Pretrained Diffusion Models",
      "abstract": "Diffusion models have demonstrated significant potential in achieving state-of-the-art performance across various text generation tasks. In this systematic study, we investigate their application to the table-to-text problem by adapting the diffusion model to the task and conducting an in-depth analysis. Our experiments cover multiple aspects of diffusion models training. We explore sampling ...",
      "score": 8,
      "relevant_tier": 0,
      "completeness_score": 5.0,
      "associated_candidates": []
    },
    "https://aclanthology.org/2023.findings-acl.721.pdf": {
      "url": "https://aclanthology.org/2023.findings-acl.721.pdf",
      "title": "PDF Can Diffusion Model Achieve Better Performance in Text Generation ...",
      "abstract": "A typical diffusion-based text generation model contains one reverse process (from noise to data) and one forward process (from data to noise), which is shown in Figure1.",
      "score": 8,
      "relevant_tier": 0,
      "completeness_score": 5.0,
      "associated_candidates": [
        "Zecheng Tang"
      ]
    },
    "https://neurips.cc/virtual/2024/poster/95436": {
      "url": "https://neurips.cc/virtual/2024/poster/95436",
      "title": "NeurIPS Poster Meta-Diffu$B$: A Contextualized Sequence-to-Sequence ...",
      "abstract": "Abstract: The diffusion model, a new generative modeling paradigm, has achieved significant success in generating images, audio, video, and text. It has been adapted for sequence-to-sequence text generation (Seq2Seq) through DiffuSeq, termed the S2S-Diffusion model.",
      "score": 8,
      "relevant_tier": 0,
      "completeness_score": 4.0,
      "associated_candidates": [
        "Yun-Yen Chuang"
      ]
    },
    "https://arxiv.org/abs/2212.11685": {
      "url": "https://arxiv.org/abs/2212.11685",
      "title": "[2212.11685] Text Generation with Diffusion Language Models: A Pre ...",
      "abstract": "In this paper, we introduce a novel dIffusion language modEl pre-training framework for text generation, which we call GENIE. GENIE is a large-scale pretrained diffusion language model that consists of an encoder and a diffusion-based decoder, which can generate text by gradually transforming a random noise sequence into a coherent text sequence. To pre-train GENIE on a large-scale language ...",
      "score": 9,
      "relevant_tier": 1,
      "completeness_score": 6.0,
      "associated_candidates": []
    },
    "https://arxiv.org/pdf/2212.11685": {
      "url": "https://arxiv.org/pdf/2212.11685",
      "title": "Text Generation with Diffusion Language Models: A Pre-training Approach ...",
      "abstract": "In this paper, we introduce a novel dIffusion language modEl pre-training framework for text generation, which we call GENIE. GENIE is a large-scale pretrained diffusion language model that consists of an encoder and a diffusion-based decoder, which can generate text by gradually transforming a random noise sequence into a coherent text sequence.",
      "score": 9,
      "relevant_tier": 1,
      "completeness_score": 6.0,
      "associated_candidates": []
    },
    "https://arxiv.org/abs/2301.05221v1": {
      "url": "https://arxiv.org/abs/2301.05221v1",
      "title": "Guiding Text-to-Image Diffusion Model Towards Grounded Generation",
      "abstract": "The goal of this paper is to augment a pre-trained text-to-image diffusion model with the ability of open-vocabulary objects grounding, i.e., simultaneously generating images and segmentation masks for the corresponding visual entities described in the text prompt. We make the following contributions: (i) we insert a grounding module into the existing diffusion model, that can be trained to ...",
      "score": 9,
      "relevant_tier": 1,
      "completeness_score": 6.0,
      "associated_candidates": []
    },
    "https://openaccess.thecvf.com/content/CVPR2025/papers/Wang_DesignDiffusion_High-Quality_Text-to-Design_Image_Generation_with_Diffusion_Models_CVPR_2025_paper.pdf": {
      "url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Wang_DesignDiffusion_High-Quality_Text-to-Design_Image_Generation_with_Diffusion_Models_CVPR_2025_paper.pdf",
      "title": "PDF DesignDiffusion: High-Quality Text-to-Design Image Generation with ...",
      "abstract": "We propose DesignDiffusion, an end-to-end, diffusion-based framework for text-to-design image generation, which enables the simultaneous generation of image and visual text elements, eliminating the necessity for prede-fined text regions or traditional two-stage separated text and image creation process.",
      "score": 9,
      "relevant_tier": 1,
      "completeness_score": 5.0,
      "associated_candidates": []
    },
    "https://eccv.ecva.net/virtual/2024/poster/527": {
      "url": "https://eccv.ecva.net/virtual/2024/poster/527",
      "title": "ECCV Poster TextDiffuser-2: Unleashing the Power of Language ... - ECVA",
      "abstract": "Abstract: The diffusion model has been proven a powerful generative model in recent years, yet it remains a challenge in generating visual text. Although existing work has endeavored to enhance the accuracy of text rendering, these methods still suffer from several drawbacks, such as (1) limited flexibility and automation, (2) constrained capability of layout prediction, and (3) restricted ...",
      "score": 9,
      "relevant_tier": 1,
      "completeness_score": 5.0,
      "associated_candidates": [
        "Jingye Chen"
      ]
    },
    "https://dl.acm.org/doi/10.5555/3618408.3619275": {
      "url": "https://dl.acm.org/doi/10.5555/3618408.3619275",
      "title": "Text generation with diffusion language models | Proceedings of the ...",
      "abstract": "In this paper, we introduce a novel dIffusion language modEl pre-training framework for text generation, which we call GENIE. GENIE is a large-scale pre-trained diffusion language model that consists of an encoder and a diffusion-based decoder, which can generate text by gradually transforming a random noise sequence into a coherent text sequence.",
      "score": 9,
      "relevant_tier": 1,
      "completeness_score": 5.0,
      "associated_candidates": []
    },
    "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_DesignDiffusion_High-Quality_Text-to-Design_Image_Generation_with_Diffusion_Models_CVPR_2025_paper.html": {
      "url": "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_DesignDiffusion_High-Quality_Text-to-Design_Image_Generation_with_Diffusion_Models_CVPR_2025_paper.html",
      "title": "CVPR 2025 Open Access Repository",
      "abstract": "DesignDiffusion: High-Quality Text-to-Design Image Generation with Diffusion Models Zhendong Wang, Jianmin Bao, Shuyang Gu, Dong Chen, Wengang Zhou, Houqiang Li; Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2025, pp. 20906-20915",
      "score": 9,
      "relevant_tier": 1,
      "completeness_score": 4.5,
      "associated_candidates": [
        "Zhendong Wang"
      ]
    },
    "https://scisimple.com/en/articles/2025-09-02-introducing-the-text-encoding-diffusion-model-for-text-generation--a37dy66": {
      "url": "https://scisimple.com/en/articles/2025-09-02-introducing-the-text-encoding-diffusion-model-for-text-generation--a37dy66",
      "title": "Introducing the Text Encoding Diffusion Model for Text Generation",
      "abstract": "TEncDM improves text generation quality through unique encoding techniques and self-conditioning. In recent years, diffusion models have gained attention for...",
      "score": 9,
      "relevant_tier": 1,
      "completeness_score": 4.0,
      "associated_candidates": [
        "Alexander Shabalin"
      ]
    },
    "https://arxiv.org/abs/2312.16476": {
      "url": "https://arxiv.org/abs/2312.16476",
      "title": "SVGDreamer: Text Guided SVG Generation with Diffusion Model",
      "abstract": "Recently, text-guided scalable vector graphics (SVGs) synthesis has shown promise in domains such as iconography and sketch. However, existing text-to-SVG generation methods lack editability and struggle with visual quality and result diversity. To address these limitations, we propose a novel text-guided vector graphics synthesis method called SVGDreamer. SVGDreamer incorporates a semantic ...",
      "score": 8,
      "relevant_tier": 0,
      "completeness_score": 6.0,
      "associated_candidates": [
        "Ximing Xing"
      ]
    },
    "https://arxiv.org/abs/2402.14314": {
      "url": "https://arxiv.org/abs/2402.14314",
      "title": "Typographic Text Generation with Off-the-Shelf Diffusion Model",
      "abstract": "Recent diffusion-based generative models show promise in their ability to generate text images, but limitations in specifying the styles of the generated texts render them insufficient in the realm of typographic design. This paper proposes a typographic text generation system to add and modify text on typographic designs while specifying font styles, colors, and text effects. The proposed ...",
      "score": 8,
      "relevant_tier": 0,
      "completeness_score": 6.0,
      "associated_candidates": []
    },
    "https://arxiv.org/abs/2405.12531": {
      "url": "https://arxiv.org/abs/2405.12531",
      "title": "CustomText: Customized Textual Image Generation using Diffusion Models",
      "abstract": "Textual image generation spans diverse fields like advertising, education, product packaging, social media, information visualization, and branding. Despite recent strides in language-guided image synthesis using diffusion models, current models excel in image generation but struggle with accurate text rendering and offer limited control over font attributes. In this paper, we aim to enhance ...",
      "score": 8,
      "relevant_tier": 0,
      "completeness_score": 6.0,
      "associated_candidates": []
    },
    "https://ieeexplore.ieee.org/document/10159911": {
      "url": "https://ieeexplore.ieee.org/document/10159911",
      "title": "An Overview of Diffusion Models for Text Generation",
      "abstract": "Given the great success that diffusion models have achieved in generating various types of continuous data, including image, video and audio, there has been a growing interest in the application of these models to text generation. However, the discrete nature of text presents a challenge for diffusion models initially designed for application in a continuous feature space. The two main lines ...",
      "score": 8,
      "relevant_tier": 0,
      "completeness_score": 5.0,
      "associated_candidates": []
    },
    "https://dl.acm.org/doi/10.1007/978-3-031-72633-0_17": {
      "url": "https://dl.acm.org/doi/10.1007/978-3-031-72633-0_17",
      "title": "DCDM: Diffusion-Conditioned-Diffusion Model for Scene Text Image Super ...",
      "abstract": "In this paper, we introduce a novel generative model for scene text super-resolution called diffusion-conditioned-diffusion model (DCDM). The model is designed to learn the distribution of high-resolution images via two conditions: 1) the low-resolution image and 2) the character-level text embedding generated by a latent diffusion text model.",
      "score": 8,
      "relevant_tier": 0,
      "completeness_score": 5.0,
      "associated_candidates": []
    },
    "https://dl.acm.org/doi/abs/10.24963/ijcai.2023/750": {
      "url": "https://dl.acm.org/doi/abs/10.24963/ijcai.2023/750",
      "title": "Diffusion models for non-autoregressive text generation:",
      "abstract": "As the background, we first present the general definition of diffusion models and the text diffusion models, and then discuss their merits for NAR generation. As the core content, we further introduce two mainstream diffusion models in existing work of text diffusion, and review the key designs of the diffusion process.",
      "score": 8,
      "relevant_tier": 0,
      "completeness_score": 5.0,
      "associated_candidates": []
    },
    "https://iccv.thecvf.com/virtual/2025/poster/1930": {
      "url": "https://iccv.thecvf.com/virtual/2025/poster/1930",
      "title": "ICCV Poster Beyond Isolated Words: Diffusion Brush for Handwritten Text ...",
      "abstract": "However, this task poses significant challenges, including the accurate modeling of complex style patterns—encompassing both intra- and inter-word relationships—and maintaining content accuracy across numerous characters. To address these challenges, we propose DiffBrush, a novel diffusion-based model for handwritten text-line generation.",
      "score": 8,
      "relevant_tier": 0,
      "completeness_score": 5.0,
      "associated_candidates": [
        "Gang Dai"
      ]
    },
    "https://iccv.thecvf.com/virtual/2025/poster/1917": {
      "url": "https://iccv.thecvf.com/virtual/2025/poster/1917",
      "title": "ICCV Poster Draw Your Mind: Personalized Generation via Condition-Level ...",
      "abstract": "Personalized generation in T2I diffusion models aims to naturally incorporate individual user preferences into the generation process with minimal user intervention. However, existing studies primarily rely on prompt-level modeling with large-scale models, often leading to inaccurate personalization due to the limited input token capacity of T2I diffusion models. To address these limitations ...",
      "score": 8,
      "relevant_tier": 0,
      "completeness_score": 5.0,
      "associated_candidates": [
        "Hyungjin Kim"
      ]
    },
    "https://openaccess.thecvf.com/content/ICCV2025/papers/Yin_The_Best_of_Both_Worlds_Integrating_Language_Models_and_Diffusion_ICCV_2025_paper.pdf": {
      "url": "https://openaccess.thecvf.com/content/ICCV2025/papers/Yin_The_Best_of_Both_Worlds_Integrating_Language_Models_and_Diffusion_ICCV_2025_paper.pdf",
      "title": "PDF The Best of Both Worlds: Integrating Language Models and Diffusion ...",
      "abstract": "Text-to-video (T2V) [6,30,55,60] has made significant progress in recent years, becoming an important research direction in the fields of computer vision and artificial in- telligence. Recent works in T2V models have primarily revolved around two predominant paradigms: autoregres- sive large language model (LLM)-based [30,55] frame- works and diffusion-based architectures [6,60]. However, each ...",
      "score": 8,
      "relevant_tier": 0,
      "completeness_score": 5.0,
      "associated_candidates": []
    },
    "https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/02357.pdf": {
      "url": "https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/02357.pdf",
      "title": "PDF DCDM:Diffusion-Conditioned-DiffusionModel forSceneTextImageSuper ...",
      "abstract": "Severe blurring of scene text images, resulting in the loss of critical strokes and textual information, has a profound impact on text readability and recognizability. Therefore, scene text image super-resolution, aiming to enhance text resolution and legibility in low-resolution images, is a crucial task. In this paper, we introduce a novel genera- difusion-conditioned-tive model for scene ...",
      "score": 7,
      "relevant_tier": 0,
      "completeness_score": 5.0,
      "associated_candidates": []
    },
    "https://peerj.com/articles/cs-1905.pdf": {
      "url": "https://peerj.com/articles/cs-1905.pdf",
      "title": "PDF Diffusion models in text generation: a survey - PeerJ",
      "abstract": "In this article, we conduct a comprehensive survey on the application of diffusion models in text generation. We divide text generation into three parts (conditional, unconstrained, and multi-mode text generation, respectively) and provide a detailed introduction.",
      "score": 8,
      "relevant_tier": 0,
      "completeness_score": 4.0,
      "associated_candidates": []
    },
    "https://eccv.ecva.net/virtual/2024/poster/1027": {
      "url": "https://eccv.ecva.net/virtual/2024/poster/1027",
      "title": "ECCV Poster DCDM: Diffusion-Conditioned-Diffusion Model for Scene Text ...",
      "abstract": "The model is designed to learn the distribution of high-resolution images via two conditions: 1) the low-resolution image and 2) the character-level text embedding generated by a latent diffusion text model.",
      "score": 8,
      "relevant_tier": 0,
      "completeness_score": 4.0,
      "associated_candidates": []
    }
  },
  "search_candidate_set": [],
  "selected_urls_set": [
    "https://jingyechen.github.io/textdiffuser",
    "https://arxiv.org/abs/2405.12531",
    "https://icml.cc/virtual/2023/poster/23708",
    "https://arxiv.org/abs/2301.05221v1",
    "https://arxiv.org/abs/2505.23606",
    "https://arxiv.org/abs/2402.14314",
    "https://aclanthology.org/2024.naacl-long.261",
    "https://scisimple.com/en/articles/2025-09-02-introducing-the-text-encoding-diffusion-model-for-text-generation--a37dy66",
    "https://eccv.ecva.net/virtual/2024/poster/527",
    "https://iclr.cc/virtual/2023/poster/11561",
    "https://openaccess.thecvf.com/content/ICCV2025/papers/Yin_The_Best_of_Both_Worlds_Integrating_Language_Models_and_Diffusion_ICCV_2025_paper.pdf",
    "https://arxiv.org/abs/2303.06574",
    "https://arxiv.org/abs/2305.10855",
    "https://research.nvidia.com/labs/genair/publication/xu2024eddm",
    "https://arxiv.org/pdf/2212.11685",
    "https://iccv.thecvf.com/virtual/2025/poster/1930",
    "https://openreview.net/pdf?id=2UFmCXQ6zn",
    "https://arxiv.org/abs/2210.08933",
    "https://huggingface.co/blog/ProCreations/diffusion-language-model",
    "https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/02357.pdf",
    "https://ieeexplore.ieee.org/document/10630478",
    "https://arxiv.org/abs/2212.11685",
    "https://dl.acm.org/doi/10.1007/978-3-031-72633-0_17",
    "https://aclanthology.org/2023.findings-acl.721.pdf",
    "https://iclr.cc/virtual/2025/33081",
    "https://neurips.cc/virtual/2024/poster/95436",
    "https://iccv.thecvf.com/virtual/2025/poster/1917",
    "https://peerj.com/articles/cs-1905.pdf",
    "https://arxiv.org/abs/2312.16476",
    "https://openaccess.thecvf.com/content/CVPR2025/papers/Wang_DesignDiffusion_High-Quality_Text-to-Design_Image_Generation_with_Diffusion_Models_CVPR_2025_paper.pdf",
    "https://aclanthology.org/2024.naacl-long.2",
    "https://arxiv.org/abs/2410.21357",
    "https://eccv.ecva.net/virtual/2024/poster/1027",
    "https://johal.in/generative-ai-models-diffusion-models-and-text-to-image-generation-2026",
    "https://dl.acm.org/doi/10.5555/3618408.3619275",
    "https://pubmed.ncbi.nlm.nih.gov/38435628",
    "https://ieeexplore.ieee.org/document/10159911",
    "https://pmc.ncbi.nlm.nih.gov/articles/PMC10909201",
    "https://dl.acm.org/doi/abs/10.24963/ijcai.2023/750",
    "https://peerj.com/articles/cs-1905",
    "https://proceedings.neurips.cc/paper_files/paper/2023/hash/7d866abba506e5a56335e4644ebe18f9-Abstract-Conference.html",
    "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_DesignDiffusion_High-Quality_Text-to-Design_Image_Generation_with_Diffusion_Models_CVPR_2025_paper.html",
    "https://openreview.net/forum?id=3s9IrEsjLyk",
    "https://proceedings.neurips.cc/paper_files/paper/2023/hash/1df4afb0b4ebf492a41218ce16b6d8df-Abstract-Conference.html"
  ],
  "selected_serp_url_set": [
    "https://arxiv.org/abs/2405.12531",
    "https://icml.cc/virtual/2023/poster/23708",
    "https://arxiv.org/abs/2301.05221v1",
    "https://arxiv.org/abs/2505.23606",
    "https://arxiv.org/abs/2402.14314",
    "https://scisimple.com/en/articles/2025-09-02-introducing-the-text-encoding-diffusion-model-for-text-generation--a37dy66",
    "https://eccv.ecva.net/virtual/2024/poster/527",
    "https://iclr.cc/virtual/2023/poster/11561",
    "https://openaccess.thecvf.com/content/ICCV2025/papers/Yin_The_Best_of_Both_Worlds_Integrating_Language_Models_and_Diffusion_ICCV_2025_paper.pdf",
    "https://arxiv.org/abs/2303.06574",
    "https://arxiv.org/abs/2305.10855",
    "https://arxiv.org/pdf/2212.11685",
    "https://iccv.thecvf.com/virtual/2025/poster/1930",
    "https://openreview.net/pdf?id=2UFmCXQ6zn",
    "https://arxiv.org/abs/2210.08933",
    "https://huggingface.co/blog/ProCreations/diffusion-language-model",
    "https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/02357.pdf",
    "https://ieeexplore.ieee.org/document/10630478",
    "https://arxiv.org/abs/2212.11685",
    "https://dl.acm.org/doi/10.1007/978-3-031-72633-0_17",
    "https://aclanthology.org/2023.findings-acl.721.pdf",
    "https://iclr.cc/virtual/2025/33081",
    "https://neurips.cc/virtual/2024/poster/95436",
    "https://iccv.thecvf.com/virtual/2025/poster/1917",
    "https://peerj.com/articles/cs-1905.pdf",
    "https://arxiv.org/abs/2312.16476",
    "https://openaccess.thecvf.com/content/CVPR2025/papers/Wang_DesignDiffusion_High-Quality_Text-to-Design_Image_Generation_with_Diffusion_Models_CVPR_2025_paper.pdf",
    "https://arxiv.org/abs/2410.21357",
    "https://eccv.ecva.net/virtual/2024/poster/1027",
    "https://dl.acm.org/doi/10.5555/3618408.3619275",
    "https://ieeexplore.ieee.org/document/10159911",
    "https://dl.acm.org/doi/abs/10.24963/ijcai.2023/750",
    "https://proceedings.neurips.cc/paper_files/paper/2023/hash/7d866abba506e5a56335e4644ebe18f9-Abstract-Conference.html",
    "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_DesignDiffusion_High-Quality_Text-to-Design_Image_Generation_with_Diffusion_Models_CVPR_2025_paper.html",
    "https://openreview.net/forum?id=3s9IrEsjLyk",
    "https://proceedings.neurips.cc/paper_files/paper/2023/hash/1df4afb0b4ebf492a41218ce16b6d8df-Abstract-Conference.html"
  ],
  "created_at": 1760707049.2755158,
  "updated_at": 1760707049.2755158
}