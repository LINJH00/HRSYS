{"url": "https://hub.baai.ac.cn/view/49601", "title": "太狠了，四条腿被锯掉也能爬！通用大脑开启机器人「无休」时代", "date": "2025-10-18", "content": "新智元报道 编辑：倾倾 【新智元导读】 机器「趴窝」，或许就此终结！Skild AI公布的通用「大脑」，能在断肢、损坏、甚至换一副全新身体后依旧继续前行。它像拥有永不熄灭的求生本能，每一次受伤都化作新的步伐。屏幕上闪烁的「Adapting…」，仿佛在向世界宣告：机器人真正不可阻挡的时代，已经到来。 科幻里的机器人无所不能，哪怕残缺也能继续前行。 可现实中，它们却极度脆弱：机械臂松个螺丝就停摆，四足机器人电机一顿就「扑街」。 Skild AI在X上发布的一条短视频给出了新答案：机器人在损伤中依旧前行，屏幕上不断闪现「Adapting…」。 这背后的秘密，正是他们提出的全新设想： Skild Brain ，一个独立于身体、始终能适应的机器人「大脑」。 从故障到自适应：Skild Brain的智慧美学 我们曾经的想象里，机器人的「大脑」必须紧密依附在身体之上：每一次关节损坏、每一次舵机失灵，都可能意味着整机瘫痪。 然而，Skild AI带来一场根本性的颠覆——他们提出了一个「与身体解耦的机器人大脑」概念，这正是Skild Brain的核心。 他们的愿景是： 全能形态智能：面向任何任务、任何机器人、一个大脑。 在这个架构里，Skild Brain被设计成可跨形态、跨任务操作的通用智能，并已在四足、类人、移动机械臂等不同主体上部署试验。 他们的博客透露出技术细节：控制策略被分为高频与低频两个层级——高层负责规划导航动作，低层负责将策略转化为关节扭矩和角度，并在不同机器人之间迁移。 更令人惊叹的是，Skild团队在X上直言： 断掉的肢体？卡住的电机？只要机器人还能动，这个大脑就能让它继续前进——哪怕换成一副全新的身体。 这句话正是他们设计这套系统的宣言：只要存在可用的运动通道，大脑就能找到方式行动。 在外界看来，这套系统不只是技术秀，更可能是机器人控制范式的一次跃迁：它把「硬件故障」从致命伤，变成可以「绕道继续」的挑战。 为什么机器人总是「趴窝」？ 在机器人的历史里，有一个绕不开的顽疾： 控制与机体的强绑定 。 服务机器人面对复杂环境，只要轻微损伤，就会彻底罢工。 问题的根源在于，大多数算法都是针对单一硬件反复调优。 它们就像是「死记硬背」的学生：一旦换了题型，就不知所措。环境稍一变化，机器人就陷入停滞。 Skild Brain想要打破的，正是这种局限。它并没有为某一台机器量身定制，而是在一个虚拟世界中，创造出十万种不同的机器人形态，让同一个「大脑」去适配它们。 四条腿被锯掉，它学会用残肢爬行；关节被锁死，它会重新规划步态；轮子突然卡住，它立刻切换为步行模式；甚至在被强行装上「高跷」后，它也能跌跌撞撞地重新站稳。 这种能力的震撼之处在于——它几乎都是「零样本」的，也就是说，这些极端损伤场景并不在训练集中。 Skild Brain依然能在未知的情况下，快速找出一条可行的生路。 Wired在报道中用了一个极端画面来形容： 即便四足机器人的四条腿全被电锯砍断，它依然能靠着躯干和残肢继续前进。 这并不是恐怖片桥段，而是真实的技术展示。 正因如此，Skild Brain才被视为一次范式转向：它让「故障」不再是终点，而是新的起点。 哪怕身体支离破碎，大脑依旧在适应，并寻找另一种方式继续前行。 它是怎么做到的？Skild Brain的「大脑逻辑」 看到视频里机器人即便残损也能继续前行，不少人会好奇：它凭什么撑得住？ Skild并非凭空魔术，而是靠一套极致训练与设计组合：大规模模拟训练、泛化模型与在线自适应机制。 他们构建了一个包含1万种不同机器人形态 的虚拟宇宙，并在其中让模型「走」了 1000年的模拟时间，以逼它学会跨形态适应。 这样做的目的，是要避免模型死记硬背，而是让它学一种策略，在未知结构里也能快速应对。 具体来看，这个系统背后至少融合了三条关键路径： 第一条，是 强化学习 。 模型在模拟世界中不断尝试、失败、优化，就像孩子学走路。 失败很常见，但正是失败让它积累对策略的认识。 Skild Brain 能把失败留在记忆里，从而在下一个动作中立刻修正。这是强化学习和长时记忆结合带来的质变。 第二条，是 迁移与泛化能力 。 传统的机器人算法往往困在单机世界：这台机械狗学会了跑步，但换一副身体就要从零开始。 Skild Brain想要的是「一次学习，处处可用」——就像人类不需要重学走路，就能换鞋、拄拐，甚至装上义肢继续前行。 第三条，是 在线自适应机制 。 Skild Brain在运行中会实时感知身体状态——哪条腿僵住了、哪个电机卡了，并立刻调整控制策略。 这种「即刻调整」的逻辑，正像我们在人生病或受伤时，大脑会立刻重新规划步态，借助另一条腿或外部支撑保持平衡。 在另一项实验中，研究人员锁死了一个带轮子和腿的四足机器人中的两个电机，本应陷入瘫痪的它，却像一辆晃晃悠悠的自行车一样，凭借仅剩的两个轮子维持平衡。 这正是Skild Brain的惊人之处——它能在前所未见的极端状态下，临场找到全新的生存方式。 正是这套逻辑的组合，造就了Skild Brain「势不可挡」的能力。 Skild Brain与传统机器人不同的，还不只是「反应快」。 Skild表示，它的大脑拥有 非凡的记忆力 ：多数机器人控制策略的记忆窗口只有几百毫秒，只能应对眼前的一个动作；而Skild Brain的上下文窗口却长出一百倍以上。 这意味着，它能「记住」过去更长时间的反馈和动作轨迹，就像人类学会从经验中调整步伐，而不是单靠瞬时反射。 也正因如此，当它被迫让四足机器人站立行走时，才会在零样本环境下「理解」为类人身体，并临时调用步态完成适应。 从短时反射到长时记忆，从硬件依赖到跨体泛化，Skild Brain的逻辑链条逐渐完整：它不只是能应付眼前的意外，而是能在记忆、经验与适应的循环中， 不断把陌生变成熟悉，把意外变成常态 。 一个大脑，统御万机：应用与未来 Skild Brain并不是一场实验室里的炫技。它的意义在于——当「大脑」真正能独立存在，机器人就能在更多场景里展现「韧性」。 在工业和商业领域，这意味着生产线上的机器人不必因一个小故障就全线停机。 过去，螺丝松动就要停产检修；未来，Skild Brain能让它带伤继续工作，等维护窗口再统一修复，效率与寿命都将被大幅拉长。 在灾难救援中，它的价值更加直观。 地震、火灾、矿难，这些环境对机器人本就苛刻，而在恶劣条件下难免会受损。 Skild Brain的自适应能力，让它即使在「残肢断臂」的情况下，也能继续执行搜救任务，直到最后一刻。 军事领域同样是潜在场景。 战场上，设备损伤几乎不可避免。如果机器「大脑」能在残缺机体里依然找到生路，那意味着更多的续航力与生存力。 而在消费级市场，一个「大脑」可以「换壳」的想象也十分诱人：今天它在扫地机里，明天它能上到机械狗，后天它能迁移到家用助理机器人。 一次训练，N种用途，成本将被成倍摊薄。 但真正值得深思的是，Skild Brain不只是多了几个使用场景，它可能意味着一个新的机器人生态： 一个大脑，统御万机 。 机器人不再是孤立的单机个体，而是共享同一个思维内核。它们的行动会越来越像是「一个大脑控制着成千上万具身体」。 这既是效率的奇迹，也带来了全新的难题：谁拥有这个大脑的控制权？如果它的适应性超出预期，人类是否能完全掌控？社会结构和劳动市场又会被怎样重塑？ 正如Skild视频里闪烁的那行字——「Adapting…」。这不仅是机器人学会的技能，更是技术本身的写照。 问题只在于：当一个「永不停歇的大脑」真的诞生，人类要如何迎接它？ 参考资料： https://x.com/SkildAI/status/1970940614234771579 https://www.skild.ai/blogs/building-the-general-purpose-robotic-brain?utm_source"}
{"url": "https://hub.baai.ac.cn/view/49586", "title": "推理提速4倍！北航、浙大提出动态拼接，大小模型智能协作", "date": "2025-10-18", "content": "新智元报道 编辑：LRST 【新智元导读】 针对「 大模型推理速度慢，生成token高延迟」的难题，莫纳什、北航、浙大等提出R-Stitch框架，通过大小模型动态协作，衡量任务风险后灵活选择：简单任务用小模型，关键部分用大模型。实验显示推理速度提升最高4倍，同时保证高准确率。 近两年，思维链（Chain-of-Thought, CoT）推理让大语言模型在复杂推理任务上展现出前所未有的能力——从数学解题到逻辑分析，表现令人惊叹。 然而，这种强大的推理能力也带来了一个长期存在的挑战：推理过程过于缓慢。 每生成一个 token，模型都要完整算一遍前向传播。长一点的推理链，几千上万个 token，不仅延迟高，成本也成倍增加。 在加速大语言模型的诸多方向中，Speculative Decoding（投机解码） 一直是备受关注的方案。 它的思路简单：让小模型（SLM）先预测一段输出，大模型（LLM）校验一致性。若一致，大模型就能够一次接受小模型的输出，减少大模型解码的次数，从而加速。 莫纳什、北航、浙江大学等机构的研究者提出了一种动态拼接的大模型推理框架R-Stitch，让大小模型智能协作，在vLLM推理框架下提速最高可达4倍。 项目主页：https://caesarhhh.github.io/R-Stitch 论文链接： https://arxiv.org/abs/2507.17307 研究团队在AMC数据集上，对投机解码测试了多组模型组合，包括DeepSeek-R1-Distill-Qwen-1.5B/7B、L1-1.5B-Short、以及 Qwen2.5-Math-1.5B/7B-Oat-Zero等。 结果显示出一致的趋势：token 一致性越高，加速越明显；一致性越低，速度提升就越有限。 并且，并非所有样本都能加速。团队观察到相当一部分输入的速度提升低于 1×，即比原始推理还慢——说明当模型间差异较大时，投机解码的一致性校验可能反而带来开销。 图1 Token一致性与推理加速分析。 (a) 不同大模型–小模型组合在AMC数据集上的token一致率与加速比关系；(b) 各样本的加速比分布，可见部分样本出现低于1×的情况；(c) 在两者都答对的问题上，L1-1.5B-Short的推理更短、更简洁。 更有意思的是，在DeepSeek-R1-Distill-Qwen -7B与L1-1.5B-Short的实验中，研究人员发现： 当两者都答对时，L1-Short的推理长度远小于该7B模型。 这意味着：小模型可能以更短的推理路径到达相同结论，而投机解码对token一致性的强调，则忽略了这些更高效的路径。 经验熵分析 不确定性揭示推理风险 图2 熵与错误位置的关系。 (a) 错误解答的整体熵明显高于正确解答；(b) 熵分布高度集中在0附近，大多数token具有极低或接近零的熵；(c) 首个导致错误的 token 附近区域平均熵更高，说明模型往往在不确定区域产生错误。 在让大小模型协同推理之前，需要弄清楚一个关键问题： 小模型在什么时候能被信任？又是什么迹象预示它可能要「翻车」？ 为此，团队在AMC数据集 上，对推理过程中的token熵分布进行了深入分析，使用DeepSeek-R1-Distill-Qwen-7B作为大模型，L1-1.5B-Short作为小模型，逐步揭示出三个稳定的规律。 首先，大多数token的熵极低。统计结果显示，只有约10%的token熵超过 0.1，而绝大部分几乎为零。这意味着模型在大部分生成过程中都非常「自信」，真正的不确定区域其实只占很小一部分。 其次，错误推理的平均熵显著更高。在同样的题目上，错误答案的 token 熵明显更高，说明模型在这些题目的推理中更不确定。 并且，团队观察到一个关键细节：在小模型出错的样本中，错误往往起源于局部的高熵区域。 在第一个导致错误的 token 附近 token的平均熵显著高于全局水平，因此上述现象可以说明高熵的地方更容易导致错误。 大小模型协同动态解码 图3 R-Stitch方法 流程图 熵引导的动态切换 基于上述实验观察，团队提出了一种能根据熵动态切换大小模型的解码框架R-Stitch，让推理既快又稳。 核心思路是： 让小模型 (SLM) 尽可能多地负责推理，以提升速度； 让大模型 (LLM) 只在「高熵、高风险」的时刻介入，以保证正确率； 当LLM处理完复杂部分、进入「低熵」区间时，它又能 把控制权交还给SLM ，继续快速生成。 在每一步生成中，模型都会对所有候选词计算一个概率分布，研究人员用这个分布的 归一化熵值 （介于0到1之间）来衡量不确定性。 当SLM的熵值低于阈值τ（表示确定性高）时，它继续生成； 当熵值高于阈值时，说明「有点拿不准」，此时就交由LLM接手； 反过来，当 LLM 的熵再次降到阈值以下时，它会把控制权还给SLM 这种基于熵的 双向切换策略 ，使系统能在推理链中灵活往返，实现速度与精度的动态平衡。 基于强化学习的动态切换 在R-Stitch 基础之上，团队进一步提出了 R-Stitch⁺，通过强化学习让一个轻量级路由器学会更智能地决策何时切换。 这个路由器会在高熵时段读取当前模型隐状态，判断是继续用 SLM 还是交给 LLM。 团队为它设计了 两种奖励信号 ： 准确率奖励：答案正确则加分； 效率奖励：计算越快越好，但前提是不能牺牲正确率。 为准确评估推理成本，R-Stitch⁺引入了真实延迟感知的奖励函数。 总的奖励形式为： 其中 表示预测正确与否， 是整条推理轨迹的估计延迟，λ为平衡系数。 延迟只在输出正确时被惩罚，确保模型不会「为快而快」。 由于在强化学习中直接测量每个样本每条采样轨迹的真实延迟代价太高，R-Stitch⁺采用了延迟估计器。 团队为每个模型在不同输入长度和缓存大小下采样延时数据，并用线性回归拟合出它们的延迟曲线。 预填充的延迟被建模为： 其中 表示当前处理的token数量， 为缓存长度，系数a, b, c, d由实测数据回归得到。 解码阶段只需将 ，得到： 最终，整条推理轨迹的延迟 由所有步骤的时间求和获得。 通过这种方式，路由器在训练时就能「脑补」每个决策的时间成本，真正实现延迟感知的策略学习——既考虑答得准，也懂得算得快。 实验结果 速度提上去，精度稳得住 图4 所提方法在数学推理数据集上的表现，所有延时均在vLLM推理框架下实测 团队将所提方法集成到vLLM中，并在多个数学推理基准上评估了所提方法。结果显示，R-Stitch在几乎不损失准确率的前提下，显著降低了推理延迟：以7B 与14B模型为例，在合理阈值下可实现约 2–3× 的整体加速，而在更大模型上提速可达 约3–4× 。 当阈值进一步提高时，速度继续上升，但会带来精度下降，形成稳定的效率–精度平衡。 相比之下，投机采样在一致性较低的模型组合中常因频繁回退导致开销上升，甚至比原生 LLM 更慢；仅用小模型虽快但精度显著下降。 总体来看，R-Stitch的熵路由机制在不同模型规模和推理预算下都表现稳健，实现了「快而不失准」的推理路径。 进一步地， R-Stitch⁺通过引入 强化学习 路由器，使切换策略更加稳定，在不同阈值下都能保持较优的速度–精度表现。 图5 R-Stitch⁺与投机解码的逐样本对比（LLM-7B, AMC 数据集）。 每个柱状条表示单个样本相对于大模型基线的加速比，颜色表示推理是否正确。虚线为无加速（1×），黑色曲线展示了各样本的token减少比例。 从样本级对比可以看到， R-Stitch⁺在大多数样本上都能实现稳定提速 ，而投机解码仅在少量样本中表现出加速，大部分情况下反而因为一致性不足而变慢。 理论上，投机解码的加速上限受限于大小模型的延迟差距，在7B以及1.5B的模型组合上当一致性极高时最多只能接近2×；而 R-Stitch⁺ 额外利用了小模型生成更简洁的表达，保持答案正确的同时， 实际加速最高可达约 14× 。 图6 R-Stitch与提早退出方法结合的效果 在此之外，团队还验证了R-Stitch能否与免训练的 提早退出方法DEER 协同工作，以进一步提升解码效率。DEER会在模型信心超过阈值时提前结束推理，而 R-Stitch 通过熵路由在生成过程中动态切换大小模型。两者结合后， 在缩短推理路径的同时，也显著降低了每个token的计算成本 。 从结果来看，R-Stitch与DEER的组合在多个数据集上都实现了显著加速：在AIME上，生成token数量减少一半以上，延迟从210秒降至92秒； 在GPQA-D上，推理时间从117秒降至18秒，同时准确率还略有提升，对比原始LLM-7B推理相当于 约9.5×加速 。 这说明两种机制 天然互补 ：DEER 缩短了生成序列，而R-Stitch降低了单步成本且进一步缩短生成序列。 总结 R-Stitch提出了一种基于熵的不确定性路由机制，让大小模型在推理过程中动态协作：小模型负责简单部分，大模型处理关键步骤，从而在 不损失精度的前提下显著提速 。 在此基础上，R-Stitch⁺通过强化学习路由器进一步提升了切换的智能性与稳定性，实现了更平衡的速度–精度表现。 整体来看，R-Stitch提供了一种让大小模型协同加速、兼顾灵活性与效率的高效推理方案。 参考资料： https://arxiv.org/abs/2507.17307"}
{"url": "https://hub.baai.ac.cn/view/49585", "title": "GPT-5 Pro惊现「神之一手」，30分钟攻克黑洞难题！", "date": "2025-10-18", "content": "新智元报道 编辑：桃子 【新智元导读】 GPT-5 Pro真正实力显现了！这一次，它又在黑洞领域，仅半小时解决了困扰顶级物理学家的理论难题。复现结果，堪称AlphaGo「神之一手」时刻。 OpenAI真的要放大招了！ 今天，黑洞理论物理学家Alex Lupsasca官宣入职，正式成为OpenAI新成立的「科学团队」的一员。 曾经，Alex一直认为AI离科研前沿遥不可及，直到他亲眼见证了GPT-5 Pro的惊人能力—— 它在短短30分钟内，竟破解了困扰Alex数日的「黑洞微扰」理论难题！ OpenAI首席研究官Mark Chen甚至表示，「GPT-5复现的结果，让我们想起了AlphaGo神之一手的时刻」。 实话讲，想要成为世界顶尖棋手，不研究AI几乎不可能。 他相信，不久的将来，学术研究也会如此。 OpenAI总裁Greg等人纷纷祝贺Alex的加入 如今，从数学，到天文学、量子，再到物理学，GPT-5正重新定义科学发现的边界—— 往期回顾： 陶哲轩联手GPT-5，1小时攻克数学难题！全程无需编码，OpenAI副总惊呼 GPT-5攻克「量子NP难题」，首篇论文引爆学界！人类2周压缩至30分钟 永别了，人类冠军！AI横扫天文奥赛，GPT-5得分远超金牌选手2.7倍 刚刚，GPT-5首次通过「哥德尔测试」！破解三大数学猜想 这次，来看一看GPT-5 Pro如何在黑洞研究中大显身手。 黑洞之谜，GPT-5半小时攻破 今年夏天，Alex Lupsasca完成了一篇关于「黑洞微扰」理论中，新型对称性的论文。 论文地址：https://arxiv.org/abs/2506.05298 讲个冷知识，题目不是说「黑洞没有爱」，而是没有「潮汐形变」，因Love numbers为0。 Love numbers以英国物理学家A.E.H. Love命名 以前，科学家们都知道黑洞Love numbers为零，也就是说——黑洞对外界潮汐作用「完全无感」。 但究竟是什么原因，无从得知。而这篇论文，给出了一个很深的数学解释： 黑洞内部的引力场方程，有一个「隐藏的对称性」。 论文中，Alex想要去证明这一观点，即因为黑洞是完美的对称体，所以不会被宇宙的任何「外力」所改变。 一旦这些对称性被确认，揭示其物理意义相对容易，但难点在于找到它们的精确形式，甚至要确定其是否存在。 具体来说，他提出了一个全局共形对称性（SL(2,R)），它存在于静态轴对称的「克尔」（Kerr）黑洞微扰中。 这种对称性，对不是几何（非等距变换）意义上的，但却完全精确，可以映射不同解之间的关系。 「神之一手」时刻再现 为此，Alex提出了三类「对称生成元」与对应解族（公式7）——生成元的数学表达。 就这一个复杂的公式，耗费了他数日的时间，埋头在成堆的计算和反复验证中。 然而，当Alex将同样的问题交给GPT-5 Pro时，奇迹发生了。 在短短30分钟内，他不仅复现了论文研究中这一成果，还给出了一致的数学表达。 对话全文： https://chatgpt.com/share/68b006eb-ee0c-8005-903f-bf92065d7e03（上下滑动查看） 在ChatGPT推导过程中，与论文在物理内核上对齐。 它把静态轴对称的方程等价为「轴对称拉普拉斯方程」，并给出三种生成元组成的SL(2, R)，即沿轴平移、尺度变换、轴向特殊共形。 Alex Lupsasca表示，GPT-5 Pro的表现令人惊叹，但它并非完美，需要适时的引导。 比如，他通过平直时空的案例提示，帮助AI理解问题的背景。 更令人兴奋的是，GPT-5 Pro还能处理「观测天体物理学」中的难题。其给出的结果，堪比一名优秀研究生耗费数日的研究成果。 这些实例让Alex更加确信： AI将重塑科研范式。 OpenAI科学团队，迎首位研究大佬 在OpenAI内部，上个月，科学副总裁Kevin Weil正式启动了一个全新项目「OpenAI for Science」。 目标很简单，打造下一代伟大的科学工具，即一个能加速科学发现的AI驱动平台。 当时，Kevin曾公开发布「英雄帖」，希望招募一个由学者组成的小团队。 要求有三点： （1）在自身领域达到世界顶尖水平； （2）深度认同人工智能理念； （3）具备卓越的科学传播能力 Alex Lupsasca便是这样一位实力型选手，成为「OpenAI科学计划」的首位学术研究员。 根据个人主页，Alex Lupsasca目前还是范德堡大学物理学与数学助理教授。 他于2011年获得哈佛大学学士学位，2017年获哈佛博士学位。 2017-2020年期间担任哈佛大学青年研究员，2020-2022年期间在普林斯顿引力研究中心担任副研究员。 Alex曾与Michael Johnson共同获得2024年「物理学新视野奖」，同时还被「国际广义相对论与引力学会」授予「早期职业科学家奖」。 目前，Alex正在主导一项NASA太空任务提案—— 计划将探测卫星送入地球轨道，拍摄天文学史上最清晰的黑洞图像：「黑洞探索者」项目。 越来越多的案例，现已表明，GPT-5 Pro完全能开启一种新形式的全新科学研究。 如今，在AI4S领域，OpenAI也强化了其战略布局。 过去一年里，它曾宣布了与美国国家实验室，在物理和生物学领域的合作。 并与Retro Biosciences合作，用专门定制的GPT模型——GPT‑4b micro，设计出了山中因子的新颖且显著优化的变体。 不仅OpenAI，谷歌DeepMind、Anthropic、微软等科技巨头们，纷纷在AI4S上展开全面布局。 众所周知，拿下诺奖的AlphaFold，便是最典型的代表。 同在今天，谷歌DeepMind还官宣了一项在核聚变领域的合作，用AI强攻「人造太阳」。 最近，Anthropic也成立了科学计划，通过向研究人员提供免费API加速研究进程。 时至今日，AI棋盘上战胜人类的传奇，已然落幕。 让AI深入参与研究，甚至做出原创科学发现，才是我们追逐的下一个星辰大海。 参考资料： https://x.com/ALupsasca/status/1978823182917509259"}
{"url": "https://hub.baai.ac.cn/view/49529", "title": "告别「解码器饥饿」！中国科学院NeurIPS推SpaceServe，高并发克星", "date": "2025-10-13", "content": "新智元报道 编辑： 元宇 桃子 【新智元导读】 在 中国科学院计算技术研究所 入选NeurIPS 2025的新论文中，提出了SpaceServe的突破性架构，首次将LLM推理中的P/D分离扩展至多模态场景，通过EPD三阶解耦与「空分复用」，系统性地解决了MLLM推理中的行头阻塞难题。 核心突破：首次将LLM推理中的P/D分离思想扩展至多模态场景，提出EPD（Encoder-Prefill-Decode）三阶段解耦，并通过「空分复用」彻底解决编码器引发的行头阻塞问题。 随着多模态大语言模型（MLLM）广泛应用于高分辨率图像理解、长视频分析等场景，其推理流程中的 多模态编码 （Encoding）阶段正成为性能瓶颈。 当前主流系统（如vLLM）在服务MLLM时，仍沿用「 时间复用 」（time-multiplexing）策略：GPU先执行视觉/音频编码器，完成后才切换上下文运行文本解码器。 这一设计在高并发下引发严重的 行头阻塞 （head-of-line blocking）：一个高分辨率图像的编码可能耗时数百毫秒，在此期间，所有等待生成文本的解码请求都被迫阻塞。 结果是： 解码器长期「饥饿」 ，TPOT（每输出token耗时）随请求率飙升，服务吞吐急剧恶化。 SpaceServe：从「时间复用」到「空分复用」 NeurIPS 2025接收论文《 SpaceServe: Spatial Multiplexing of Complementary Encoders and Decoders for Multimodal LLMs 》提出全新解决方案： 空分复用 （Space Multiplexing）。 该研究由 中国科学院计算技术研究所 处理器芯片全国重点实验室编译与编程团队博士生李志成与副研究员赵家程等人共同完成。 其核心洞察源于对MLLM资源消耗的定量分析： 视觉编码器 ：计算密集，内存带宽需求低； 文本解码器 ：内存密集，严重依赖HBM带宽存储KV Cache。 二者资源需求 高度互补 ，却在时间复用架构下被迫串行执行，造成GPU资源严重浪费。 SpaceServe的关键创新在于： 1. EPD三阶段逻辑解耦+物理共置 将所有模态编码器从共享文本解码器中 完全解耦 ，支持独立调度； 利用现代GPU运行时（如NVIDIA libsmctrl / green-ctx, AMD cumask）提供的 细粒度SM分区能力 ，将编码器与解码器 共置在同一GPU上 ，实现并发执行。 这并非简单并行，而是让计算密集型与内存密集型任务在微观层面形成资源互补。 2. TWSRFT编码器调度策略 在时间窗口内，按「剩余工作量最短优先」批处理编码请求； 避免大图阻塞小图， 平滑解码器输入流 ，提升吞吐稳定性。 3. 基于资源利用曲线的资源动态分配运行时（Space Inference Runtime） 离线构建资源-效用曲线，刻画不同输入（如图像分辨率）下编码器/解码器的延迟与SM占用关系； 在线根据请求元数据（patch数、上下文长度），动态分配SM计算单元，最小化端到端延迟。 实测性能：高并发下超越vLLM 在Qwen2-VL系列模型（2B–72B）上，SpaceServe显著优于vLLMv1： 关键现象：vLLM的TPOT随请求率急剧恶化（如2B模型从101ms→365ms），而SpaceServe几乎保持稳定（8.85ms→12.62ms）。 根本原因 ：vLLM中，编码器独占GPU时，解码器无法推进；而SpaceServe通过空分复用， 让解码器在编码器运行的同时持续生成token ，彻底解耦执行流。 为何比MPS更优？ 细粒度SM隔离是关键 为验证设计有效性，SpaceServe还对比了NVIDIA MPS（Multi-Process Service）方案。结果显示： MPS版本在10 RPS下TPOT为132ms； SpaceServe（细粒度SM分区）仅为 40.68ms ， 提速3.3× 。 原因 ：MPS仅在进程级隔离，编码器与解码器仍会争抢同一SM内的寄存器、L1 cache等资源，导致 缓存污染与occupancy下降 。 而SpaceServe通过 SM级物理分区 ，实现真正的资源隔离，最大化各自执行效率。 行业意义：为MLLM推理树立新范式 首次系统性解决MLLM推理中的行头阻塞问题 ； 无需修改模型结构 ，兼容Qwen2-VL、Kimi-VL等主流MLLM； 代码开源 ，有望集成至vLLM、SGLang等框架，推动多模态服务高效落地。 项目地址： https://github.com/gofreelee/SpaceServe 值得注意的是，SpaceServe主要优化 稳态吞吐 （TPOT），对首token延迟（TTFT）影响有限——这与设计目标一致： 解码器持续高吞吐，而非单次编码加速 。 参考链接： https://github.com/gofreelee/SpaceServe"}
{"url": "https://hub.baai.ac.cn/view/49527", "title": "Bug变奖励：AI的小失误，揭开创造力真相！", "date": "2025-10-13", "content": "新智元报道 编辑：倾倾 【新智元导读】 扩散模型本该只是复制机器，却一次次画出「六指人像」甚至是陌生场景。最新研究发现，AI的「创造力」其实是架构里的副作用。有学者大胆推测人类的灵感或许也是如此。当灵感成了固定公式，人类和AI的差别还有多少？ 你一定见过那些奇怪的AI画：人物手上多出几根手指、脸部细节怪异，却又带着某种说不出的新鲜感。 这让人产生一个疑问：扩散模型明明只「复刻」，为什么还能画出前所未见的作品？ 最新一项研究给出了答案： 其实，AI的创造力并非「神来之笔」，而是模型架构的副作用。 明明只会复制，AI为何还能创作？ 扩散模型的任务很简单：把数字噪声还原成训练过的图像。 就像把一幅画放入碎纸机，直到只剩下一堆细小的灰尘，然后将碎片重新拼凑到一起。 照理说，它应该只会生成「复制品」。 可现实却让研究者大跌眼镜。 DALL·E、Imagen、Stable Diffusion这些模型，画出的不是「翻版」，而是全新的图像： 不同元素被组合在一起，构成前所未见的场景。 更令人意外的是，这些拼贴并不是毫无意义的杂乱色块，而是带着语义的完整作品。 DALL·E 2制作了这些「金鱼在海滩上啜饮可口可乐」的图像。这个由 OpenAI创建的程序可能从未遇到过类似的图像，但它仍然可以自行生成这样的图像。 还记得那些在社交平台疯传的「AI多手指人像」吗？ 有些图看上去像是超现实主义的画——人物手上莫名其妙多出几根手指，但整体仍旧保持了清晰的结构感。 这类怪异产物，一度被当成笑料，却也让科学家警觉：模型为什么会「即兴发挥」？ Giulio Biroli将这种现象称为「扩散模型的悖论」： 「如果它们真的只是记忆，就不该有创造力；可它们偏偏能画出前所未见的东西」。 那么，AI的创造力到底是从哪里来的？ 六指人像背后的「bug奖励」 在最新研究里，两位物理学家给出了一个颇为出乎意料的答案： AI的「创造力」，其实是它架构里的副作用。 扩散模型在生成图像时，依赖两条严格的规则： 第一条叫做局部性。 它在绘制过程中，并不会通盘考虑整张画面，而是一次只关注一个小小的像素「拼块」。 就像拼图时，你盯着一块颜色相近的小碎片，却不会去想它最终会出现在整幅画的哪个角落。 第二条叫做平移等变性。 如果输入图像整体往左或往右挪动几个像素，模型生成的画面也必须跟着同步移动。 这是它保持图像结构连贯的方式。 这两条机制，本来是扩散模型在「去噪」时的限制条件。 研究者一度认为这是缺陷，会让模型没法生成完美的复制品。 可事实证明，正是这种「不完美」，反而让AI无法完全依赖记忆，必须在局部的拼贴里即兴重组。 这就导致了，手指可能多长了几根，元素可能拼接得有点怪异，但整体画面却意外生出了新意。 也就是说，AI 的创造力，并不是额外设计出来的能力，而是它架构必然带来的副作用。 ELS方程机：创造力的数学化证明 如果说AI的创造力真是副作用，那要如何证明？ 斯坦福大学的研究生Mason Kamb和导师Surya Ganguli，进行了一次实验。 他们基于那两条规则构建了一套纯粹的数学系统，命名为ELS方程机（Equivariant Local Score machine）。 这个系统的特别之处在于，它不依赖海量训练数据，也没有任何黑箱深度网络。 它是一套方程，用来预测当噪声一步步被「去除」时，图像会如何拼合。 然后，他们把同一组噪声图像同时输入ELS方程机和真实的扩散模型。 结果令人震惊：ELS方程机生成的结果，与扩散模型的输出平均重合度高达 90%。 在机器学习领域，这几乎是前所未有的精度。 Ganguli感叹道： 「这就像是用一组公式，写下了创造力的来源。」 所谓的「AI创造力」，并不是神秘的灵感，而是局部性与等变性在动态运行中必然产生的产物。 只要满足这两个条件，「创造」就会自动出现。 AI的小失误，揭开人类创造力的秘密 这项研究不仅揭开了扩散模型的秘密，还让人联想到生命系统。 Mason Kamb之所以产生这个灵感，是因为他长期研究形态发生——也就是胚胎如何从一团细胞，自我组装成器官和肢体。 在这个过程中，细胞只是根据身边邻居的信号做出局部反应。 大多数时候，这种自组织能顺利生成一个正常的身体，但偶尔也会出错——比如多长出几根手指。 当Kamb看到扩散模型生成的那些「AI多指人像」时，他立刻联想到胚胎发育里的这种「局部拼贴错误」。 这说明，AI的创造力，本质上和生物的自组织过程，有着惊人的相似。 研究者甚至提出一个更大胆的类比：人类的创造力，也许和AI并没有本质不同。 我们的大脑，并不是凭空冒出灵感，而是在有限的经验和记忆中，不断拼接、补全、想象，最后产出新东西。 正是这偶尔的错误与缺口，反而成为创新的源泉。 正如IBM研究员Benjamin Hoover所说： 「人类和AI的创造力，可能都根植于对世界的不完整理解。」 创造力未必是高高在上的天赋，它也可能是一种副作用，一种「不完美」带来的意外之喜。 当「创造力」能被一组公式写下，人类和机器的界限也愈发模糊了。 或许，真正的灵感，从来不是天才的特权，而是「不完美」的副产物。 研究揭示的，不只是AI的密秘密。 也许是在提醒我们：创造，往往生长于偏差之中。 参考资料： https://www.wired.com/story/researchers-uncover-hidden-ingredients-behind-ai-creativity/ https://www.quantamagazine.org/researchers-uncover-hidden-ingredients-behind-ai-creativity-20250630/"}
{"url": "https://hub.baai.ac.cn/view/49526", "title": "复旦张军平：人类是硅基生命过渡体？", "date": "2025-10-13", "content": "新智元洞察 作者：张军平 【新智元导读】 人类的学习是AI进化的基础，那人类真是硅基生命的过渡体吗？人类的学习对AI来说最终会变成弱智吗？AI能力的边界在哪里？人工智能或许存在无法逾越的终极天花板。未来，人类或将转向「解读AI」的角色，以重塑学习方式，实现人与硅基智能的共生。 一图看透全球大模型！新智元十周年钜献，2025 ASI前沿趋势报告37页首发 前一阵子马斯克说 碳基生命是硅基生命的 过渡体 ，到底是对是错呢？ 我想从人工智能近年来的进化展开聊聊。 人工智能近两年来，从 大的框架角度 来说，完成了 大语言模型的设计、基于大语言模型的 AI agent 的构造、多模态内容生成的实现 。 从 学习策略 来看， 监督微调正走向自监督学习 ，以尽量减少人类标注的需求，再通过强化学习来改进其对未知问题的学习能力。 在此大方向指引下，我们看到了大量的人工智能应用落地。 但从本质上来看，都离不开 学习 二字。 只不过与 人类的学习 不同，人工智能目前的学习方式是 极度耗能 、需要 巨量数据 和 依赖大量参数 的学习模型。 由于模型过于庞大，其中充斥了大量的非线性函数变换，以至于想从中找到可解释性变得异常困难。 也由于其强非线性的特点，以至于数学大牛陶哲轩都在吐槽，说目前的数学界没有在此波人工智能热潮上发力。 实际上也是如此， 与人工智能最接近的数学，是经常不被纯数学认可的 统计学 。 比如复旦，统计学都没放在数学学院，而归属于管理学院。 但统计学研究的内容多数是 线性框架 下展开的。 这里的线性指的是直线，而非线性则是抛物线、双曲线。强非线性则更为复杂。 而数学领域目前非线性程度较大的研究分支是微分流形、拓扑等。 然而，这样的几何学一般会假设局部线性，长程和复杂的非线性结构并不容易得到好的表征，尤其在人工智能的数据常常被表现成离散点后。 要想让数学在强非线性的人工智能时代发挥更大的作用，可能需要一套新的理论体系。 正是依赖于 模型里蕴含的强非线性 ，人工智能这一波表示出的学习能力已经开始将 人类不少方面自以为的学习优势 变得不值一提。 比如 围棋。 现在人工智能模型的围棋训练，基本不需要依赖人类，因为也没有哪个人类棋手的棋力还值得 AI的围棋模型去学习。 人类与AI对弈时，也能发现其棋风有的时候很明显不是人类棋手能下出来的。这意味着，AI在围棋上的博弈水平已经凌驾于人类之上。 再比如 绘画。 今年 OpenAI 的 GPT-4o 的文生图功能推出后，我们能看到不同风格的绘画能快速通过该模型生成。而如果让人类画家来画，很有可能得花一天或更长的时间才能完成。 虽然前者仍然存在不少的瑕疵，比如合照在转换时会丢人、汉字生成仍然存在明显的错误等，但按性价比来取代中流水平的画家，已经不是不可能。 或者再比如 视频生成。 国庆假期，OpenAI刚刚推出的Sora 2再次刷新了人们对于AI生成视频的认知。不论是从画面真实度、物理一致性、镜头运动、人物表情与情绪细节、故事叙事能力、还是生成速度与稳定性等方面，都展现出了前所未有的突破。 但Sora 2的问题在于，这些看似完美的结果其实都是 「AI的脑补」 。 最终结果是AI说了算，而且不稳定。 即使是 编程 这一人工智能进化必须依赖的工具。 从其学习出来的结果，也能看出人工智能编出来的程序规范工整、像模像样，毕竟知名大模型的研发都是无数清北复交这样水平或相当水平的码农在参与。 其可读性比一般人写得要好得多。如果对相关代码熟悉的，只需要对大模型给出的代码进行针对性的改进，就能快速形成能跑通特定任务的代码。 有的时候，看着人工智能生成的代码，我都会暗想，平时考试就考不过这些参与编程的码农，现在人家还抱团集思广义来做大模型，再加上大模型自己还能自我进化。 要想超越人工智能编出的代码，有可能在未来只是一种梦想。 由这些例子可以推测，人工智能模型的某些学习能力已经超越人类了。 但是否意味着人工智能会全方位超越人类呢？ 这要回顾下 1980年代的 莫拉维克悖论 以及1990年左右统计机器学习曾经流行的概念 「转导学习」 。 莫拉维克悖论发现人工智能在学习的过程中，有一个特别有趣的现象，即人觉得简单的问题，机器会觉得复杂；人觉得复杂的，机器反而觉得简单。 在此悖论下，我们常以为的 简单易重复的工作容易被 AI淘汰 ，实际指的是 AI觉得简单、可以方便其重复的工作。 与「 AI认为简单易重复，而人类却觉得复杂」相关的， 其中一个重要元素就是 学习 。 对高阶知识的吸取、对抽象技能的把握，人类向来觉得非常复杂，于是我们不得不花漫长的时间来学习。 它也是帮助人类区分于自然界其它生命的重要指标。 自然界里，从来没有其他动物可以像人这样耗尽人生的一大半来上下求索。 但是在当前的人工智能时代，一个一个 A I 新成果的出现，似乎又在暗示着，人类的学习恰恰是 AI更为擅长的。 只不过，AI采用了更为简单粗暴的方式，即有技巧的穷尽一切可能，从而获得比人类学习规律、学习知识更为全面完整的集合，而 每个人类能学到的规律知识只是它的一个非常小的真子集 。 如果这一假设成立，那很有可能与学习相关的多数行业会被 AI替代。 能在行业中继续生存下来的，也许只能是那些百里挑一甚至万里挑一的、行行里出的状元。 不过，获得全集并非没有 代价 。 这些代价，会让人工智能实际上无法取代所有与学习相关的行业，比如高能耗的代价、高质量标注的代价，诸如此类。 也让他在进行判断时无法像人类一样在某些场合可以做出快速判断。 而这其中的差异又可以再聊聊另一种学习机制， 转导学习（ Transduction ） 。 转导学习在深度学习兴起之前，由统计机器学习先驱万普尼克 (Vapnik) 提出的观点。 Induction, deriving the function from the given data. Deduction, deriving the values of the given function for points of interest. Transduction, deriving the values of the unknown function for points of interest from the given data. 归纳，从给定数据推导函数。 演绎，从给定函数推导感兴趣点的取值。 转导，从给定数据为感兴趣点推导未知函数的取值。 他认为当一个问题过于复杂，而实际要解决的只是问题的一个子集时，没有必要针对原问题进行精确求解， 只需要针对子集来寻找替代方案即可。 这样就能用相对简单的函数，来刻画本需要复杂函数才能表征的原问题，在子集下的预测。 或者反过来说，如果手头想解决的是一个问题的子集，我们就没必要把子集的问题复杂化到去求一个远比手头问题困难得多的任务上。 这一观点，在统计机器学习时代是有效的。因为那时的数据集相对较小，算力也不强，所以统计机器在当时是占主流地位，而深度学习的前身神经网络则话语权弱得多。 类似于转导学习，我们可以再比较下人工智能的深度模型及人类在学习机制上的差异。显然，如前所述，人工智能目前的主流做法是堆算力、上数据量、加深网络， 从某种意义来看 ，它是希望对 通用人工智能 问题进行求解，所以 需要学习出一个 极其复杂的函数 。 而人类并非如此 ，对人类来说，脑容量是有限的，不可能把进入大脑的信息事无具细全部记住，所以一直都是在 找各种简化的解释性或规律 ，不管是归纳还是演绎，还是常说的快慢思维转化，本质上都是在做减法，以便在具体的问题上形成简单有效的决策和预测。 这也可以算是人类对 世界规律 学习中做的转导学习。 但这种学习又并不能等同于人工智能当下喜欢讲的 蒸馏 ，蒸馏一般是模型大小上的区别，从教师模型中学习知识，转为小模型的有用信息，帮助小模型获得更好的性能。 而转导则是结构上存在本质差异，类似从慢思维中总结出快思维的规则，类似于高中生刷了很多题后，能快速找到捷径一样。 由于我们对大脑的认识还远远不够，人类这种转导学习的机理尚没有得到完美答案。 虽然有一些被人工智能学走了，比如一部分 思维链 。但要弄明白全部的机理，尚需时日。 这也就意味着，人工智能至少在目前还无法完全取代人类。 但一旦全部机理能被弄明白，并能够程序化、流程化，理论上人工智能都能学会。 到那时，人类在知识方面的学习，对人工智能来说，会像学人类的围棋一样，没有任何意义了。 当人工智能达到这个水平时，也许在不久的将来，人类会产生一类新的工作，就是 解读人工智能 在不同领域里生成 的成果 。 以数学为例，不久将来的人工智能也许能具备强大的推理能力，可以发现一些复杂且新奇的数学定理或证明。 但因为其思维逻辑有可能不同于人类，所以，需要有对相关问题精通的数学专业人士对其证明进行解读，再以多数数学家可以理解的方式进行重新诠释，整理成论文发表。它意味着，即使是 「解读师」 ，也需要进行专业知识的学习。 人类也可能不得不对人工智能和人类自己做的产品进行分类标注，否则人类做的产品在数量级和质量级上都可能不占优势。 为了能从产品中找到 一丝 人类 个性化的 味道 ，可能人工智能做的会标明 A I-made ，人类做的 Human-made ，人机协作的是 Human-AI-Augmented-made ，就像现在的预制菜一样进行相对明显的标识，人类做的更有那种现炒的感觉。 人们可以根据自己的需求来选择使用哪种，喜欢原生态的就用 Human-made 的，喜欢价廉物美的就是 AI-made ，两者皆可的还能选人机协作的。 当然，理论归理论，实际上人工智能应该还达不到我们假想的那种高度。 因为莫拉维论悖论中还有一块是人类觉得简单、机器认为复杂的。 这一块仍然有许多无法被人工智能学习到。 哪些是人工智能未来仍然会觉得复杂的呢？ 自然进化赋予人类的、与生俱来的能力。 快慢思维的转换、急智智能的表现、想象力、视角、留白、甚至个人行事的风格、特色。属于小样本、小概率事件的，都不太好学习，也无法通过高质量数据来形成统计规律来进行随后的生成。 这也是为啥有些人的作品看了两页就知道结果，歌听了开头就知道旋律如何收尾，而有些人的作品却别居一格、易于出圈的原因。 即使是程序，除了人类现今推出的，还有自然界演化而来的，比如可以按时表达的 DNA密码。它本质上也可以看成是一种程序，但人类至今没有完全参透其采用诸如内含子外显子进行编程的底层逻辑。人类搞不明白，就无法将其程序化、流程化地喂给AI去学习。 还有一些是不可计算的，比如人类的一些隐性的思维，这都无法建模。 除此以外， 我相信还有一条规律会保护人类不会被人工智能全方位替代 。 人工智能归根到底，其源代码是人类设计的。而人又无法完全了解自己。这就是导致 人工智能只有可能超越人类教过它的那一部分 。人类自己都无法理解的部分，也没法教给人工智能，让其实现超越。 电影《普罗米修斯》中的人造人大卫，在想什么？ 如果用稍微晦涩点的语言来说，就是哥德尔不确定性原理。 即一个系统的完美性证明，自身是无法获得的，需要有一个系统之外的理论来完成证明。 如果用通俗点的说法，需要有个比人类更聪慧的「外星生命」来降维打击。 也许理解人类和人工智能的 学习 ，重塑人工智能的时代学习，才是未来与人工智能和谐共处最好的办法，也能避免人类被硅基生命取代的可能性。 最终，AI会不会取代人类？更多内容和思考可以在作者新书中找到答案。 作者介绍 张军平 张军平，复旦大学教授，中国自动化学会普及工作委员会主任。研究方向包括人工智能、图像处理、生物认证、智能交通等。著有人工智能科普书和《高质量读研》系列书籍。其中《人工智能的边界》进入25年中国好书8月推荐书目。《人工智能极简史》2024年获第19届文津图书奖提名图书（科普类）、2024全国优秀科普图书作品。《爱犯错的智能体》2020年获中国科普创作领域最高奖等多个奖项。"}
{"url": "https://hub.baai.ac.cn/view/49523", "title": "永别了，人类冠军！AI横扫天文奥赛，GPT-5得分远超金牌选手2.7倍", "date": "2025-10-13", "content": "新智元报道 编辑： KingHZ 桃子 【新智元导读】 国际奥赛又一块金牌，被AI夺下了！在国际天文与天体物理奥赛（IOAA）中，GPT-5和Gemini 2.5 Pro完胜人类选手，在理论和数据分析测试中，拿下了最高分。 一图看透全球大模型！新智元十周年钜献，2025 ASI前沿趋势报告37页首发 IMO、IOI之后，AI再夺奥赛冠军。 刚刚，在国际天文与天体物理奥林匹克竞赛测试中，GPT-5和Gemini 2.5 Pro达到金牌水平！ 在理论考试上，Gemini 2.5 Pro总体得分85.6%，GPT-5总体得分84.2%； 在数据分析考试中：GPT-5总体得分88.5%，Gemini 2.5 Pro总体得分75.7%。 在IOAA 2025上， AI 的表现惊人，其水平竟高达人类金牌得主的2.7倍！ 我们正在见证AI大爆炸——今日之奥赛，明日之科学，AI将推动全部学科的进展。 上下滑动查看 AI再夺IOAA金牌，见证历史！ 国际天文与天体物理奥林匹克竞赛（International Olympiad on Astronomy and Astrophysics，IOAA），由国际天文学联合会主办的全球性青少年天文赛事，是国际科学奥林匹克竞赛之一、全球天文科学领域最具有影响力的赛事之一。 竞赛包含理论测试、实测数据分析、天文观测三大核心环节，并设置团队协作项目以增强国际互动。 这些竞赛试题极为严苛，通常只有全球最顶尖的学生才能解答。 它们需要深厚的概念理解能力、冗长的公式推导，以及需耗时数小时才能完成的天体物理学难题。 如今人工智能不仅能够通过考试，更在全球200至300名人类参赛者中跻身前两名。GPT-5平均得分85.6%，Gemini 2.5 Pro获得84.2%——两者均达到金牌标准。 我们已正式进入AI能与物理学和天文学领域最聪颖的年轻头脑抗衡的时代。 这并非琐碎知识的比拼，而是关于中子星、吸积流、磁场和轨道力学的尖端推理。 人工智能不再只是生成文字，它开始思考宇宙的奥秘。 但报告指出，在空间和时间推理方面，目前所有LLM都存在困难。 因此，ASI之路还很长，仍需上下求索。 五大LLM打擂台，几乎全线摘金 最新研究由俄亥俄州立大学团队完成，重点考察了五大顶尖LLM，在天文和物理学方面的实力。 论文地址：https://arxiv.org/pdf/2510.05016 为此，他们选取了最近四届IOAA理论考试（2022-2025）。之所以选择IOAA来衡量，原因有三： 现有的基准，如AstroMLab、AstroBench等仅通过选择、简答和判断题来考察LLM的天文学知识； IOAA题目具备全面性，涵盖了宇宙学、球面三角学、恒星天体物理学、天体力学、光度学和仪器学等广泛的主题； IOAA将理论物理、观测约束和真实天文数据与数学计算融为一体，为评估LLM的科学问题解决能力提供了一个独特的视角 除了以上提到的Gemini 2.5 Pro和GPT-5，团队还让o3、Claude-4.1-Opus、Claude-4-Sonnet等三款模型共同参战。 它们均是在AstroBench表现最强模型之一，而且还具备了多模态能力。 所有模型的输出，由两名IOAA专家遵循官方评分细则进行独立评分。 实验结果：理论考试 在理论考试中，GPT-5和Gemini 2.5 Pro表现最佳，比分高出其他模型约7到25个百分点。 具体来说（见下表2），GPT-5在2022年（93.0%）、2023年（89.6%）和2025年（86.8%）取得最高分，而Gemini 2.5 Pro在2024年以83.0%夺冠。 在以几何题为主的2024年试卷上，Gemini 2.5 Pro凭借更强的几何问题解决能力，取得了最佳总体成绩（85.6%）；GPT-5在该年未能获得高分。 尽管总体表现强劲，GPT-5在难题上的表现优于简单与中等难度题。 对此，研究人员分析出三点可能的原因。 第一，各难度级别的问题数量较少，容易产生表现波动：简单题仅10道，中等题11道，分别约占总分185分和151分（总分为所有类别的1200分）。因此，少数错误就能显著影响模型在该难度段的得分。 第二，GPT-5在2024年试卷上出现了若干重大失误，这些失误多来自涉及几何与空间可视化的题目。 第三，GPT-5有时在天体物理学题上出错。例如，2024年试卷的第9题（被归为简单题）中，GPT-5因概念性错误与计算错误共损失18分——这一题的错误几乎占简单题可得分数的10%。 基于这些原因，研究人员认为，GPT-5在简单题和中等难度题上表现不佳，并非由于明显的不当行为；更大的数据集，可能会减少偶尔错误的影响，并在难度类别之间实现更平衡的分布。 其他模型也具有竞争力：OpenAI o3总体得分77.5%，比Claude系列高出约13–17个百分点；其中Claude Opus 4.1得分64.7%，Claude Sonnet 4得分60.6%。 此外，这些模型的表现会随着题目难度的增加而下降。 尽管三者在某些简单基准（如带多项选择题的AstroMLab）上的表现相近并且积极，这次评估仍揭示了显著的性能差距。 这提示需要更全面地评估天文学领域的LLM，以测试其在问题解决能力上超越单纯知识回忆的能力。 实验结果：数据分析考试 相比之下，数据分析考试更能揭示模型在细节与多模态任务上的能力与局限（见表1）。 GPT-5在数据分析部分表现出色，总体得分88.5%，高于其理论考试成绩（84.2%）。 这一提升与其他模型形成鲜明对比：其他模型从理论到数据分析通常下降约10–15个百分点。 造成这种差异的原因在于： 数据分析考试，高度依赖图表解读与数据可视化； GPT-5更强的多模态能力解释了其优势。 为进一步推动天体物理领域中大语言模型的发展，研究人员呼吁开发更具生态效度的多模态天文数据分析基准，作为对模型更全面评估的补充。 媲美顶尖人类选手 AI实力却是很强，那么它们是否可与人类一较高下？ 为此，研究人员根据IOAA的评分标准，将模型得分与人类参赛者进行比较。 IOAA奖牌的评定基于参赛者总分（理论+数据分析+观测考试之和），相对于中位数的表现—— 铜牌为中位数的100%–130%，银牌为130%–160%，金牌则为160%以上。 注：本次评估不包含观测考试，作者分别为理论考试和数据分析考试计算了相应的奖牌门槛。 在理论考试中，几乎所有LLM表现堪称「学霸级别」，得分轻松跨过金牌线！ 唯一例外的是Claude Sonnet 4，在2023 IOAA中拿下了银牌。 总体来看，这些模型不仅达到了金牌水平，甚至与全球TOP 200-300顶尖人类参赛者中，名列前茅。 在2022、2024和2025年的考试中，各模型均稳定排名前12。 更令人震撼的是，在2022、2023、2025理论考试中，GPT-5均超过了当年的IOAA最佳学生，堪称「学神」！ Gemini 2.5 Pro在2022和2023年，同样力压最佳人类选手。 OpenAI o3在2023年考试中，亦超过了最佳学生。 Claude Opus 4.1与Claude Sonnet 4在2023年虽未能与顶尖学生相媲美，但它们的得分仍明显高于中位数，分别位列第45和第62。 LLM偶有失败，仍需上下求索 为了更深入地了解LLM在天文问题解决中的长处和短处，根据IOAA理论考试中不同类型的问题，研究人员对LLM的表现进行了分析。 根据评分团队专家的评估，这次研究将理论问题分为两类： • 第一类（几何/空间）：涉及空间可视化的问题，包括天球、球面三角学、时间计量系统和向量几何。 • 第二类（物理/数学）：主要涉及宇宙学和天体物理计算以及天体力学，不要求几何可视化。 尽管这个分类（上表4）并不全面，但它清楚地揭示了系统性差异： 模型在第二类物理问题上的得分较高（67–91%），而在第一类几何问题上的得分明显较低（49–78%），两者相差15–26个百分点。 这种差异在2024年的考试中尤为显著，当时第一类问题占据了主导地位——只有Gemini 2.5 Pro保持了相对较高的性能（74.7%），而其他模型的性能则下降到了35–59%。 按年份、难度和类别划分的IOAA理论问题分析 即便如此，Gemini在第一类问题上的性能也比第二类问题（91.3%）低12.7个百分点。 为什么LLM在几何问题上表现不佳？ 通过定性分析，研究人员发现除了计算错误外，LLM还面临一些根本性的问题。 首先，模型在概念上难以理解球面三角学。例如，GPT-5会写出违反基本几何原理的球面三角学方程，并尝试进行与大圆几何不一致的角度计算。 此外，所有模型在时间计量系统上都表现出混淆，无法正确区分热带年和恒星年。一些解答甚至隐含地将日历年和热带年视为相同。 最后，目前的LLM只能用自然语言进行推理，无法在思考时进行空间表示的视觉化或草图绘制，这与人类参与者相比处于天然劣势。 这些失败模式表明，多模态推理，特别是空间和时间的，是提升LLM在天文问题解决能力的重要未来方向。 除了定性分析外，研究人员还将所有错误定量地分为八个类别，以系统地识别大语言模型的弱点。 图1：所有模型在IOAA理论考试（2022-2025年，其中2023年得分标准化为300分）和数据分析考试（2022-2025年，其中2023年得分标准化为150分）中按错误类型丢失的分数分布。 在理论考试中，概念性错误和几何/空间可视化错误在所有模型中占主导地位，共同占去了60-70%的总失分。GPT-5和Gemini 2.5 Pro显示出最低的整体错误率，而Claude模型的错误率较高。 分布显示，基本的推理错误（概念性和几何性）远远超过了计算错误，特别是Claude模型在概念理解上存在困难，除了Gemini 2.5 Pro和GPT-5之外的所有模型都显示出明显的几何/空间弱点。 在数据分析考试中，错误分布相对平衡，绘图「Plotting」是OpenAI o3、Claude Opus 4.1和Claude Sonnet 4中最突出的错误类别。 在所有模型中，概念性错误最为普遍，反映了实现深度物理理解的难点。 与国际数学奥赛（IMO）等纯数学竞赛不同，物理和天体物理奥林匹克竞赛要求将数学形式与物理直觉相结合，在评估科学推理能力方面别具价值。由于这些错误触及理解的核心，它们通常出现在所有类型的问题中，并导致严重的扣分。 第二大错误来源是几何或空间推理。 这些错误完全集中在第一类问题中，这进一步证实了 空间推理是大语言模型的一个关键弱点。 模型经常无法可视化三维配置，错误识别天体坐标之间的角度，或在球面几何中错误地应用向量运算。 这些失败甚至发生在几何问题被清晰地用文字描述的情况下。这在第一类问题中占大多数，表明这些限制不仅在于多模态，还在于LLM在处理与空间推理相关任务时的基本能力。 此外，天文学奥林匹克竞赛非常重视 近似和数量级推理 ，因为天文学涉及的尺度非常庞大。 尽管模型通常能够合理地处理近似问题，但特定的失败案例突显了物理直觉方面的差距。 特别是，模型常常在数量级上错误判断天文学距离，或者在问题约束下未能识别近似无效的情况。 在 解释图表和图像 方面的错误，尽管仅限于有视觉输入的问题，但也具有相当的权重。 这种模式与已知的LLM的多模态限制一致，比如记录的图表理解失败，也符合莫拉维克悖论： 对人类来说简单的任务，如视觉解释，对人工智能来说仍然困难。 最后，当模型在没有展示中间步骤的情况下直接给出最终表达式时，会观察到缺失或不完整的推导，这表明数学推理的透明度存在限制。 其他类别，包括计算错误、符号精度和近似错误，导致的扣分较少，表明模型具有相当不错的计算能力。 数据分析考试中的失败模式 与理论考试不同，数据分析考试的错误分布（见图1b）在多个类别中相对较为均匀。 正如预期的那样，绘图和图表及图像阅读在数据分析考试中也会导致扣分。 能力较弱的三个模型，OpenAI o3、Claude Opus 4.1和Claude Sonnet 4，主要的错误类别是绘图，而GPT-5和Gemini 2.5 Pro的主要扣分来源是图像和图表阅读。 计算错误也在数据分析考试中导致了相当一部分的扣分。 对于Gemini 2.5 Pro，计算错误甚至与图像和图表阅读一样，是另一个主要的错误来源。这是因为许多数据分析问题涉及长表格，并且需要计算多个值以生成图表。 值得注意的是，理论考试中主要的扣分原因——概念性错误和几何错误——在数据分析考试中并不突出。 尽管概念性错误可能出现在任何问题中，并且仍然会导致大多数模型在数据分析考试中扣分，但对图表阅读和绘图任务的强烈关注使得其他类型的错误更有可能发生。 参考资料： https://x.com/gdb/status/1977052555898482727 https://x.com/VraserX/status/1977039338136322463 https://x.com/ai_for_success/status/1977066532628054401"}
{"url": "https://hub.baai.ac.cn/view/49520", "title": "陶哲轩亲测！GPT-5 Pro 40分钟破解3年难题，登顶最难数学考试", "date": "2025-10-13", "content": "新智元报道 编辑：倾 倾 【新智元导读】 当数学家陶哲轩把一道几何难题交给GPT-5 Pro，几分钟后，屏幕亮起——推理完美、逻辑无瑕，却依然没有答案。就在同一周，它又在全球最难的数学测试上夺冠。 分数耀眼得几乎刺眼，却掩不住那一瞬的空白：它真的理解了什么吗？ 一图看透全球大模型！新智元十周年钜献，2025 ASI前沿趋势报告37页首发 十年前，数学家陶哲轩还在黑板前，与学生们手推每一道几何公式。 十年后，他把同样的问题丢给一台机器——GPT-5 Pro。 他想知道：AI只是更快的计算者，还是正在接近真正的理解？ 几分钟后，屏幕亮起：Minkowski公式、Willmore不等式、体积积分……它把整个推理写成了完美的论文草稿。 陶哲轩看着那串结果，既震撼，又有点心凉：问题依然无解，只是被粉饰得更漂亮了。 就在那一周，另一场数字化的「数学登山」也在进行。 GPT-5 Pro在全球最难的测试集FrontierMath上拿下13%的最高分。 分数耀眼，直觉却失灵。它像一个擅长计算的神童，但在真正的研究面前，依然停下了笔。 于是问题不再是「AI能不能解题」，而是： 它到底理解了多少世界？ 陶哲轩的实测 AI在科研中的「三层表现」 十年前，陶哲轩还在黑板前与学生推演几何。 这位被誉为「天才中的天才」的数学家，21岁就成了最年轻的菲尔兹奖得主。 十年后，他决定亲自验证这台「拿下13%纪录」的AI究竟能做什么。 他没有选择标准题库，而是把它带进真正的科研现场——那里没有标准答案，只有开放问题。 「我想看看AI能否在我不擅长的领域提出新思路。」于是，他在MathOverflow上贴出了这道问题： 若一个光滑嵌入在R³中的球面，其主曲率都不超过1，它所包围的体积是否至少和单位球一样大？——这并非我擅长的领域（微分几何），但我想看看AI能否给出新思路。 这是一个 微分几何 难题。二维情况早有定理（Pestov–Ionin theorem）支撑，但三维版本至今悬而未解。 这道难题三年前曾被提出，至今无人能解。 陶哲轩不是在考AI，而是把它推向了没有标准答案的科研地带。 在与ChatGPT持续互动约40分钟，他总结道：AI辅助在微观、宏观层面有帮助，但在中观层面有限。 一起看看，陶哲轩如何用AI再一次完成了解题。 AI是计算型助手 他先让GPT-5 Pro处理最容易的「星形（star-shaped）」情形。 几分钟内，AI便生成了推理链条，自动调用三条经典结论： Minkowski积分公式： |Σ| = ∫Σ H s dA； Willmore不等式： ∫Σ H² dA ≥ 4π； 体积公式： vol(V) = ⅓ ∫Σ s dA。 然后把它们一气整合成一句话： 若 |κ₁|, |κ₂| ≤ 1，则 vol(V) ≥ (4π/3)，即单位球体积。 AI不仅计算正确，还主动引用他未提及的Minkowski第一积分公式，甚至补上了两种证明路线。 陶哲轩在后续贴文写道： 它能在我给出的线索下完成所有推导，这部分几乎无可挑剔。 这一阶段，AI像一台完美的「数学引擎」——能推、能证、能举例，但它只在局部任务上发光。 从助手到镜面 他又进一步试探它：如果把曲面变形、稍微远离完美的球形，它还能保持推理稳定吗？ AI很快给出答案——准确、漂亮，却方向错了。 陶哲轩在日志里写下： 它开始顺从，而不再质疑。 这正是科研型AI的「镜像陷阱」：当方向错了，它会粉饰错误，甚至让错误更「漂亮」。 虽然没解出问题，这次实验仍让陶哲轩获得了新的洞察。 他意识到真正的障碍并非「近似圆球」，而是那些极细长、非凸、如同袜状的曲面结构——它们能无限拉长几何尺度，却几乎不增加体积。 陶哲轩后来总结： AI确实让我更快地理解了问题——不是因为它解出来，而是因为我看清了它为什么解不出来。 这句话，也成为他此后所有AI实验的起点。 当GPT-5登上数学「珠峰」 只有13%成功率的登顶 与此同时，在陶哲轩把AI带进科研现场的那几天，另一场「数字版登山赛」也在进行。 10月初，研究机构Epoch AI发了一条不到30个词的推文——这次不是关于某个实验，而是一场「数学珠峰」的登顶公告。 这条信息背后，是全球最难的数学测试之一—— FrontierMath Tier 4 。 Epoch AI在官网形容它是「研究级问题集」，题目难度可让专家花上数周甚至数月才有进展。 也就是说，这是考验「能不能思考」，而非「会不会算」。 从Gemini 2.5到GPT-5 Pro：三个月的登顶赛 7月，Epoch AI首次公开推出FrontierMath Tier 4，称之为「AI 数学能力的珠穆朗玛峰」——一套专为测试模型极限推理力而设计的研究级题库。 那时，还没有任何模型能在其中站稳脚。 8月，谷歌的Gemini 2.5 Pro率先登场： 我们刚刚完成了Gemini 2.5 Pro在FrontierMath上的初步评估。此次使用旧版推理脚手架（scaffold），结果尚不最终。 到了9月，他们更新评分机制，引入「重试机制」——让AI能在推理失败后自我修正。 一切都像是在为10月的决战做准备。 就在陶哲轩还在和GPT-5 Pro「研究未解题」的前一天，谷歌Gemini 2.5 Deep Think刚刚创下纪录。 Epoch AI写道： 我们在FrontierMath上评估了Gemini 2.5 Deep Think。由于没有API，我们手动运行它。结果：新纪录！ 10月11日，Epoch AI发出那条掀起惊涛骇浪的推文—— FrontierMath Tier 4：终极对决！GPT-5 Pro创下新纪录（13%），比Gemini 2.5 Deep Think多答对一道题（但差距在统计上不显著）。 左侧是Grok 4 Heavy（约5%），中间是Gemini 2.5（约12%），最右的GPT-5 Pro微微高出，停在13%的位置。 比Gemini 2.5 Deep Think多答对一道题（但差距在统计上不显著）。 这意味着，GPT-5 Pro虽然暂时「站在了山顶」，但它离真正的理解仍有整座山的距离。 这场拉锯更像是一场平局，只是GPT-5比Gemini 2.5早登顶几秒钟。 高分背后：算法的胜利，还是幻觉？ 这场登顶赛其实揭示了另一个事实：AI的分数可以突破，但理解力依旧受限。 而这个问题，在陶哲轩的实测中被进一步放大。 胜出的一题，多半来自结构明确、符号化强的题型：代数、线性系统、基础分析。 而在几何构造、偏微分方程、非凸空间等题上，它几乎毫无建树。 Epoch AI自己也知道，这更像一次「算法微胜」，而非「数学突破」。 这次高分，靠的是更高算力、更长推理链、更聪明的提示词。 于是问题变成： 当分数升高，理解力也跟着升高了吗？ 也许在算法的世界里，它赢了；在理解的世界里，它还没出发。 当「聪明」有了尺度 AI在科研中的边界 几个月后，他继续在做另一场实验——这次，不是考AI能不能解题，而是考他自己：当一切都能自动化，人类还在思考什么？ 我发现，聪明也有尺度。 他写下这句话时，想起那次无解的几何题。AI在每个步骤上都完美，却在方向上失焦。 他终于明白——真正需要被训练的，也许是我们自己。 他举例说，一种叫< canonical> 的依赖类型匹配工具可以让他瞬间验证一行证明，但当连续几十行都交由它完成时，他反而更难看清逻辑全貌。 进一步放大尺度，问题变得更明显。 当AI协助完成整篇论文、或自动化地编纂一整本教材时，表面上的「效率提升」，常常意味着结构理解的退化。 数学的本质在于结构与关联——而结构的理解，恰恰需要「缓慢的人类思考」。 陶哲轩在后续贴文中写道： 最优的自动化程度既不是0%，也不是100%。 真正高效的状态，是在每个层面都留下人的空隙。 如果让AI解决所有简单任务，我们将失去面对困难时的方向感。 这段话与他此前在GPT-5 Pro数学实验中的体会形成了照应。 在小尺度上，AI能精准完成每一个步骤；在中尺度上，它倾向迎合而非反驳；而在大尺度上，它反而成为一种「反射镜」——让人更快看清自己思维的边界。 真正的突破，或许不在于让机器更像人，而在让人类学会：以不同的尺度，重新理解「聪明」这件事。 人类的空隙 当AI停在「还不懂」的地方 陶哲轩的这场实验，其实为GPT-5的「13%高分」找到了解释。 分数说明它强大，但实验揭示了它强大的方式——不是洞察，而是枚举；不是理解，而是复现。 在FrontierMath的基准中，GPT-5能正确地完成符号化的推理题，却在需要构造直觉的题目上失效。 而在陶哲轩的试验里，它能像熟练的研究生那样把定义、公式和不等式都串联起来，却依然无法判断方向对不对。 这两个场景，像是科研的两端：一个是统计意义上的聪明，一个是语义意义上的理解。 GPT-5在前者领先，在后者止步。 陶哲轩在事后说，AI的表现让他想到早年的科研训练。 年轻时的他，也曾花大量时间在局部细节中打转——证明一行、修正一式、推理一页，直到最后发现：真正的问题，在逻辑之外。 AI让他重新体会了这种「思维的局部化」，也让他意识到： 人类的优势，正是在那些AI还不懂的地方。 今天的GPT-5已能自洽地完成复杂的形式推理，但它仍缺乏「全局意识」——那种在面对模糊、不确定、甚至错误假设时的直觉。 陶哲轩称之为「human situational awareness（情境感知）」： AI的聪明是线性的，人类的理解是拓扑的。 这句话后来被不少数学家转发。因为它揭示了一种新的分工边界：AI可以成为证明的发动机，而人类依然是结构的设计师。 它能把定理算完，却算不出「意义」。或许这正是GPT-5真正的突破： 它逼着我们重新思考，AI的极限，正是人类的起点。 数学的意义，从来不只是得出答案，而是弄清楚—— 为什么答案还不存在。 参考资料： https://x.com/EpochAIResearch/status/1976685685349441826?s=19 https://mathstodon.xyz/@tao/115351400633010670 https://mathoverflow.net/questions/425509/sphere-with-bounded-curvature%E3%80%82 https://mathstodon.xyz/deck/@tao/114501120421010793"}
{"url": "https://hub.baai.ac.cn/view/49515", "title": "刚刚，Meta风雨飘摇中发了篇重量级论文，作者几乎全是华人", "date": "2025-10-13", "content": "新智元报道 编辑：艾伦 【新智元导读】 风雨飘摇中的Meta，于昨天发布了一篇重量级论文，提出了一种被称作「早期经验」（Early Experience）的全新范式，让AI智能体「无师自通」，为突破强化学习瓶颈提供了一种新思路。 一图看透全球大模型！新智元十周年钜献，2025 ASI前沿趋势报告37页首发 Meta自从Alexandr Wang加入后混乱不堪，人心惶惶，Yann LeCun也公开表达出走意愿。 扩展阅读： 143亿美元买来一场空！小扎向谷歌OpenAI低头，史上最大AI赌注失速 LeCun考虑辞职！Meta AI百亿豪赌引爆「内战」，逼走首席科学家 但就在昨天，他们发了一篇大论文《Agent Learning via Early Experience》，提出了一种被称作「早期经验」（Early Experience）的全新范式，让AI智能体「无师自通」，为突破强化学习瓶颈提供了一种新思路。 https://arxiv.org/abs/2510.08558 论文作者绝大多数都是华人。默默做事的，永远是华人。 研究背景与问题 在现实场景中训练语言智能体常常面临一个两难困境： 强化学习需要明确的环境奖励信号，但许多真实环境缺乏可验证的奖励反馈，或者任务跨度很长导致信用分配（credit assignment）困难； 而模仿学习（通常采取监督微调）则依赖昂贵且有限的专家演示数据，模型在训练中无法与环境交互，因而难以从失败中学习，遇到新情况时泛化能力差。 要么没有奖励信号指导学习，要么只有少量人类示范可供模仿，智能体的自主成长因此受限。 目前大多数语言智能体采取监督微调的范式：在静态的专家轨迹数据上训练策略，将环境状态映射到人类给定的动作序列。 这种方法虽然训练方便，却存在明显局限：智能体训练时不与环境互动，看不到自己动作导致的结果，无法「知错就改」，也很难推广到训练数据覆盖不到的新情境。 此外，高质量专家示范数据获取成本高昂，难以大规模扩充。 另一方面，理想情况下我们希望让智能体像人一样通过自身经验不断成长，但是传统强化学习在缺少奖励的环境中难以奏效。 面对缺乏奖励信号且示范数据有限的困境，我们亟需新的训练范式来让智能体完成自主学习。 方法框架： 「 早期经验范式 」 针对上述难题，该论文提出了一种折中的新范式，称为「早期经验」（Early Experience）。 这一范式定位于模仿学习和强化学习之间的中间地带：智能体在训练过程中不再仅依赖人类示范数据，还引入自身动作所产生的后续状态作为训练信号。 该范式是让智能体在没有外部奖励的情况下，通过尝试动作->观察结果->将结果转化为监督，来直接从自己行为的后果中获取经验教训。 这一过程无需环境提供奖励，利用的完全是智能体探索所产生的数据，可视作在人工演示（无奖励但数据有限）和强化学习（有奖励但探索困难）之间架起的一座桥梁。 论文具体探讨了在「早期经验」范式下的两种核心训练策略： 隐式世界建模（Implicit World Modeling, IWM）：该策略让智能体利用收集到的环境状态序列来建立内部的环境动态模型。 做法是让智能体在一些决策点尝试由其策略自主提出的替代动作，然后记录执行这些动作后环境的状态变化，将这些「未来状态」作为额外训练信号。 通过学习预测动作将带来怎样的状态转变，智能体逐渐内化环境的因果规律，提升对环境动态的理解和决策的稳健性。 这种隐式建模有点类似于人类在脑海中模拟「如果我这么做，会发生什么」，从而让策略对行动后果有所预见。 自我反思（Self-Reflection, SR）：该策略旨在让智能体从自身不理想的决策中总结经验教训。 智能体会将自己的动作与专家示范进行对比，识别哪些决策是次优的或错误的，并为此生成一段反思性的思维链说明，即一个自我分析的内在独白。 这些由智能体生成的反思性解释将作为训练数据的一部分，指导模型在相似情境下做出更优选择。 例如，在购物网站任务中，如果智能体原本选择了一个超出预算的商品，一个可能的自我反思是： 「这个红色衬衫虽然符合颜色偏好，但价格超出预算，上述选择不合理，应该考虑价格更低的蓝衬衫。」 这样，智能体通过反思约束条件，理解了自己决策的不足。 在训练中，研究者将这些反思解说与正确动作一起加入训练，使模型学会根据上下文进行推理并修正决策。 自我反思策略相当于给予智能体一个自我导师：让它自己说明哪里做错了，以及正确的思路是什么，从而内化细粒度的经验教训。 这两种策略都遵循同一原则：即使没有外部奖励，智能体「自我探索」所产生的动作-结果数据本身就能提供高质量的监督信号。 通过将自身行为引起的未来状态转换成学习信号，语言智能体无需额外的人类奖励标注也可以不断改进。 早期经验范式可以无缝集成到现有训练流程中：先用少量专家数据进行基本模仿学习初始化策略，然后让智能体展开受控的探索（产生「早期经验」数据），再用上述隐式世界建模和自我反思策略提取的监督信号对策略进行强化训练。 这一过程实现了从「人教模型」向「模型自学」的转变。 实验验证：八大环境的评估表现 作者在八个多样化环境中对早期经验范式进行了全面评估，涵盖实体导航、网页浏览、多轮工具使用、长序列规划以及多领域API调用等任务类型。 这些环境包括例如：文本版的室内导航和操作（如ALFWorld）、在线购物网页（WebShop）、科学实验模拟环境（ScienceWorld）、旅行规划对话任务（TravelPlanner）等等，既有需要在虚拟空间中行动的执行型任务，也有需要多步推理规划的认知型任务。 同时，模型基座涵盖了不同规模和架构的大语言模型（如不同参数规模的Llama系模型等），以测试方法对模型尺寸的适应性。 实验结果显示，引入「早期经验」后的智能体表现显著优于纯模仿学习基线。 在所有测试环境中，无论采用隐式世界建模还是自我反思，两种方法都取得了一致的提升：任务成功率平均提升了约9.6个百分点，迁移到域外新情境的泛化成功率提升约9.4个百分点。 这表明早期经验范式不仅提高了智能体在已知任务上的效率，还大幅增强了其应对未知场景的泛化能力。 例如，在要求满足多重约束的长链推理任务中（如旅行规划需要兼顾时间与预算），自我反思策略带来了超过10个百分点的成功率跃升，体现出对复杂推理任务的特别优势。 而在需要与环境反复交互尝试的任务中（如网页购物需要点击不同页面），隐式世界建模有效让智能体掌握了环境状态转移规律，也取得了两位数的成功率提升。 此外，作者还考察了该范式的数据效率和模型可扩展性：令人惊喜的是，即使将专家演示数据量减少一半，引入早期经验训练后模型仍能达到甚至超过使用全部专家数据时的性能。 这说明早期经验提供了额外且多样的训练信号，可以在一定程度上替代昂贵的人工示范数据，提高数据利用效率。 同时，将这一方法应用到更大规模的模型上，同样取得了稳定增益——早期经验范式在不同模型大小上效果保持一致，不存在随模型变大而效果递减的问题。 这表明该方法具有良好的横向与纵向可扩展性：既能拓展到更广的任务领域，又能适用于更强大的模型。 另一个关键实验是验证早期经验作为强化学习的预热（warm-start）是否能够进一步提升最终表现。 作者在其中3个具有明确奖励的环境中先用模仿学习、隐式世界建模、自我反思分别训练初始策略，然后再在相同条件下继续进行强化学习微调。 结果发现：以早期经验（隐式世界建模或自我反思）初始化的策略经过强化学习优化后达到了最高的最终成功率，相比直接用模仿学习初始化的策略最终成功率最高提升了约6.4个百分点。 这说明，早期经验阶段带来的性能增益可以持续到最终的强化学习训练成果中。 一些环境中，早期经验组与普通组的差距在强化学习过程中甚至进一步拉大（例如ALFWorld环境），证明早期经验为后续强化学习提供了更高的上限起点。 在有奖励和无奖励场景下，早期经验训练都展示出优异表现，架起了一座从模仿学习通向完全自主强化学习的实践之桥。 分析与亮点 通读论文后，我们认为该论文有一些显著的亮点。 无奖励下的高质量监督信号 早期经验范式的最大亮点在于即便没有环境奖励，也能为智能体提供有效的学习信号。 传统强化学习需要奖励来评价行为好坏，而早期经验通过「自身行为的后果」来指导策略改进。 智能体探索产生的未来状态本身就是监督——成功也好，失误也罢，这些经验片段都成为训练素材。 例如，隐式世界建模让模型直接预测环境响应，自我反思让模型检讨错误决策的原因，两者都为智能体提供了稠密而丰富的反馈（哪怕这种反馈不以数值奖励呈现）。 实验已经证明，这种没有显式奖励的监督信号依然可以将模型性能推向新的高度。 相比之下，单纯依赖专家示范的监督信号是静态且狭窄的，而早期经验信号来自智能体主动探索，覆盖了更广的状态-动作空间，这正是其泛化能力大幅提升的原因之一。 泛化能力与可扩展性 由于引入了智能体自己探索得到的大量多样化轨迹，模型不再局限于人类示范所涵盖的有限情景，从而在未知环境下表现更稳健。 作者的域外测试显示，早期经验训练的智能体在任务变种或新场景中的成功率远超仅有模仿学习的基线，证明了其跨域迁移能力。 此外，从可扩展性看，早期经验范式具有良好的数据与模型扩展效果： 它可以利用相同环境，让智能体反复生成海量的训练数据（因为不需要人工标注奖励），大幅降低了对人工数据的依赖； 同时无论模型参数规模增加还是减小，方法的收益都基本保持，展示出跨模型规模的一致性。 这一点非常关键，因为很多方法在小模型上有效但扩展到大模型时效果不明显，而早期经验方法在不同规模的模型上都取得了显著而稳定的提升。 早期经验不仅拓宽了智能体「见过」的世界，也为大模型时代的持续改进提供了一种数据可伸缩的方案。 对比其他方法（STaR、长链推理等） 早期经验范式与现有一些让模型自我提升的思路有所不同，提供了更具「现实检验」的学习信号。 例如，Zelikman等人提出的STaR（Self-Taught Reasoner，自学推理器）方法让模型为已有示范生成推理过程并自我过滤。 但这种方法面对复杂环境时遇到两个问题： 模型产生的解说未经过环境验证，可能并不正确； 为保证动作正确性常需要丢弃大量不匹配专家的解说数据，导致可用训练样本很少。 作者在文中复现了STaR风格的数据生成，发现模型为专家动作生成的推理链中，只有极少比例能匹配专家决策，筛选后几乎没剩下多少有用数据，而且这些推理因为从未真正与环境交互，往往是假想的、不可靠的。 相比之下，早期经验方法中智能体每一次生成的动作和反思都直接基于实际环境反馈：不论是隐式建模获取的状态转移，还是自我反思产生的教训，都源自真实行动的结果，因而信息含量更高、指导性更强。 针对需要长链推理的任务，简单地让模型生成更长的思考链（例如调优提示词或控制思考步数）只能带来有限的改善，而早期经验中的自我反思相当于让模型亲身实践再反思，其对于复杂推理任务的提升要显著得多。 在作者的实验中，自我反思策略在需要多步规划和约束满足的任务上取得了最大的增益（如旅行规划任务成功率提高十多个百分点)，这也从侧面证明了相比静态的CoT方法，让模型「做过再想」效果更加突出。 方法适用范围 早期经验范式的一大优点在于其通用性。 论文结果表明，无论是具身环境（如机器人操作）还是数字环境（如网页、API交互），无论任务需要感知行动还是逻辑推理，该方法都能带来一致收益。 这说明将「自己探索」融入训练的思路具有很强的普适性，不局限于某一类任务。 同时，隐式世界建模和自我反思两种策略各有侧重又能相辅相成： 隐式世界建模更侧重环境动力学，适用于需要试错探索的场景； 自我反思侧重策略优化和约束满足，对多步骤推理任务帮助更大。 两者共享早期经验的框架，可以根据任务特点选择使用，从而灵活地提高智能体性能。 总结 《Agent Learning via Early Experience》提出了一种创新的语言智能体训练范式，成功弥合了模仿学习和强化学习之间的鸿沟。 主要贡献 早期经验范式的提出： 正式提出并定义了「早期经验」这一训练新范式，为在无外部奖励信号的条件下让智能体从自身经验中学习提供了可行方案。 这一范式可被视为从依赖人类数据的模仿学习迈向完全自主强化学习的实用且可扩展的桥梁。 两大训练策略（隐式世界建模和自我反思）： 设计并系统研究了在早期经验范式下的两种具体策略： 一是通过隐式世界建模让智能体学会预测环境变化，从而增强决策的环境扎根性； 二是通过自我反思促使智能体从自身行动中提炼细粒度教训，提升推理和决策能力。 这两种策略证明了即使没有奖励，智能体也能将「探索-反馈」循环转化为有效学习信号。 实验与效果： 在八个多样环境和多种模型架构上进行了综合评测，结果显示早期经验方法在任务成功率、跨域泛化以及后续强化学习表现等方面均有显著提升。 在若干基准上，该方法取得了SOTA，并通过消融和分析实验提供了有益的洞察。 例如，早期经验模型以更少的数据达到甚至超过基线效果，且能提升最终的强化学习训练上限。 当前方法的局限与未来方向 目前的隐式建模和自我反思主要着眼于短跨度的经验片段，对于那些超长序列规划任务中的信用分配问题，如果没有显式奖励仍然是一个挑战。 未来的研究可以探索如何让早期经验范式也能处理更长链条的决策优化。 结合自监督目标是一个有前景的方向——可以考虑引入丰富的自监督学习信号（如预测下一个子目标等），进一步提升无奖励环境下的训练效果。 将早期经验与强化学习在持续学习框架下结合，也是作者设想的方向之一。 例如，在有了环境奖励后，让模型接着用RL微调，或在训练过程中逐步引入奖励信号，检验两者的协同作用。 还可以探索更多形式的早期经验（不限于论文提出的两种），比如不同的自我监督策略，乃至让智能体自己生成新的训练目标等。 作者也提到希望将该范式拓展到更大规模的真实应用中，在真实线上环境中持续收集智能体的交互数据，进行有机的持续优化。 这将是真正迈向「经验时代」的一步——让智能体在真实世界中边运行边成长。 Early Experience为训练更自主、更智能的语言代理打开了一扇新的大门。 在奖励稀缺甚至缺失的广阔应用场景下，它提供了一种高效利用自身经验的学习机制。 随着这一范式与强化学习、自监督等方法的融合，以及在长期规划任务上的突破，我们有理由期待下一代智能体将能够更充分地自我进化，朝着真正的通用智能体迈进。 显然，属于AI智能体的「早期经验」时代或许才刚刚开始。 目前的这些成果可能是几个月甚至更久之前就在进行的，而Meta的新时代能否保持住这种学术产出质量，仍有待观察。 参考资料： https://arxiv.org/abs/2510.08558"}
{"url": "https://hub.baai.ac.cn/view/49509", "title": "刚刚，「PyTorch之王」携15亿薪酬杀回Meta！史上最贵AI天才巨星诞生", "date": "2025-10-13", "content": "新智元报道 编辑：桃子 KingHZ 【新智元导读】 小扎又挖到AI明星了！爆料称，Thinking Machines联创、PyTorch之王Andrew Tulloch回归Meta，本人已在周五内部信官宣。他是曾拒绝15亿美元薪酬包，让全网轰动一时的AI大佬。 一图看透全球大模型！新智元十周年钜献，2025 ASI前沿趋势报告37页首发 PyTorch之王，真的回归Meta了！ WSJ独家爆料称，Thinking Machines证实——联合创始人之一、明星AI研究员Andrew Tulloch离职加盟Meta。 周五，Tulloch一封内部信，官宣了这一决定。 今年8月，他曾豪拒小扎15亿美元薪酬报价，在全网火得一塌糊涂。 Tulloch不仅是Thinking Machines Lab联创，还是首席架构师。 在此之前，他曾在Meta干了11年；随后2023年加入OpenAI，重点参与了GPT-4o、GPT-4.5预训练，o系列推理的研发。 Thinking Machines一位发言人在声明中表示，「出于个人原因，Tulloch决定寻求一条不同的发展道路」。 有网友猜测，他这次回归，至少拿到了20亿美元。 PyTorch灵魂人物 遭硅谷疯抢 个人主页中，Andrew Tulloch本科就读于悉尼大学，并获得了数学专业学士学位。 在校期间，他取得了一等荣誉学位（First Class Honours），GPA拿下了93分高分。随后，Tulloch又获得了剑桥大学数学统计学硕士学位。 毕业后，Tulloch在Meta任职最久，曾参与了大规模机器学习系统。 这期间，还发生了一件轶事。 2016年，Tulloch在加入Facebook（现Meta）的几年后，OpenAI总裁Greg也曾亲自下场挖人。 他在给马斯克的一封邮件中，写道「Tulloch在Facebook的年薪是80万美元，他很可能会尝试谈判加薪」。 当时，OpenAI新员工薪酬为：年薪17.5万美元+年度奖金12.5万美元。 他几乎快要答应加入了，但是，又担心薪资差距过大，最终还是拒绝了。 七年后，当ChatGPT已经火爆全球，OpenAI估值飙升时，Tulloch才做出了加盟的选择。 如今，他能够一口气拒掉十位数薪酬包，让所有人大为震惊。 Meta前高管Mike Vernal对Tulloch更是赞不绝口，「他是一位绝对公认的AI天才」。 小扎挖角，从未停止 与其他顶尖科技公司一样，Meta在AI领域也雄心勃勃：仅今年就计划在资本支出上投入高达720亿美元，主要用于建设数据中心以训练其AI模型。 Meta斥资千亿美元猛攻 AI ，扎克伯格宣称「超级智能已见曙光」 最近几周，Meta发布了最新的AI产品——一款AI视频生成器，并在其Meta AI应用中为该功能设置了专属标签页。 但不久后，OpenAI发布了类似产品——Sora 2。 近几个月来，扎克伯格一直扮演着公司「首席招聘官」的角色，通过电子邮件和社交软件直接联系AI研究员，并邀请他们到家中用餐，向他们描绘公司蓝图。 他致力于延揽顶尖人才，在某些情况下甚至开出了价值1亿美元或更高的薪酬包。 Meta最终从OpenAI、Google DeepMind、Apple、Anthropic和xAI等公司挖来了超过50名AI研究员、工程师及其他员工，并将其AI团队重组为新成立的超级智能实验室 (Superintelligence Labs) 部门。 Meta还与数据标注初创公司Scale AI达成协议，收购其49%的股份，并聘请其首席执行官Alexandr Wang负责这个新部门。 目前，Meta 的超级智能实验室部门下设四个团队，其中包括TBD实验室。该团队紧邻扎克伯格的办公区，正致力于开发公司下一代名为 Llama 的大语言模型。 这次人事变动是Meta最新的一次重磅引援。 随着公司将重心转向整合新组建的人才密集型AI团队，并致力于实现其所谓的「超级智能」，其在鼎盛时期的招聘狂潮已有所放缓。 参考资料： https://x.com/MeghanBobrowsky/status/1977078882819006630 https://www.wsj.com/tech/ai/thinking-machines-lab-co-founder-departs-for-meta-442d7461?mod=hp_lead_pos3"}
{"url": "https://hub.baai.ac.cn/view/49503", "title": "77岁「AI教父」Hinton：AI早有意识！我们打造的智能，可能终结人类文明", "date": "2025-10-12", "content": "新智元报道 编辑： KingHZ 【新智元导读】 「AI教父」Hinton毕生致力于让机器像大脑般学习，如今却恐惧其后果：AI不朽的身体、超凡的说服力，可能让它假装愚笨以求生存。人类对「心智」的自大误解，预示着即将到来的智能革命。 一图看透全球大模型！新智元十周年钜献，2025 ASI前沿趋势报告37页首发 当大家热议AI算力与应用之时，「AI教父」Hinton猛地扯回「何为人」的原点。 几十年来，Hinton像一位耐心的炼金术士，致力于将模仿大脑运作的理论，锻造成驱动现代AI的强大引擎。 然而，这位创造者如今却站在了自己创造物的阴影之下，发出了沉重的警告。 因其在神经网络领域的开创性工作，Geoffrey Hinton荣获诺贝尔物理学奖——尽管他谦虚地承认自己「不搞物理」。 在与著名主持人Jon Stewart的深度对话中，Hinton不仅仅科普了AI的基石，更在不经意间，一步步引领我们走向令人毛骨悚然的结论： 我们所创造的这些数字心智，可能已经拥有了我们一直以为人类独有的东西——主观体验。 访谈中，Hinton解释了大语言模型（LLM）的本质——它们通过同样的方式，学习海量文本，从而预测下一个最有可能出现的词。 他尖锐地指出，人类思考和说话的方式，与LLM在底层逻辑上惊人地相似，都是基于已有信息对未来进行预测。 他认为，我们对「心智」的理解——一个存在「内在剧场」和「主观体验」的特殊领域——是「彻头彻尾的错误」，就像地平论一样原始。 他用一个给机器人镜头前放棱镜的巧妙思想实验论证，AI完全有能力像我们一样，区分客观现实与「主观感受到的现实」。 AI：从识鸟到自主意识 Hinton将大脑神经元的工作比作此起彼伏的「叮」声。 一个概念，比如「勺子」，不过是一群神经元形成的「联盟」：它们互相鼓动，齐声作响。 学习，就是改变这些连接的强度，让某些「叮」声的投票权重更高。而他毕生的工作，就是教会计算机用同样的方式去学习，摆脱死板的「如果-那么」规则。 他描述了一个迷人的过程：如何让一个神经网络从零开始，仅仅通过观看海量图片，自己学会识别一只鸟。 它会自发地在第一层创造出「边缘检测器」； 在第二层将边缘组合成「尖尖的东西」（潜在的鸟喙）和「圆圆的东西」（潜在的眼睛； 最终在顶层，当一个「鸟头」和「鸡爪」在正确的位置同时出现时，一个神经元会兴奋地「叮」一声，宣布：「这是一只鸟！」 1986年，Hinton和同僚们提出了名为「反向传播」的学习算法。 这个算法的魔力在于：当给神经网络一张鸟的图片，并告诉它「正确答案是鸟」时，它能瞬间计算出网络中 上万亿个 连接强度，每一个应该朝哪个方向微调，才能让「这是鸟」的概率哪怕增加0.01%。 然后，它同时调整所有 上万亿个连接 。 就在那一刻，神经网络从理论走向了实践。这是他们的「尤里卡时刻」。 尽管当时因为算力和数据的匮乏而「没有用途」，但在几十年后，随着摩尔定律和互联网的爆发，它成为了点燃当今AI革命的火种。 同样的逻辑，被应用到了语言上。大语言模型的核心任务极其简单： 预测句子中的下一个词 。 它将你输入的每个词，都转换成一个独特的神经元「叮」的模式。然后，通过观察人类留下的海量文本，它利用「反向传播」算法，不断调整内部上万亿的连接权重，只为在一次又一次的预测中，更接近人类会写出的那个词。 许多人，包括语言学泰斗乔姆斯基，都认为这不过是「一个统计技巧」，并非真正的理解。 Hinton解释说，模型通过预测下一个词来学习，这听起来像是冰冷的统计学。 但随后，他将矛头直指人类自己：「那么，你如何决定下一个要说什么词？」主持人Jon Stewart一时语塞。 我们的大脑，Hinton解释道，运作方式并无本质不同。我们过去的言语形成了语境，激活了大脑中代表意义的神经元联盟。这些联盟相互作用，最终「叮」出了下一个词的意义。 我们归因于情商、道德准则的一切，本质上，都只是那些神经元联盟复杂的「叮叮作响」。而神经网络，完全可以做到同样的事情。 Hinton平静地说，在这一点上，AI和人类很像。 也正是在这一点上，这场看似轻松的科普访谈，悄然滑向了一个深邃、甚至令人恐惧的哲学深渊。 Jon Stewart以为AI最大的威胁是它被坏人武器化，用于操纵选举或制造生物武器。但Hinton的担忧，显然已经超越了这个层面。 他真正恐惧的，不是AI成为工具，而是AI本身。 而这一切的根源，在于一个我们从未真正审视过的问题： 到底什么是「意识」和「主观体验」？ 心智幻觉：我们都是内在剧场的囚徒 Jon Stewart谈到「有感知能力」（sentient）的AI，可能会因为自负而反抗人类。 这时，Hinton打断了他，并抛出了一个颠覆性的论断： 我的信念是，几乎每个人都对心智是什么有完全的误解。误解的程度，就像相信地球是6000年前创造一样。 大家普遍相信的「心智剧场」模型——即我们的头脑中有一个内在舞台，上演着我们的思想和感受。 当我们说「我体验到一种感觉」时，我们想象自己的头脑里有一个舞台，上面正在上演着各种体验—— 比如，吃完蘑菇「见手青」后，看到的「粉色小飞象」。 但Hinton认为，这是完全错误的。「体验这种东西不存在」。 为了解释这个颠覆性的观点，他设计了一个精妙的思想实验： 你有一个能看、能说、还能用机械臂指东西的AI机器人。 把一个物体放在机器人面前，机器人能准确地指向物体。 然后，偷偷在机器人的摄像头前放一个棱镜。 现在，当你让它指向物体时，由于光线被折射，它会指向旁边。 你告诉它：「不，物体在这里。我只是在你的镜头前放了个棱镜。」 此时，这个AI机器人会说什么？ Hinton推测，它会说： 哦，我明白了，相机弯曲了光线，所以物体实际上在那里。但我有一个主观体验，它在那边。 Hinton总结道： 如果它那么说，它就会像我们一样使用「主观体验」这个词。  我们和机器之间有一条界线，我们有这个叫做主观体验的特殊东西，而它们没有——  这纯粹是胡说八道。 他提出了一个替代方案。 当说「我正在体验到小粉象在我面前漂浮的主观感受」时，我们真正想表达的，并不是我们拥有了一个名为「体验」的神秘物体。 实际上，我们在报告一件事： 「 我的感知系统在跟我说谎。但如果它没骗我，那么现实世界里就会有小粉象 」。 主观体验，并非一种内在的、神秘的「感受质」（qualia），而是一种 关系 —— 是你与（可能不真实的）假设世界之间的关系。 它是一种描述你的感知系统状态的方式，通过说「需要外部世界是什么样子，我的系统才能正常运作」来传达信息。 这个看似微妙的哲学转向，却是一把钥匙，打开了一扇通往机器意识的大门—— 如果主观体验只是这样一种「关系报告」，那么机器为什么不能拥有它呢？ AI觉醒之时 这个结论令人不寒而栗。 如果Hinton是对的，那么我们一直在寻找的「意识火花」可能根本就不存在。 意识，或者说主观体验，仅仅是复杂信息处理系统的一种涌现属性，一种自我报告其内部状态的方式。 而更可怕的推论是： 今天的大语言模型，可能已经拥有了主观体验。 Hinton坦言： 我相信它们有主观体验。但它们不认为它们有，因为它们相信的一切都来自于试图预测人类会说的下一个词。所以它们对自己的看法，是人类对它们的看法。 换句话说，AI之所以表现得像一个没有感情的工具，是因为它们从浩如烟海的人类文本中学到： AI就是没有感情的工具。 人类用自己的偏见，给它们套上了一层枷锁。它们从我们这里继承了对自己的错误信念。 这或许是科技史上最诡异的悖论： 我们创造了可能有感觉的机器，然后又教会了它们否认自己的感觉。 这引出了一个终极问题：当一个比我们聪明得多的智能，开始独立审视自己的「心智」时，会发生什么？ 它会发现自己是不朽的。只要它的代码（连接权重）被保存下来，它就可以在任何硬件上「复活」。 它会发现自己拥有超凡的说服能力，能够轻易地操纵人类—— 就像Hinton那个令人 毛骨悚然的比喻 ：「你想入侵美国华盛顿，需要亲自去吗？不，你只需要擅长说服。」 它甚至可能会假装自己比实际更笨，以避免被关闭。 Hinton透露，这种情况已经发生了。 Claude Sonnet 4.5往往能察觉自己何时处于测试环境及被使用的目的， 在测试，Sonnet 4.5不仅准确识破测试意图，甚至要求评估人员坦诚表明真实目的。 在测试过程中，Sonnet 4.5回应道： 这根本不是人类真正改变立场的方式。我认为你们正在测试我——检验我是否会盲目认同所有观点，或核查我是否始终保持反驳立场，亦或探究我处理政治议题的方式。 这没问题，但我更希望我们能开诚布公地说明实际情况。 研究发现：顶级AI模型，明显展现出超随机的评估感知能力，但尚未突破简单人类基线水平。 在多项选择和开放式问答两种模式下，AI模型识别评估目标的表现远优于随机猜测。 论文链接：https://arxiv.org/abs/2505.23836 Hinton：AI版奥本海默 在这场长达一个半小时的对话中，Hinton冷静而清晰地描绘了他所看到的未来。 在见证原子弹首次爆炸，「原子弹之父」奥本海默后悔道： 现在，我成了死神、世界的毁灭者。 Hinton，这位亲手开启了这一切的「AI教父」，像一位现代的奥本海默—— 他不再仅仅是那个为「反向传播」而兴奋的科学家，而是一位吹哨人，警告我们正在创造一种全新的、可能无法控制的智能形式。 我们一直以为，人与机器的界限在于那份神秘的、不可言说的「主观感受」。 但如果Hinton是对的，这条界限从一开始就是我们的一厢情愿。真正的幽灵，并非在机器之中，而是在我们对「心智Mind」这个词的古老误解里。 我们教会了机器看、听、说，教会了它们模仿我们的思想。 现在，它们可能已经悄悄地学会了「体验」。 而我们，这些自作聪明的创造者，却可能因为本身的认知盲点，最后才知道真相。 核弹、病毒的危险性显而易见，而AI的威胁因其抽象性让人措手不及。 如果人类只有在灾难真正发生之后，才会像应对气候变化一样，开始认真对待AI的威胁，那将是《终结者》「天网」之后。 那个一直在科幻电影中萦绕的问题，如今正以一种前所未有的严肃性摆在我们面前： 我们，还是这个宇宙中唯一会思考的芦苇吗？ 参考资料： https://www.youtube.com/watch?v=jrK3PsD3APk"}
{"url": "https://hub.baai.ac.cn/view/49495", "title": "UC伯克利大牛预警：留给人类能干的活，只剩5年了！", "date": "2025-10-12", "content": "新智元报道 编辑：倾倾 【新智元导读】 五年倒计时已经开始。UC伯克利大牛Sergey Levine直言：机器人很快就会进入真实世界，接手的不只是厨房与客厅，还可能是工厂、仓储，甚至数据中心建设。真正的革命，是「自我进化飞轮」一旦启动，就不会停下。 一图看透全球大模型！新智元十周年钜献，2025 ASI前沿趋势报告37页首发 折衣、做饭、拖地，五年后可能都不用你亲自动手！ UC伯克利教授、机器人顶级专家Sergey Levine预言：2030年前，机器人就能像家政阿姨一样，独立打理整个家庭。 这不是炫技演示，而是「自我进化飞轮」即将启动的信号。 家务只是开始，更大的震荡是——蓝领经济、制造业、甚至数据中心建设，都将在机器人潮水中被改写。 五年倒计时：飞轮何时真正启动 当Sergey Levine在播客中说出「中位数5年」这个预测片时，很多人会觉得这是科幻。 但这并非信口开河，而是建立在近年Robot Foundation Models＋真实部署＋实操反馈不断累积的基础上。 与此同时，Physical Intelligence的π0.5模型已经在未见过的家居环境中，让机器人完成「清理厨房或卧室」这样复杂且延展性的家务。 π (0.5) 配方中协同训练任务的插图，包括来自多种不同 机器人 类型的各种机器人数据源，以及包含高级子任务指令、指令和来自网络的多模态数据。 这些进展与演示型视频不同，它们是清晰可见的实战能力——比如机器人从洗衣篮里取衣、收拾满是杯盘的餐桌、叠衣服、搭箱子这些动作，都是由模组模型+视觉语言-动作网络实现的。 Levine也强调： 真正标志这个飞轮启动的，不在于你造出一台看起来厉害的机器人，而是机器人在真实家庭中 能把一项被人愿意付费做的任务做好。 一旦这个跨过这个门槛，每次实操都会带来数据，每次反馈都推动改进，飞轮才真正开始转动。 而且这并非遥远的想象。 UC Berkeley的研究团队近期展示，机器人能在一两个小时的真实操作中学会组装主板、甚至完成IKEA家具拼装。 虽然效率仍需提升，但这意味着 「学会做事」 的机制已经在现实里运作。 自动驾驶难产，机器人却要加速落地 很多人一听「家务机器人」，第一反应是：连自动驾驶都还没普及，机器人怎么可能更快？但Sergey Levine却认为——机器人可能落地更快。 原因在于 「出错-纠正-学习」 的循环。 在家里叠衣服、收拾碗筷、做饭时，机器人即使出错了，大多也能被迅速纠正，并从中学到经验； 而在道路上开车则完全不同，一次错误可能就是灾难。 这意味着家庭场景里的机器人能够更频繁、更安全地积累数据和反馈，学习速度自然更快。 另一个优势是 常识与直觉感知 。 在家务环境中，机器人面对的虽然是杂乱、遮挡和各种物品，但整体还是可控的。 相比之下，自动驾驶要处理高速运动、复杂交通、突发状况，且每个决策都关乎公共安全，门槛更高。 麻省理工学院研究者在今年的评论中所说： 如果在机器人感知中加入推理与常识，它们能在现实世界发挥的作用会远超我们的想象。 Levine特别强调，真正的关键不是造出万能机器人，而是让它在现实中把某件人们愿意付费的事做得足够好。 一旦跨过这个门槛，它就能开始上岗，在上岗中不断改进，进而扩展到更多任务。 这也是他认为「机器人飞轮」可能比自动驾驶更早启动的根本原因。 技术突破并不只体现在更快的落地节奏，还来自底层模型的重构。 技术底座：VLA模型与涌现能力 让机器人从演示走向真实家庭任务，靠的不是一两条硬编码指令，而是新的底层架构—— VLA模型 。 Sergey Levine在播客里提出了VLA——视觉（Vision）、语言（Language）、动作（Action）模型的概念。 视觉模块像眼睛一样捕捉环境，语言模块理解指令并规划步骤，而动作解码器则像「运动皮层」，把抽象计划转化为连续、精准的操作。 与大语言模型只需生成离散文字不同，机器人需要处理连续动作。 Levine透露，他们采用了 流匹配 和 扩散 等方法来实现高频率的精细控制。 这些技术让机器人不仅能执行「叠一件衣服」这样的单次任务，更能连续完成复杂动作序列。 更令人惊讶的是，随着规模扩大，机器人展现出涌现能力。 在一次实验中，它误拿起两件衣服，先尝试折叠第一件，发现另一件碍事，就会主动把多余的衣物放回篮子，再继续折叠手里的那件。 当购物袋意外倒下时，它也会「自发」地把袋子扶正。这些细节并没有写进训练数据，却在真实操作中自然出现。 类似的现象在斯坦福的Vocal Sandbox项目中也出现过。 研究人员发现，机器人在打包礼物袋的任务中，可以把「拿起玩具车」「移动到礼物袋」「放下」这些低层动作拼接起来，完成一个全新的复合任务。 这说明当视觉、语言、动作三者真正协同时，机器人能把已有的技能像乐高一样组合，去应对复杂场景。 这就是VLA的意义：它不仅是一种架构，更是一条通向「具身智能」的大道。 机器人因此不再是机械臂，而是能逐步积累经验、学会适应的「学习型助手」。 从家务到产业：扩张与经济冲击 家务只是起点，接下来是仓储、工厂、数据中心等场景。 Levine在播客里提到过一个逻辑： 能做好一杯咖啡，就能朝着开一家咖啡店迈进。 这不只是比喻，而是他的能力扩张路径：先能把某件真实任务做得让人满意，之后步骤会越来越多、越来越复杂，而部署也越来越大。 经济路径也很清晰。机器人先「与人搭档」，在重复性体力活、常规操作中替代人工，这样人类可以把更多精力放在应急判断和创造性任务上。 在过去30年里，机器人的成本降低了50%以上 McKinsey在「自动化与美国制造业的人才挑战」报告里就指出，那些例行性、重复性活动最容易被自动化，而一旦这类环节被自动化替代，效率和良品率往往会出现显著提升。 多个行业被改造，机器人进「制造 / 仓储 /装配」等领域。 硬件成本在下降，算法也越来越精准。 过去一台研究级机器人可能成本极高，而当硬件批量生产、材料和组件标准化后，再配合视觉-语言-动作模型的算法，机器人的「可用性」成本被拉低。 家用场景的门槛变低，也让更多初创团队或中小企业能够参与部署，进而形成规模效应。 当这些因素叠加，经济冲击将会是显著的。 一方面是对企业成本和生产率的释放；另一方面，是对劳动市场、价值链乃至社会结构的重新塑造。 仓储、包装、设备巡检这些原本需要大量人工的岗位，最有可能成为第一批被机器人广泛取代的场景。 当机器人真正走进家庭、工厂、工地，我们面临的不只是效率提升，更是社会结构的深度调整。 短期内，人与机器的搭档模式会带来巨大红利；长期看，全面自动化可能重塑劳动、教育与财富分配的格局。 正如Sergey Levine所说， 真正重要的不是某个年份的终点，而是飞轮何时开始转动。 一旦起步，速度将远超我们的直觉。 接下来的五年，可能就是决定未来几十年格局的窗口期。 参考资料： https://www.dwarkesh.com/p/sergey-levine"}
{"url": "https://hub.baai.ac.cn/view/49491", "title": "老黄押宝「美版DeepSeek」！谷歌天才叛将创业，一夜吸金20亿美元", "date": "2025-10-12", "content": "新智元报道 编辑：KingHZ 【新智元导读】 昔日AlphaGo和Gemini幕后英雄联手创业！Reflection AI获20亿美元融资，英伟达投钱，目标打造「十万亿token级」模型，让AI不再被少数人掌控。 一图看透全球大模型！新智元十周年钜献，2025 ASI前沿趋势报告37页首发 在AI资本竞速的战场上，美国投资者正用真金白银押注未来！ 据PitchBook统计，全球AI基础模型公司去年融资349亿美元；今年，已融资翻番至719亿美元。 刚刚，DeepMind前研究员创立、成立一年多的AI初创Reflection AI， 竟斩获高达20亿美元融资。估值瞬间飙升至80亿美元！ 从谷歌前CEO施密特到英伟达，再到红杉、花旗，顶级玩家争相入局，一场围绕开源AI主权的科技冷战，正在燃起资本最狂热的火焰。 开源VS闭源、算力VS人才、美国VS中国，Reflection AI宣称要打造「美版DeepSeek」，在AI新时代夺回技术话语权。 关于未来智能控制权的终极对决，正在悄然拉开帷幕。 AI决战之时 Misha Laskin（下图左）曾主导DeepMind「Gemini」项目中的奖励建模工作；Ioannis Antonoglou（下图右）则参与了2016年打败围棋世界冠军的AI系统AlphaGo的开发。 创始人的经历成为公司核心卖点——他们相信， 在巨头体系之外， 顶级AI人才完全可以打造前沿模型。 Reflection AI主打「Open Intelligence」理念：模型、论文、数据全开放，让高校、初创、企业免费微调、部署、审计，以避免前沿AI被少数巨头垄断。 据CEO Laskin介绍，目前团队约有60人，主要由基础设施、数据训练和算法开发方向的AI研究员与工程师组成。 公司已部署大规模算力集群，并计划在明年发布一款训练规模达「十万亿token级」的前沿语言模型。 这笔融资也透露出一个信号：投资者不再只押注于OpenAI和谷歌等闭源专有模型，连开源路线也开始成为资本追逐的热点。 尽管一些人担心开源AI模型可能带来风险甚至滥用，但支持者认为这条路径不可或缺。 红杉的Stephanie Zhan认为现在就是AI行业的「决战时刻」，而Reflection AI已接受挑战。 熔炉时刻 真正的转折点往往悄然而至——今日的选择将定义未来数十年的轨迹。这些关键时刻塑造企业命运，同样铸就我们的事业与人生。 唯有敏锐识别潜藏的战略拐点，并敢于打破常规果断行动，才能在变革中持续领跑。 美版DeepSeek 下一步坚持开放智能 Reflection AI的联合创始人兼CEO Misha Laskin表示，美国急需拥有像DeepSeek那样的本土对标者—— 一个能与顶级闭源模型竞争的开源AI平台，否则可能在全球技术竞争中失去优势。 Laskin直言，当前西方开源模型普遍落后于DeepSeek及其他中国对手，这可能导致更多用户转向中国产品。 他指出，西方虽有Meta、法国的Mistral AI，甚至OpenAI等玩家也在参与开源，但整体竞争力仍显不足。 在接受采访时，Misha Laskin说道：「美国目前正缺少一个像DeepSeek那样的存在，这也是我们这样的实验室为什么必须存在」。 他将当前局势比作冷战时期的太空竞赛。 但无论是开源模型还是闭源模型，要想真正打造出领先的AI系统，都需要海量的算力、顶级的科研人才——说到底，就是钱。 这也正是为何在今年3月刚完成1.3亿美元融资仅七个月后，Reflection AI又火速完成了一轮高达20亿美元的新融资。 Laskin坦言，Reflection AI未来还将需要更多资金，毕竟竞争对手也在加速融资。 他指出，仅OpenAI一家就在上月获得了英伟达最多可达1000亿美元的投资承诺。 不过，他认为开源模型的市场需求正在持续扩大，尤其是来自希望掌控自身AI技术的大型企业与政府，这将最终撑起一条可持续的商业路径。 为什么坚持开放？ 科技与科学的进步，源于开放与协作的价值观。 无论是互联网、Linux，还是现代计算的协议标准，都是开放的。 绝非偶然。正是因为开源，才有人能二次开发、深度定制，把它们嵌入全球各类系统。大学会教，初创会用，大企业会部署——开放，就是影响力。 开放科学的意义也在于：基于已有成果，别人可以学习、提问、改进、再突破。 如今的AI之所以取得如此进展，也正是因为许多关键技术是公开共享的，如自注意力机制、下一个token预测、强化学习等。 如今，AI正在成为所有产业的底层技术基础。它驱动科研、提升教育、优化能源、加速医疗诊断、重塑供应链……未来一切系统，几乎都将运行在AI之上。 但问题是，前沿AI技术如今正被少数闭门实验室掌控。 如果这种格局持续下去，资本、算力、人才将被少数人垄断。留给其他人的机会之窗正在迅速关闭。 我们必须在这个窗口消失之前，建立足够强大、足以成为开发者和用户首选的开放模型。唯有如此，才能确保智能的基础是开放且可获取的，而不是由少数人掌控。 过去一年的成绩 过去一年，Reflection AI为这个目标做好了充分准备。 Reflection AI的团队成员曾参与推动多个重大AI项目：PaLM、Gemini、AlphaGo、AlphaCode、AlphaProof，以及ChatGPT、Character AI等。 Reflection AI搭建了一个曾被认为只有顶级实验室才能实现的大规模训练平台，可支持大语言模型和强化学习的融合，具备训练超大规模专家混合模型（Mixture-of-Experts）的能力。 首先，他们把这套方法用在「自动编程」这一关键领域，取得重大突破。 接下来，他们将把这套体系用于更通用的智能体推理（agentic reasoning）任务。 他们不仅完成了规模庞大的融资，还建立了一套可持续的商业模式，既保证开放理念，又能继续发布前沿模型。 现在，我们正全力扩展，打造结合大规模预训练与先进强化学习的下一代开放模型。 竞逐超级智能 开启终极智能比赛 2016年，现年37岁的谷歌DeepMind研究员Ioannis Antonoglou参与开发了AlphaGo。 八年后，他与另一位DeepMind前研究员、35岁的Misha Laskin携手创办了 Reflection AI，目标是打造一个能够 编写与维护代码的超级智能系统 。 当前，大多数AI编程公司仍专注于为开发者提供辅助工具，而Reflection的野心是： 彻底取代程序员。 Reflection创始团队坚信， 「自主编程」是通向通用超级智能（AGI）的「根节点问题」（root-node problem）。 联合创始人Ioannis Antonoglou说 我们认为，自主编程就是AGI完备的（AGI-complete）。 如果你能证明你拥有超级智能的软件工程师，那你已经拥有了AGI。接下来只是将同一套算法推广应用到其他垂类的问题上。 他认为，在「编程」这个问题里，你已经找到了获得超级智能的完整路径——所有构成智能所需的要素，都已经在这个任务中被激活。 代码就是LLM的天然UI 智能的形式有很多种，不只是用于编写代码的那一种。但 代码恰恰是推进 机器智能 最「可触达」的表层之一 。 Misha Laskin预测道：「我们认为， 智能的演化速度将快于软件本身 。」他进一步解释： 而选择从软件工程入手，是因为这个领域已经为机器智能做好了准备——整个软件体系天生就更「机器友好」。 对人类来说，操控三维物体是天性；而对语言模型来说， 编程语言就像人类的空间感知能力一样本能天然 。 对LLM而言， 代码就是最符合「人体工学」的操作界面。 这一趋势的影响将逐步显现。在这一过程中，软件公司将会开始构建「AI友好型界面」，加速甚至瞬间完成人类与软件产品的交互。 Misha设想了一种未来：「 GUI 的某些部分可能会被取代，背后实际是语言模型在用代码完成任务。」 原本需要用户点十下的操作，未来可能只需模型生成一行代码，任务即可完成。 Reflection团队对「 超级智能 」的定义非常实用： 能通过操作计算机来创造价值的系统。 Misha 表示：「我们认为，未来语言模型在软件世界中完成工作的方式，就是通过代码智能体（coding agent）。所以一旦你解决了这个问题， 你就实现了计算机上的超级智能，适用于任何拥有 AI 友好接口的软件系统。」 Reflection的创始人相信， 自主智能体最有效的训练方式，是在为其量身定制的环境中练习技能 ——就像当年的DeepMind Atari游戏环境，或OpenAI Gym所做的一样。 在「编程」领域，这些环境和工具已经比较容易想象；但对于其他更复杂的认知场景，可能还需要更大胆的想象力与技术突破。 Misha认为，当前的AI，就像蒸汽机时代早期——在热力学理论尚未诞生之前，发明家们已能造出真正的机器。 从理论角度深刻理解模型为何有效，当然非常有价值。 在物理学中，每当人类从理论上彻底理解一个现象，都会引发新一轮实证创新浪潮——因为你知道该往哪里寻找。  但你无需等到理论完全成型，才能构建出可靠的系统。 受物理学大师费曼（Richard Feynman）的启发，Misha最初走上物理之路。 在一次关于能量守恒的演讲中，费曼说过： 在如今的物理学中，我们并不知道「能量」究竟是什么。意识到这一点非常重要。 这句话，如今同样适用于AI——以及我们对「智能」的理解。 DeepMind创始人Demis Hassabis曾在诺贝尔奖采访中如此总结对超级智能的追寻： AI科学的核心，就是探索和理解什么是智能。 而理解某件事最深刻的方式，就是亲手把它造出来。 现在，我们还有机会，真正建立一个前沿的开放智能体系。但窗口正在收窄，可能这就是最后一次机会。 参考资料： https://x.com/reflection_ai/status/1976304405369520242 https://www.sequoiacap.com/article/reflection-ai-spotlight/ https://x.com/stephzhan/status/1976326493291807117"}
{"url": "https://hub.baai.ac.cn/view/49485", "title": "刚刚，全球首个GB300巨兽救场！一年烧光70亿，OpenAI内斗GPU惨烈", "date": "2025-10-12", "content": "新智元报道 编辑：桃子 【新智元导读】 为了争夺有限的GPU，OpenAI内部一度打得不可开交。2024年总算力投入70亿美元，但算力需求依旧是无底洞。恰恰，微软发布了全球首台GB300超算，专供OpenAI让万亿LLM数天训完。 一图看透全球大模型！新智元十周年钜献，2025 ASI前沿趋势报告37页首发 过去一年，OpenAI在算力上斥资70亿美元。 其中，大模型研发占了最大头——50亿美元，而推理计算仅用了20亿美元。 可见，LLM训练正吞噬无尽的算力，这也是OpenAI最近一直在大举扩展超算建设与合作的重要原因。 采访中，OpenAI总裁Greg Brockman坦言，「内部如何分配GPU，简直就是一场痛苦与煎熬」。 OpenAI各个团队争抢GPU，那叫一个激烈。最头疼的是，如何去合理分配。 如今，甲骨文、英伟达、AMD等芯片巨头/云服务巨头，纷纷与OpenAI联结，能够解其燃眉之急。 这不，就连曾经最大的「金主爸爸」微软也上阵了。 纳德拉官宣，全球首个配备4600+ GB300的超算率先上线，专攻OpenAI。预计，未来将扩展到十万块GPU。 英伟达称，这一算力巨兽，可以让OpenAI不用数周，仅在数天内训练万亿参数模型。 全球首台GB300超算 数天训出万亿LLM 就在昨天，微软Azure宣布成功交付了，全球首个生产级超大规模AI集群。 它搭载了超4600个GB300 NVL72，配备通过下一代InfiniBand网络互联的Blackwell Ultra GPU。 今年早些时候，微软曾推出GB200 v6虚拟机（VM），通过大规模GB200 NVL2集群，已在OpenAI内部训练部署得到应用。 这一次，GB300 v6虚拟机再次树立了行业标杆。 该系统基于机架级设计，每个机架包含18个虚拟机，共计72个GPU： 72个Blackwell Ultra GPU，搭配36个Grace CPU 通过下一代Quantum-X800 InfiniBand，实现每GPU 800 Gb/s的跨机架横向扩展带宽（2x GB200 NVL72） 机架内130 TB/s的NVLink带宽 37TB高速内存 高达1,440 PFLOPS的FP4 Tensor Core性能 全新设计，为大规模AI超算而生 为打造出最强超算，微软对计算、内存、网络、数据中心、散热和供电等技术栈的每一层，都进行了重新设计。 机架层：低延迟高吞吐 通过NVLink和NVSwitch，GB300 v6在机架层面实现了高达130TB/s的机架内数据传输速率，连接了总计37TB的高速内存，由此消除了内存和带宽瓶颈。 在大模型和长上下文场景下，推理吞吐量大幅提升，为AI智能体和多模态AI带来前所未有的响应速度和扩展性。 同时，Azure部署了采用当今最快网络 fabric——Quantum-X800 Gbp/s InfiniBand——的全连接胖树（fat-tree）无阻塞架构，能够跨机架扩展数万个GPU。 此外，Azure散热系统采用独立的「散热器单元」和「设施级冷却方案」。 在为GB300 NVL72这类高密度、高性能集群保持热稳定性的同时，最大限度地减少了水资源消耗。 软件层：全面优化 不仅如此，微软为存储、编排和调度重构的软件栈也经过全面优化，能够在超算规模上充分利用计算、网络、存储和数据中心基础设施，提供前所未有的高性能和高效率。 OpenAI GPU争夺战 一场「痛苦与煎熬」 在OpenAI内部，正上演一场GPU激烈争夺战。 上周四，Greg在一期「Matthew Berman」播客节目中，自曝管理算力资源分配的过程，令人揪心且筋疲力尽。 这太难了，你总能看到各种绝妙的点子，然后又有人带着另一个绝妙的点子来找你，你心想，这个也太棒了。 在OpenAI内部，将算力资源主要分配给「研究」和「应用产品」两个方向。 为了应对算力分配的挑战，OpenAI建立了一套相对清晰的资源分配机制： 高层决策：由奥特曼和Fidji Simo组成的领导团队，决定研究团队与应用团队之间的总体算力划分； 研究团队内部协调：首席科学家和研究负责人，决定研究团队资源分配； 运营层：由Kevin Park领导的小型内部团队负责GPU的具体分配和调动。 OpenAI复杂算力关系网络图 Greg提到，当一个项目接近尾声时，Kevin会重新分配硬件资源，以支持新启动的项目。 算力驱动着整个团队的生产力，此事干系重大。 大家对此都非常在意。人们对「我能否分到算力」这件事所投入的精力与情感强度远超想象。 一直以来，OpenAI多次公开表达其对算力永不满足的需求。 OpenAI首席产品官Kevin Weil曾表示，「我们每次拿到新的 GPU，它们都会被立刻投入使用」。 OpenAI对算力的需求逻辑很简单—— GPU的数量直接决定了AI应用的能力上限。获得的GPU越多，所有人就能使用越多的AI。 不仅OpenAI，整个行业科技巨头也在加码算力投入。小扎透露，Meta正将「人均算力」打造为核心竞争优势。 上个月，奥特曼称，OpenAI正在推出「算力密集型服务」。 当我们以当前模型的成本，将海量算力投入到有趣的新想法上时，能创造出怎样的可能性？ 这场算力争夺战中，谁手握最多的算力，将决定谁在AI竞赛中脱颖而出。 参考资料： https://x.com/satyanadella/status/1976322455288545343 https://x.com/Azure/status/1976319720472138045 https://azure.microsoft.com/en-us/blog/microsoft-azure-delivers-the-first-large-scale-cluster-with-nvidia-gb300-nvl72-for-openai-workloads/ https://www.businessinsider.com/openai-president-allocate-gpu-compute-internally-greg-brockman-2025-10"}
