{"url": "https://huggingface.co/blog/zh", "title": "帖子、文章和讨论", "date": "2025-10-13", "content": ""}
{"url": "https://huggingface.co/blog/community", "title": "view all", "date": "2025-10-20", "content": ""}
{"url": "https://huggingface.co/blog/lbourdois/huggingface-models-stats", "title": "Model statistics of the 50 most downloaded entities on Hugging Face", "date": "2025-10-14", "content": "It's not possible to add .hmtl graphics to Hugging Face blog posts. If you speak French, we really invite you to read the French version here instead where the graphics display well. For non-French speakers, we can only include screenshots in this article. Under each of them you will find a link to a dynamic plot.\nPlease note that in the following, in the text within a blue box, we express only our personal reflection.\nFinally, it should be noted that on October 9, Elastic announced the acquisition of Jina.ai. We haven't had time to modify the various graphs to take this into account.\nIn this blog post, we analyze the most impactful open-source models in practice . To do so, we focus on a very pragmatic metric: \"Which models are downloaded most often on the Hugging Face Hub?\". The assumption is that models that are downloaded massively are those that are used in the real world. This approach is also intended to be fairer to individuals/organizations that do not have a communication service or are not followed/liked massively on the Hub.\nThe analysis of the 50 most downloaded entities on the Hugging Face Hub (80.22% of total Hub downloads) shows that:\nAmong all open-source models where it is possible to know the size of the model (96.94% of the top 50 and 77.76% of the Hub), small models are by far the most downloaded: - 92.48% of downloads are for models with fewer than one billion parameters, - 86.33% are for models with fewer than 500 million parameters, - 69.83% on models with fewer than 200 million parameters, - 40.17% on models with fewer than 100 million parameters.\nModel downloads primarily concern NLP (58.1%), followed by CV at 21.2%, audio at 15.1%, various forms of multimodality at 3.3%, time series at 1.7%, with the rest  undetermined due to a lack of correctly annotated metadata.\nText encoders represent (base models + their fine-tuning on a specific task) more than 45% of total downloads (or 77.5% of the NLP modality), compared to only 9.5% for decoders (16.5% of the modality) and 3% for encoder-decoders (5% of the modality). Thus, contrary to the hype surrounding these models, LLMs are not being downloaded massively in open source. Could it be that their real-world use is more on the side of private APIs?\nEnglish represents more than 79.46% of downloads of models (monolingual or multilingual) using a language (and even 92.85% if we only consider models with a language tag). This language is far ahead of the others. For example, French, which comes in second place, accounts for only 17.48% (20.43% of models with a language tag).\nCompanies are the largest contributors to open source, accounting for 63.2% of downloads (20 entities out of 50), followed by universities at 20.7% (10 entities), then individuals at 12.1% (16 entities), non-profit organizations at 3.8% (4 entities), and finally hybrid laboratories at 0.3% (1 entity).\nThe United States is presents everywhere, covering all modalities (NLP, vision, audio, time series, multimodalities) and all model sizes (from less than 5M parameters to tens/hundreds of billions). Americans are notably driven by their open-source companies (crushing all competition in this segment) but also have strengths in all types of existing organizations (excluding hybrid laboratories) since they are represented 18 times in this top 50. Europe (notably Germany, France, and the United Kingdom) is also positioned in all types of existing organizations outside of hybrid laboratories (present 20 times) but stands out due to the impact of its specialized universities on small models (<200M parameters). It is also present in all modalities except time series. China (represented by five entities) has a strong presence in the large open-source model segment (31.8% vs. 43.1% for the United States and 24% for Europe on models with more than 7.5 billion parameters). However, it is badly placed in all other model size categories (only 130 million downloads of models with fewer than 100 million parameters, compared to 7.05 billion for the United States and 5.3 billion for Europe). Its lack of positioning in vision (barely 4 million downloads) and audio (0 downloads) also penalizes it. These are not areas in which it is known to be lagging behind, but it is clear that it does not currently produce open-source content in these areas on Hugging Face (a platform that is not accessible in the country). It dominates the non-profit sector and is the only player in the university/business hybrid laboratory sector. Finally, other countries in this top 50 only benefit from a specialized player in a given modality.\nThe data shown in this article was collected on October 1, 2025. After identifying the 50 open-source entities with the most downloads, we collected all the models associated with them. \nThis represents 72,423 models out of the 2,126,833 hosted on the Hub, or 3.41% of the total. These accounts represent exactly 36,450,707,797 downloads out of a total of 45,438,836,957, or 80.22%.\nFor each of the models, the pipeline and language tags were also collected when available. Similarly, the size was estimated based on the .safetensors file.\nWhen information was missing (no tags or model size in particular), we manually corrected the data by consulting the model card or publication associated for the 1,000 most downloaded open-source models. These alone represent 77.89% of all Hub downloads and 97.10% of the 50 entities analyzed.\nEverything was finally stored in a dataframe that looked like this:\nAll the graphs shown below are generated from this data.\nThe amounts are rounded to the nearest million for clarity, but also because the Hub is experiencing some display issues. For example, for the sentence-transformers/static-retrieval-mrl-en-v1 model, no downloads are available . The goal here is mainly to understand the orders of magnitude rather than to focus on the exact numbers evolving every day.\nIn this first section, we display the overall downloads for each of the top 50 entities contributing to open source, as well as their category type and country of origin. We will discuss these last two points in dedicated sections.\nNote that we use the term \"entity\" rather than (Hugging Face) \"account\"  because an entity can be composed of several accounts. For example, Google is composed of google , google-bert , google-t5 , and albert . We therefore offer a global plot allowing you to compare the most downloaded entities, and another plot by sub-account to visualize how the different accounts are distributed within them.\nOverview\nSub-accounts view\n1. The Google entity is composed of google , google-bert , google-t5 and albert . More than 74% of its downloads come from \"old models\", namely 64% from BERT, 6.8% from T5, and 3.2% from ALBERT.\n2. The Meta entity is composed of FacebookAI , facebook and meta-llama . Similar observations as for Google but to a lesser extent, with 48.3% of downloads coming from its RoBERTa models versus 9% for Llamas.\n3. The Sentence-transformers entity (from the Ubiquitous Knowledge Processing Lab in Darmstadt, Germany, and more specifically the work of Nils Reimers) completes the trio. This entity is composed of sentence-transformers and cross-encoder . The sentence-transformers account is actually the most downloaded on all of Hugging Face.\n4. The Hugging Face entity is composed of timm (52.4% of downloads), distilbert (44.6%), llava-hf , HuggingFaceTB and HuggingFaceM4 .\n5. The OpenAI entity is composed of openai (72%) and openai-community (28%). Although it publishes little in open-source, the company is extremely impactful when it does (its CLIP and Whisper are particularly downloaded).\n6. 99% of MIT's downloads come from its ast-finetuned-audioset-10-10-0.4593 model, which is the second most downloaded model on the Hub.\n7. Microsoft is another popular Big Tech company. The chart in the section on modalities shows that it is the most diversified organization in terms of modalities addressed (whereas, with the exception of a few others, most entities in the top 50 are highly specialized in a given modality).\n8. Jonatas Grosman is the most downloaded individual. With barely 300 followers on the Hub, he is an illustration of the fact that it is not necessarily the most followed/liked entities that are the most impactful. He specialized for a time in finetuning wav2vec2 but has not published new models for 3 years now.\n9. Pyannote specializes in small audio segmentation and diarization models.\n10. 99% of Falcons.ai's downloads (350 followers) come from its nsfw_image_detection model, which is the seventh most downloaded model on the entire Hub.\n11. BAAI is the most downloaded Chinese entity on Hugging Face, notably through its bge models. It is also the most downloaded non-profit entity.\n12. The Alibaba entity is composed of Qwen (81.5%), Alibaba-NLP (9.5%), thenlper (7.7%), Wan-AI , AIDC-AI , alibaba-pai , alibaba-damo . It is thus primarily downloaded for its Qwen models (especially the Qwen2.5-1.5B-Instruct which represents 20% of its downloads and is the most downloaded LLM on the Hub).\n13. The Amazon entity is composed of amazon (80.1%) and autogluon (19.9%). It is the only entity whose downloads are massively focused on time series.\n14. Dima806 is primarily downloaded for its dima806/fairface_age_image_detection model. It is an individual still active in 2025.\n15. Cardiffnlp is downloaded for its numerous sentiment classification models.\n16. The Stability AI entity is composed of stabilityai (80%) and stable-diffusion-v1-5 (20%). It is primarily downloaded for its various versions of Stable Diffusion.\n17. The Maziyar Panahi entity is composed of MaziyarPanahi (80.7%), which is his individual account where he offers GGUF versions of LLMs, and OpenMed (19.3%), which is a non-profit organization he created dedicated to medical models. He is an individual still active in 2025.\n18. Helsinki-NLP is downloaded for its numerous machine translation models.\n19. Laion is primarily downloaded for its numerous models reproducing CLIP .\n20. Juan Manuel Pérez via the pysentimiento organization is primarily downloaded for his sentiment classification models in Spanish. He has not published models since 2023.\n21. Bingsu is primarily downloaded for the various YOLOs contained in Bingsu/adetailer . He is an individual still active in 2025.\n22. Half of AllenAI's downloads come from its longformer .\n23. Tohoku-nlp is a university group downloaded mainly for its Japanese version of BERT .\n24. Manuel Romero is massively downloaded for his T5 finetunings but especially his financial model distilroberta-finetuned-financial-news-sentiment-analysis . He is an individual still active in 2025.\n25. Mistral AI is primarily downloaded for the 7B instruct versions of its models.\n26. Prajjwal1's downloads are based on PyTorch conversions of bert-tiny (4M parameters) and bert-small (29M). He has not published models since 2023.\n27. Deepset is downloaded for its English QA model .\n28. Salesforce has found success with its various versions of its BLIP model.\n29. Intfloat is considered a Chinese individual since Liang Wang decided to publish his work under his name on Hugging Face. In practice, these are the e5 models created as part of his work at Microsoft Asia. He is an individual still active in 2025.\n30. TheBloke is known for offering quantized versions of models. His most successful version is the phi-2-GGUF . He has not published models since 2024.\n31. CompVis is popular for its stable-diffusion-safety-checker model.\n32. CIDAS is widely used for its segmentation model clipseg-rd64-refined .\n33. Emily Alsentzer found some success with her Bio_ClinicalBERT . She has not published models since 2020.\n34. NVIDIA is extremely balanced (it's not a single model that drives all the entity's downloads). Its speakerverification_en_titanet_large model stands out slightly from the others.\n35. LM Studio is composed of lmstudio-community (44.1%), bartowski (55.8%) and lmstudio-ai . Please note that we were really hesitant to include bartowski account in this entity. He describes himself on HF as the \"Official Model Curator for https://lmstudio.ai/\"\" but is primarily a \"Research Engineer at arcee.ai\".\nFurthermore, he is located in Canada, while LM Studio is in the United States. The choice was to either combine them or not include either one, as they are not individually in the top 50 (answerdotai being the 51st entity).\n36. David S. Lim is downloaded for his English NER models. He has not published models since 2024.\n37. Unsloth is primarily downloaded for its quantized versions of LLMs.\n38. mradermacher is primarily downloaded for his quantized versions of LLMs. He is a group of individuals still active in 2025.\n39. Moritz Laurer is primarily downloaded for his NLI models, notably DeBERTa-v3-base-mnli-fever-anli . He has not published models since 2024.\n40. Jean-Baptiste Polle is downloaded for his French NER models. He has not published models since 2023.\n41. HFL is a hybrid laboratory (university x company) downloaded mainly for its Chinese version of BERT .\n42. Deepseek is fairly balanced with a slight prevalence of R1 models.\n43. Big Science is an international collaborative project (led by Hugging Face and CNRS) for its bloom, bloomz and t0 models.\n44. Flair is primarily downloaded for its monolingual NER models in multiple languages.\n45. Sam Lowe is primarily downloaded for his emotion classification model. He has not published models since 2023.\n46. Patrick John Chia, whose nationality we were unable to determine, is known for his fashion-clip model. He has not published models since 2023.\n47. Almanach is a university group downloaded mainly for its French version of BERT .\n48. Supabase is primarily downloaded for its gte-small model.\n49. Jina AI is primarily downloaded for its embedding models.\n50. colbert-ir is primarily downloaded for its colbertv2.0 model.\nFor the country of origin, in this version we are interested in the locations of the individual and their company's headquarters. The aim here is to estimate the number of countries with an environment enabling the creation of the most downloaded models.\nWith more than 20.6 billion downloads, the United States accounts for 56.4% of downloads of the 50 most downloaded open-source entities on Hugging Face. This is driven in particular by Big Tech and its high density, with 18 of the 50 entities located there. Four times smaller, Germany is second with 13.2% of the total, with 4.8 billion downloads, of which 79% come from sentence-transformers. It accounts for 8 of the 50 entities. France is third with 3.4 billion downloads, representing 9.3% of the total. With five entities, its distribution of contributions from different players is slightly more balanced. In fourth place, China accounts for 1.9 billion downloads, or 5.2% of the total, also with five entities (four if we consider intfloat associated to Microsoft). With the exception of the United Kingdom, which also has four entities, all other countries are represented by a single entity.\nIt may be surprising to observe that China ranks only fourth in terms of downloads. This can be explained by the fact that Hugging Face is not accessible in this country, preventing entities there from counting local users in addition to international ones. In the rest of the post, analysis of the collected data highlights other possible explanations for this observation (see the section on model size or the section on modalities).\nWhen the countries of the European Union (13 entities) are grouped together, their share of total downloads rises to 24%.\nA comparison at the continental level shows that North America accounts for 56.7% of downloads of the top 50 open-source models (and 19 entities), Europe 29% (20 entities), Asia 8.9% (8 entities), and South America 4.9% (2 entities). The remainder is either undetermined or consists of international initiatives (2 entities).\nThe type of entity is determined by what it has chosen from the options offered by Hugging Face when creating an organization ( Company , University , Classroom , Non-profit , Government , Community ). We added Individual to designate individuals offering models under their own name outside of an organization. Note also a special case, HFL, which is a joint laboratory between a university and a company. We therefore had to create a Hybrid Lab category not available on Hugging Face.\nCompanies (20 entities out of 50) account for 63.2% of downloads of open-source models in the top 50, universities (10 entities) 20.7%, individuals (16 entities) 12.1%, non-profit organizations (4 entities) 3.8%, and hybrid laboratories 0.3% (1 entity).\nFor universities, most entities are in fact only research teams/groups and not the entire University/Institution. The inability to create sub-teams within a large Hugging Face organization results in the creation of several organizations (one per team, or individuals publishing under their own name rather than that of the University), with the consequence of splintering the total account and making them less visible. For example, the CNRS (not technically a university) is not represented in the top-50, even though pyannote was created there by Hervé Bredin, Maziyar Panahi works there and Bloom from Big Science was trained on its servers.\nLooking at the activity of the different entities on the Hub, we can see that overall, most of them are still active in 2025. One notable exception is the Individual category. Of the 16 entities in this category, only 6 published new models in 2025. It therefore appears that, unlike other categories, the contribution of individuals to open source is not a sustainable activity over time. This phenomenon can be offset by a renewal of contributors, but it would be interesting to know the reasons why these people are turning away from open source in order to find a way to support them and remedy this problem.\nThe large share of companies in the open-source model could also be a vulnerability of the model if they decide in the future to stop contributing, as some have done in the past.\nThe United States is present in four categories: first by a wide margin among companies with 76.3% of downloads, second among universities with 30% of the category, third among individuals with 12.5%, and third among non-profits with 15.3%. Germany is also in four categories: sixth among companies with 1.4% of downloads, first among universities with 54.1%, 2.5% among individuals, and second among non-profits with 20.3%. China appears in four categories too: fourth among companies with 3.7%, absent from universities (alone) but the only case of a hybrid university/company laboratory in the top 50, with 3.6% among individuals (the intfloat case otherwise absent) and finally first among non-profits with 57.5%. France is present in three types of categories: second among companies with 12.7%, sixth among universities with 1.2% and fourth among individuals with 8.9%. The United Kingdom is ranked in three types of categories: fifth among companies with 1.9% of downloads, third among universities with 7.4% and 5.4% among individuals. All other countries are represented only once in a specific category.\nNot applied in this first version, a system for weighting the various amounts advanced would need to be applied. Indeed, the number of downloads in a country can be influenced by its population size, the number of companies in the country, the rate of AI usage among the population, etc. That is why, for example, we offer individual graphs but also graphs at the EU or continental level. In addition, discussions are taking place with the Hugging Face teams to try to determine the location of model downloads, in order to distinguish what constitutes \"domestic consumption\" and what constitutes \"exports\".\nIn this configuration, the European Union ranks second among open-source companies at 14.1%, first among universities at 60%, third among individuals at 15.6% and second among non-profits at 20.3%.\nAt the continental level, North America's position is essentially the same as that listed for the United States. Only the share of individuals increases from 12.5% to 14.4% including Canada contributor. Europe benefits from the figures for the United Kingdom and Ukraine, giving it second place for companies at 16%, first place for universities at 67.4%, second place for individuals at 34.3%, and second place for non-profits at 20.3%. Asia ranks third for companies at 7.7%, third for universities at 2.6%, fourth for individuals at 8.6%, first for non-profits at 57.4%, and alone in the hybrid laboratory segment. Finally, it should be noted that South America stands out for its individual contributions, where it ranks first with 40.7% of the total.\nWe can see that NLP is the most downloaded modality among the top 50, with 58.1% of downloads, followed by CV at 21.2% and audio at 15.1%. The “ Unknown ” modality includes all models whose language tag was not specified and could not be corrected.\nThe number of downloads for a modality seems to be related to whether or not Hugging Face also includes a model for that modality in its transformers library. NLP thus appears to be favored (Hugging Face being known for this modality). It is not clear whether practitioners of other modalities use this library for their use cases. For example, there are several alternatives in vision with tools from open-mmlab, roboflow, etc. In the vision modality, the most downloaded model is CLIP, integrated into transformers .\nIn the following graph, for each entity, we show the proportion of each modality in its downloads.\nWe observe that few entities are diversified. Each seems to have a specialty. 32 are mainly involved in NLP, 10 in vision (paradoxically Hugging Face with the acquisition of timm and OpenAI as a result of not having released any NLP models in open source since gpt2 before August 2025), 4 in audio, 2 in multimodal NLP/vision, 1 in time series, and 1 undetermined ( mradermacher offers quantified versions of models, so this would likely be NLP).\nThe chart below provides a little more detail by showing the breakdown by sub-account for each entity.\nIt can be noted that the United States ranks first in all modalities. Except for vision, where it scores \"only\" 46.6%, it captures the majority in all other modalities. France ranks second or third in NLP, vision, and audio modalities. China, for its part, is absent from CV and audio modalities. This could explain the observation made above. It ranks \"only\" fourth in overall downloads, or perhaps rather that Germany (present in NLP and vision) and France register more downloads.\nThe European Union thus ranks second or third in terms of NLP, vision, and audio modalities.\nThe modality graphs could be obtained from the tag pipelines using the associations available in the following dictionary.\nIn this section, we display the different tasks by modality to get a better overview of the most popular ones.\nNLP tasks are shown in different shades of blue, vision tasks in different shades of yellow, and audio tasks in different shades of red.\nAfter exchanges with the Hugigng Face team, we have decided to merge the sentence-similarity and feature-extraction pipeline tags.\nFinally, we did not group text2text-generation with text-generation . The first tag is mainly used by encoder-decoder models that generate text (T5, BART), while the second is used by decoder-only models. The goal here was to show that these encoder-decoders account for as many downloads as Alibaba's Qwen and more than Meta's Llamas.\nBase encoder-only models visible via the fill-mask task are by far the most downloaded models, accounting for 22.3% of downloads.  Their fine-tunings account for 22.7% of downloads, mainly on the sentence-similarity task (which we hesitated to merge with text-ranking ) followed by text-classification , token-classification , zero-shot-classification , and finally question-answering .\nPure decoder generation models account for 9.5% of downloads. Encoder-decoder models account for 1.4%, to which we can add the translation task at 1.6%, for which they are mainly used. Next comes vision, with 11.1% of downloads in classification , 6.4% in zero-shot-classification (CLIP and derivatives), 1.8% in image feature extraction, 0.9% in object detection, and 0.5% in object segmentation (the rest of the tasks are not significant).\nFor audio, we have 7% ASR , 6% classification , and 1.2% activity detection .\nThe graphs by sub-account and by country are not displayed. They are completely illegible with more than 50 tasks listed in total.\nIn this section, we focus only on models related to tasks where language is applicable (NLP tasks, ASR, text-to-image, etc.). This represents 24,592,908,565 downloads out of the initial 36,450,707,797 in the top 50, or 67.47%.\nIn practice, it turns out that for 14.42% of these 24.6 billion downloads, the language tag is not specified for these models.\nAfter analysis, 184 languages are referenced in this top 50 (224 other values were found but are not ISO639-1 or ISO639-3 tags). Here too, for visibility reasons, only the top 20 languages are displayed in the following graph. All the figures can be found in the dataset available here .\nIt can be noted that among all the models available in the top 50 based on language, English accounts for more than 79.46% of downloads of these models (monolingual or multilingual) and even 92.85% of models with a language tag. It is far ahead of other languages. For example, French, which comes in second place, accounts for only 17.48% (20.43% of models with a language tag). In this top 20, languages with a Latin alphabet occupy the top positions but account for only 8 out of 20, showing that multilingual models are quite diverse.\nIn this section, we focus exclusively on the 11,263 models for which we were able to determine a model size.\nThey account for 35,333,543,289 downloads out of 36,450,707,797, or 96.94% or 77.76% of the total.\nWe can see that:\nIn more detail:\nLLMs are therefore not massively downloaded models (at least in open source). One hypothesis is that the explanation could lie in the profile of Hugging Face users (particularly their computing power, which can be entered here ), but we have not found a way to retrieve this information automatically. In any case, an entity wishing to have an impact in open source probably needs to propose models with fewer than 500 million parameters, or even fewer than 200 million, in order to be downloaded by a large target audience. Given that 92.5% of downloads are for models with fewer than one billion parameters, it would be useful that Hugging Face allows for more refined filtering options.\nCompanies are present in all model size ranges. They account for a significant part of models with fewer than 5 million parameters (94% of downloads in this range) and more than 500 million parameters (77 to 85% of downloads). Between 5 million and 500 million parameters (more like 200 million), university models can be seen as alternatives to those developed by companies.\nLe NLP est présent dans toutes les tranches de tailles, notamment de 100 à 500M de paramètres. La CV est principalement portée par des modèles de moins de 100 de paramètres bien qu'également présente entre 100 et 500M. L'audio est principalement réparti de <5 à 500M de paramètres mais étonnement absent sur la tranche 100 à 200M. Les séries temporelles pour les modèles de moins de 50M de paramètres.\nWe have a little more detail at the task level (for example, for vision, we can see that models <100M are mostly classification, while between 100 and 500M they are mostly CLIP).\nReaders can make their own analyses based on the fact that NLP tasks are different shades of blue, vision tasks are shades of yellow, and audio tasks are shades of red.\nWe invite readers to click on the legend to keep only the entities that interest them. This allows a given entity to see the distribution of downloads of its models according to the different segments and thus reveal its profile, or even compare n different entities.\nFor example, if we compare Google and Meta, which are the two most downloaded open-source entities, Google dominates models with <200M parameters (and is extremely strong in the 100-200M range), whereas Meta dominates models with 200M and above.\nIt is also possible to zoom in:\nIn this example, focusing only on the top 50 large models players that exceed 5% of downloads on one of the ranges above 1B parameters (namely Google, Meta, Microsoft, Alibaba, Mistral, Unsloth, Deepseek + individuals offering quantified versions of models such as TheBloke and Maziyar Panahi), we can see:\nOverall (not shown in this graph), in the 1B+ segment: Meta is the most downloaded with 23.2% of the segment, ahead of Alibaba at 20%, Maziyar Panahi at 11.1%, Google at 7%, Mistral at 6.8%, TheBloke at 4.5%, Deepseek at 3.8%, and Microsoft at 3.3%.\n15.6% of the segment is captured by individuals who offer quantified weights for the base models. The entities creating these models could capture these downloads if, when uploading them, they made these quantified versions directly available rather than leaving it to the community. Without Meta releasing any new open-source models in the near future, according to a very naive estimate (linear extrapolation based on downloads from the Qwen and meta-llama accounts between September 21 and October 1, 2025), Alibaba should become the leader in this segment by the end of November. Its Qwen/Qwen2.5-1.5B-Instruct model is already the most downloaded textual LLM ahead of meta-llama/Llama-3.1-8B-Instruct (the smallest models being the most downloaded).\nA Chinese player would therefore be in first place in this metric (and in the open-source LLM segment), although overall US entities still have a margin over all Chinese entities (49.8% for the US in the 1B+ range compared to 24.2% for China, and 43.1% vs. 31.8% in the 7.5B+ range).\nThe United States and the European Union are present across all model size segments. The EU has a majority position in models with fewer than 25 million parameters, while the United States leads in all other segments. It should be noted that China is absent (with barely 130 million downloads) from models with fewer than 100 million parameters. Given that these are the most downloaded models, this explains the observation made at the beginning of the article that the country ranks only fourth in global downloads behind Germany and France, while it ranks second for models with one billion parameters and above, which are not widely downloaded in open source.\nIn an update to this work (which will require analyzing a large number of publications), we would like to add two additional layers, namely the city where the authors of the model are located and their nationality. The aim will be to determine in which cities the most downloaded models are developed, as well as the countries with the education systems that produce the most downloaded authors.\nFor example, downloads of LLaMA 1 would not be counted as 100% American, but as 12/14 French (Paris), 1/14 American (San Francisco), and 1/14 Pakistani (London). Several weighting systems could be applied, such as a higher weighting for the main authors, for example.\nWe would also like to offer a view that distinguishes the impact of a model within the ecosystem, i.e., in addition to downloads specific to a given model, add all downloads resulting from fine-tuning/merges/adapters/quantifications. To this end, we have already conducted a few experiments using the Model tree , but it turns out that this is often incomplete, especially for older models. One method would be to analyze the names of the fine-tuning heads used by the models, if not the base model, then at least its architecture.\nThe goal is then to determine which base models have the greatest impact (this recent article by the Hugging Face teams, which has just been released, would be a good starting point).\nAs mentioned in the introduction, downloads of certain models are currently being incorrectly counted on the Hub. This could devalue certain entities. We are currently in contact with the Hugging Face teams to correct this issue as best we can.\nFinally, we are considering writing an article similar to the one for datasets in place of models."}
{"url": "https://huggingface.co/blog/hugging-science/ai-for-food-allergies", "title": "AI for Food Allergies", "date": "", "content": "So, what can we do about it?\nIn recent years, biomedical research has made several remarkable advances: from experimental vaccines and desensitization-based immunotherapies to improved diagnostic tools capable of identifying specific allergen sensitivities with unprecedented precision. These developments are pointing us in the right direction toward building long-term immune tolerance, but we’re not quite there yet.\nIn the meantime, we’ve also witnessed groundbreaking progress in artificial intelligence applied to biology and medicine. Models like AlphaFold and Boltz-1 have revolutionized protein structure prediction, while AI-driven approaches in genomics , drug discovery , and molecular modeling are accelerating the pace of biomedical innovation. The convergence of these worlds is opening up new possibilities for understanding, predicting, and ultimately treating complex immune conditions such as food allergies.\nFour among the major allergenic proteins folded by AlphaFold. Up left to bottom right: glycinin (soybean), ovalbumin (egg), alpha lactalbumin (milk), ara-h-2 (peanut).\nOur vision with the AI for Food Allergies project is to build the first community-driven research lab dedicated to exploring how artificial intelligence can meaningfully advance the field of food allergy research. We aim to bridge the gap between cutting-edge AI and biomedical science by developing open, collaborative projects that contribute tangible value to researchers, clinicians, and patients alike.\nThe last couple of years have been transformative for food allergy research. Artificial intelligence, once limited to image recognition or text translation, now operates comfortably in the biological and regulatory spaces that define food safety.\nThis evolution began with early bioinformatics techniques utilized sequence alignment and physicochemical descriptors to detect and flag potential allergens. Databases such as SDAP and AllergenOnline were used to identify cross-reactive proteins. Machine-learning algorithms such as AllerHunter , and NetAllergen later enhanced these methods, training on thousands of known allergens and non-allergens to improve predictive accuracy.\nToday, at the molecular level , deep learning models like ProtBERT , ESM-2 , and AllergenBERT can analyze amino-acid sequences to predict whether a protein might act as an allergen. They identify subtle biochemical patterns, sequence motifs, secondary-structure signals, and epitope similarities, which correlate with immune reactions. For example, AllergenAI applies convolutional neural networks to allergen sequences from SDAP 2.0 , COMPARE , and AlgPred 2 , uncovering motifs essential for IgE binding and demonstrating the promise of integrating structural data into prediction pipelines What used to require months of lab experiments can now be screened computationally, dramatically accelerating allergen discovery in novel foods and plant-based proteins.\nConcurrently, AI is expanding the scope of allergy therapeutics through advances in drug-target interaction (DTI) modelling. Deep neural networks, graph neural networks and transformer models utilize data from chemogenomic datasets such as DAVIS , PDBbind to predict binding affinities, enabling virtual screening of compounds that can potentially inhibit IgE–FcεRI binding or modulate inflammatory pathways. Multimodal datasets that contain molecular structures, transcriptomics and imaging readouts can be utilized for tasks such as small molecule generation, prediction of properties and assessment of immune cell response. The subsequent subheadings present critical datasets supporting the mentioned AI approaches and explain how each resource is employed in food allergy drug design.\nIn clinical research , AI is helping refine diagnostics. Traditionally, allergists rely on a mix of skin-prick results, serum-specific IgE levels, and patient history, but interpreting these together is difficult. Machine learning models have begun combining these modalities to estimate the true probability of a food allergy, reducing unnecessary oral food challenges and improving patient safety. Importantly, these models don’t replace doctors, they simply reduce uncertainty and provide interpretable probabilities rather than binary outcomes.\nOn the consumer and regulatory side , advances in natural language processing (NLP) and computer vision (CV) have made it possible to read and understand ingredient labels at scale. NLP models trained on multilingual data can detect hidden or misspelled allergen names (“tahini” → sesame, “paneer” → dairy), while vision models can read curved, low-light packaging and extract ingredient text more reliably than standard OCR systems. Combined with live monitoring of FDA and USDA recall feeds, AI can now alert consumers to undeclared allergen risks in near real time.\nA fundamental step in applying Machine Learning to this field is having access to high-quality data . As highlighted by Channing and Ghosh in their position paper “ AI for Scientific Discovery is a Social Problem ” , the real challenge in ML for science goes beyond advanced models and powerful GPUs. It lies in the scarcity, fragmentation, and inaccessibility of data . This issue is particularly evident in the biomedical domain, where data gatekeeping, inconsistent standards, and lack of interoperability often hinder collaboration and slows down progress.\nThe first milestone of our community is dedicated to addressing this very challenge. We have curated Awesome Food Allergy Datasets , the first open collection of datasets on food allergies , meticulously annotated and categorized to serve as a foundation for future research. By making this resource openly accessible, we aim to accelerate discovery, foster collaboration, and lower the entry barrier for researchers and innovators interested in applying AI to this critical field.\nStats about the distribution of our datasets by data type, category and public availability.\nWe organize this resource into three complementary layers , each designed to serve a specific part of the AI-for-Food-Allergies ecosystem.\nAt the molecular level, we are assembling what may become the most complete open dataset for allergen and protein analysis ever built. It merges classical allergen repositories with next-generation molecular and drug-target databases, enabling deep learning models to move seamlessly from sequence to structure to immune response.\nThis layer draws from trusted allergen-focused sources such as WHO/IUIS Allergen Nomenclature Database , AllergenOnline , Allergen30 , AllerBase , AllFam , Allermatch , AllerHunter , AllerCatPro 2.0 , AllergenAI , NetAllergen , AllerTOP v1.1 , Alleropedia , Allergome , and the Allergen Family Database . These provide verified allergenic and non-allergenic protein sequences, family classifications, and cross-reactivity annotations.\nTo capture the biochemical and structural side of allergenicity, we integrate resources like SDAP 2.0 , PDBBind+ , ProPepper , and quantum-chemistry datasets including nabla²DFT , QM , QDπ , QCML , and QCDGE . These datasets provide molecular surfaces, binding affinities, and electrostatic descriptors that help AI models learn why certain proteins interact with IgE antibodies.\nBecause allergic response often overlaps with pharmacology, this layer also incorporates drug–target and compound databases such as DAVIS , QSAR , e-Drug3D , Stanford Drug Data , DrugCentral , MedKG , Therapeutic Target Database , STITCH , Probes & Drugs , IUPHAR Pharmacology , and Enamine REAL . These enable studies of cross-reactivity between allergens and drugs, side-effects that mimic allergic reactions, and opportunities for immunomodulatory therapy.\nEach record in this unified dataset is annotated with sequence data, taxonomy, molecular descriptors, and literature references. We apply homology reduction to prevent data leakage between training and test sets and evaluate model quality using AUROC , AUPRC , MCC , and calibration scores . This layer serves as the foundation for building transformer-based models that predict allergenicity, drug–allergen interactions, and cross-reactive epitopes.\nAllergies begin at the immune level, and understanding that requires human data. The second layer combines immunology, clinical, and trial datasets to help researchers model how allergic sensitization, tolerance, and treatment evolve over time.\nFrom the immunological perspective, we include the IEDB (Immune Epitope Database) and its Analysis Resource , alongside specialized datasets like AlgPred 2.0 , Allergen30 , Allergen Peptide Browser , and ProPepper , which map B- and T-cell epitopes and antibody binding regions.\nFor studying patient-level outcomes, we integrate clinical and population datasets such as Food Anaphylaxis ML Dataset (TIP) , Food Allergy Risk Stratification Dataset , Food Allergy & Intolerance Dataset , and AllergyMap . Large-scale cohorts like HealthNuts , CHILD , and DIABIMMUNE , plus microbiome-focused datasets (e.g., Dysfunctional Gut Microbiome Networks in Childhood IgE-Mediated Food Allergy and Akkermansia muciniphila in Fibre-Deprived Mice ), enrich this layer with genetic and microbial context.\nSimulated datasets — such as the Simulated Allergen Immunotherapy Trials Dataset , Simulated AIT Trials Dataset , and FARE Food Allergy Research data — allow us to model the long-term response to desensitization therapies without exposing patients to risk.\nGenetic and biochemical variability is represented through GWAS , DNA Methylation GSE59999 , and the Human Metabolome Database . These allow multi-omics studies of how genes, metabolism, and environment combine to shape allergic disease.\nTogether, these resources form the backbone for predictive models that estimate reaction risk, identify candidate biomarkers, and simulate therapy outcomes — a foundation for safer, more personalized allergy care.\nThe final layer connects lab science to real-world food safety. Here, we focus on datasets that describe what consumers actually eat, how products are labeled, and how authorities respond to allergen incidents.\nWe curate large multilingual ingredient and product databases such as Open Food Facts , Food Ingredients and Allergens , Ingredients with 16 Allergen Tags , Allergen Status of Food Products , and FSA Allergen Database Service (UK Nut Allergy Registry). Complementary regulatory datasets include Swiss Legislation on Food Allergens , COMPARE , and the FSA Allergen Database Service , which provide consistent allergen codes and labeling standards.\nFor real-world adverse-event tracking, we rely on CAERS (CFSAN Adverse Event Reporting System) , PEAR – Partners’ Enterprise-wide Allergy Repository , and Food: Allergen and Allergy , which capture anonymized clinical reports and recall histories. Government recall sources from FDA , USDA , and CFIA , as well as global registries, are continuously ingested to monitor undeclared-allergen events and labeling failures.\nThese datasets feed into our Multilingual Ingredient and Label Corpus , where text data are normalized through an ontology that maps local terms (“tahini,” “gingelly,” “sesame paste”) to canonical allergens. Synthetic label images are generated to mimic supermarket conditions — glare, blur, curved surfaces, and multi-language fonts — allowing models to learn in realistic settings.\nBy combining structured recall data with visual and linguistic information, this layer empowers AI systems that can read packaging, understand its content, and flag inconsistencies in real time.\nOur collection is available through our dedicated Hugging Face datasets repository . You can explore it interactively using the hf space we've developed, which features name-based search along with convenient filtering options by category, task, and data type.\nInteractive HuggingFace space\nContributing\nWe welcome contributions! Our datasets list is maintained on a dedicated GitHub repository where you can submit pull requests to help us grow the collection.\nThis first work is a testament on the fact that community-driven open science not only is possible, but is a great idea. Take our case: in just a few weeks, more than 20 contributors from different backgrounds came together working on a food allergy related project. This shows how even a specialized scientific topic perceived as nieche can spark geniune interest and momentum.\nIt’s our 0-to-1 moment : proof that when people unite around a clear purpose, even a small initiative can grow into something transformative. And who knows — maybe one day, food allergy research will have its own AlphaFold moment.\nLooking ahead, our focus will shift toward hands-on, scientifically meaningful projects that build on this foundation. Guided by scientific advisors and domain experts, we aim to foster collaborative, community-driven research that advances the science of food allergies.\nOur goal is to harness the power of AI to tackle key scientific questions, such as:\n💡 Get Involved\nWhether you’re a researcher, student, developer, or simply passionate about open science, we’d love to have you join us.\n👉 Apply to contribute or collaborate via our short interest form\n💬 Join the HuggingScience discord community to connect, discuss ideas, and build the future of AI for food allergy research together. For any question, you can reach out to @ludocomito, the team leader for this project.\n🌐 Visit our community wiki to learn more about our initiative and keep track of active projects.\nThe realization of this first project has been possible by the coordinated effort of our contributors, showing that indeed open science is a viable way. In particular, thanks to:\nMoreover, thanks to the 20+ contributors who worked on finding and annotating the datasets for our collection.\nA thorough explanation of key datasets we identified, together with some inspiration for possible food allergies applications.\nSDAP 2.0 is a web server with a database of allergenic proteins and computer programs that help in structural biology research. It allows access to the cross-reactivity between known allergens, screens FAO/WHO allergenicity guidelines for new proteins and predicts IgE-binding ability of genetically modified food. Its activities include anti-allergy drug design, protein structure analysis, prediction of epitopes and prediction of cross-reactivity. SDAP 2.0 contains 1657 hand-curated allergen sequences, 334 experimentally validated and 1565 predicted structures with tools such as property distance and Cross-React to identify IgE-binding epitopes and cross-reactive allergens ( Updated Structural Database of Allergenic Proteins ). Hypoallergenic protein design and immunotherapies are aided by the database as it allows researchers to display epitopes, align structural motifs and screen candidate mutations. For the food allergies, SDAP 2.0 can be combined with DTI data sets to model how small molecules or peptides would interfere with IgE–epitope binding. For AI researchers, SDAP’s rich dataset of allergen structures and epitopes serves as a basis for training models to predict IgE-binding sites or assess how modifications to protein structure might reduce allergenicity.\nThe DAVIS data set contains dissociation constants for 68 drugs against 379 protein targets. It is widely used in benchmarking drug-target interaction prediction models and anti-allergy drug design tasks. Frontiers in Pharmacology recognizes that the Davis dataset provides 30,056 drug–target affinity samples with K_d values that are traditionally used to train sequence-based deep-learning models ( review of the recent advances on predicting drug target affinity ), as these pairs are equiped with continuous affinity measures. Although assembled initially for use with kinase inhibitors, the structure–activity pairs available in the dataset can be repurposed for allergy drug discovery by linking inflammatory pathway targets (e.g., SYK, PI3K) and screening molecules blocking IgE signalling. Because the dataset lacks 3D structures, it is often completed with PDB or ZINC structures for modeling.\nQsarDB is a smart repository that holds quantitative structure–activity relationship (QSAR) and quantitative structure–property relationship (QSPR) models along with their datasets. Its operations entail providing access to peer-reviewed, open (Q)SAR models for anti-allergy drug design. The QsarDB repository stores models in content-aware form and offers facilities for analysis, visualization and prediction ( QsarDB ). For allergy research, annotated models in QsarDB allow the rapid estimation of physicochemical parameters (e.g., lipophilicity, solubility) or biological activity of candidate molecules, which allows triage before the actual execution of DTI simulations. The repository emphasizes transparency and reproducibility; a model consists of documentation and citations, which is important when regulatory agencies require substantial evidence for novel allergen therapeutics.\ne-Drug3D is a three-dimensional database of drug-like molecules and their molecular conformations. It provides structural data for structure-based drug design and is utilized for anti-allergy drug design and DTI applications. According to the official website and an ACS Med Chem Lett paper, e-Drug3D contains over 2,162 structures of FDA-approved compounds with molecular weights ≤2000, including pharmacokinetic and pharmacodynamic information such as volume of distribution, clearance and half-life ( e-Drug3D , Datasets FDA Approved Drugs ). By offering conformers for approved drugs and active metabolites, e-Drug3D allows virtual screening for drug repurposing. For food allergy, researchers can search for molecules that are inhibitors of histamine release or blockers of allergic signal transduction and screen off-target effects with structural similarity.\nThe dataset includes the Offsides database of drug and drug–drug interaction side-effects and the Twosides database of side-effects of drug–drug interactions. The Sci Translational Medicine article on it notes that the authors built a large database of drug effects and side effects of drug–drug interactions, making it possible to correct confounding factors of adverse event reports ( Data-Driven Prediction of Drug Effects and Interactions ). By integrating this dataset with allergy-focused data, AI models are able to predict possible side-effects or cross-reactive adverse effects when on antihistamines or biologics and identify interactions that exacerbate food allergy disease.\nOpen source web-based repository of over 4,950 drugs including structural, physicochemical and pharmacological data. It supports anti-allergy drug design and DTI operations. A 2023 review says that DrugCentral covers ~20,000 bioactivity data points, 724 mechanism-of-action targets, >14,300 on- and off-label indications, 27,000 contraindications and ~340,000 adverse drug events ( DrugCentral ). It relates every drug to curated mechanism-of-action targets and approved indications. For AI researchers working on allergy, DrugCentral provides a ready catalog of all the drugs used in allergic conditions (e.g., epinephrine, antihistamines, corticosteroids, leukotriene modifiers, monoclonal antibodies like omalizumab) and their target profiles. That is, an AI model can just query what approved drugs target IgE or histamine or IL-5, etc., and utilize that as curated training data. As an example, to train a model to predict new antihistamines, one can fetch all histamine H1 receptor antagonists in DrugCentral to generate a positive set. DrugCentral data can also facilitate side-effect prediction models; knowledge of the polypharmacology of allergy drugs (most antihistamines also bind off-target receptors) can allow AI to predict adverse effects or cross-reactivity. Moreover, DrugCentral is widely used for training natural language processing models for pharmacology, as it contains text descriptions of drug indications and effects. It is possible to fine-tune an NLP model on DrugCentral entries to, e.g., summarize the potential action of a new compound, or translate between chemical structure and described effect (a kind of \"Chemo-BERT\" for drug repurposing).\nMedKG is a strong medical knowledge graph involving data from 35 expert sources with 34 node types and 79 relations. MedKG authors share an integrative biomedical knowledge graph with continuous integration and update processes ( MedKG ). In other words, MedKG is a network of biomedical knowledge available for direct algorithmic digging. For food allergy, a knowledge graph is especially valuable to uncover latent relationships: e.g., between an allergen protein in food and the gene encoding it, to a pathway that it activates, to known drugs targeting that pathway. An AI conclusion on MedKG could propose drug repurposing targets – perhaps finding that a drug for the treatment of an autoimmune disease directed against a given interleukin is also applicable for the treatment of food allergy symptoms directed against the same interleukin. MedKG can further denote patient data (if integrated with EMR sources) which would allow AI models to provide predictions like allergy risk or treatment efficacy based upon graph algorithms. Because MedKG contains many types of data, one example of a real-world use is supplying its data to graph neural networks (GNNs) or knowledge graph embedding models to make new link predictions, such as making a prediction that Drug X can treat peanut allergy based on the neighborhood in the graph. Furthermore, the in-built molecular embeddings within MedKG help in correlating chemical space with biological effect, which is extremely useful in allergy drug design where we want to go from a target (e.g., IgE) to \"find me a molecule that binds here and does not have toxicity.\". In summary, MedKG is a cutting-edge, AI-friendly biomedical knowledge graph that offers an integrated view, enabling advanced machine learning algorithms to generate knowledge for allergy therapeutics and personalized medicine\nPDBBind+ refers to an enhanced, “leak-proof” reorganization of this dataset to improve its quality and splitting methodology ( PDBBind+ ). It offers a carefully chosen collection of protein–ligand pairs with known affinities, such that the training and test sets exhibit no significant overlap in either protein or ligand similarity, which is vital for building robust AI models. For the area of allergies, PDBBind/PDBBind+ constitutes the foundation for drug–target affinity predictive models. There are a number of targets of relevance in allergic disease (e.g., inflammatory mediator receptors, arachidonic acid metabolism enzymes, etc.) with structures in the Protein Data Bank and their respective ligands in PDBBind ( PDBBind ). By training on PDBBind+, AI models can be instructed to predict how closely a small molecule will bind to a protein, which is a critical factor in designing drugs to block allergic pathways. For example, if somebody wishes to find new inhibitors of mast cell tryptase (an enzyme in allergic reactions), a previously trained model on general binding data can be re-fitted to whatever tryptase–inhibitor data exist and apply this to virtually screen compounds. Further, PDBBind's focus on 3D structure is orthogonal to ligand-based datasets like DAVIS: combined, they can give better structure-based AI predictions. The \"Plus\" version's focus on data quality and unbiased analysis ultimately leads to AI predictions being more reliable – a necessity when translating into real-world drug discovery for allergies.\nThe HMDB is an exhaustive database of human small-molecule metabolites, with over 220,000 metabolite records found in the human body ( HMDB ). It includes comprehensive chemical details, clinical information (normal and abnormal concentration ranges in biofluids), and references to enzymatic pathways for each metabolite. Far from being an allergy database per se, HMDB is incredibly valuable for studying allergy from the point of view of biomarker discovery and mechanistic insights.Allergic reactions result in release or usage of other metabolites – e.g., histamine (a biogenic amine) is a significant metabolite released from mast cells, and lipid mediators like prostaglandins and leukotrienes (also in HMDB) are produced in allergic reactions. AI algorithms can utilize HMDB to identify metabolic signatures of allergy: using machine learning to apply metabolomic profiles of allergic patients versus controls, it is feasible to find a collection of metabolites that would indicate an impending anaphylactic event or quantify the size of an allergic event. HMDB would provide the benchmark of what those metabolites are and under what biochemical circumstances. Also, during drug design, metabolism needs to be known about – many allergy drugs (e.g., corticosteroids, leukotriene modifiers) include active or inactive metabolites. An AI drug metabolism prediction model would use HMDB's data to predict whether a new anti-allergy molecule will be metabolized into poisonous waste or how it would be excreted. HMDB's relationship of metabolites to pathways and enzymes also suggests that if we're examining gut microbiome actions or dietary interventions on allergy, AI can use HMDB to relate diet-derivative metabolites or microbiota metabolites (like short-chain fatty acids) to immune modulation. In essence, HMDB offers the chemical and clinical context in which to view the metabolic component of allergy so that AI models can bridge proteins and genes to the universe of small molecules that ultimately instigate or detect allergic disease hmdb.ca .\nTTD is a comprehensive database of discovered and documented therapeutic targets that are linked to the drugs targeting them and the diseases they are associated with ( TTD ). In addition to protein targets (enzymes, cytokines, receptors, etc.), TTD also includes nucleic acid targets, along with their pathways and other annotation. In food allergy and asthma (allergic disease clinical manifestations), TTD is a handy information base: it enumerates targets like IgE, IgE receptors, IL-5, IL-13, TSLP, CRTH2, and other immune molecules being targeted for allergy treatment. For use in AI, TTD can be used to define task objectives and construct training data. For instance, one can request TTD for all targets with \"Asthma\" or \"Allergic rhinitis\" – TTD would return a list of studied or validated targets and known ligands/drugs. This can be utilized to direct constructing datasets for drug–target interaction prediction specifically for the category of allergy. Additionally, TTD has information on all the drugs (clinical status, mechanism, etc.), and hence a model could be learned to predict drug efficacy or development phase as a function of target attributes (enabling drug repurposing knowledge). More broadly, TTD's curated set of target–disease–drug associations is a fertile ground on which knowledge graphs can be built or reasoner models that reason over biological networks can be learned. For example, a knowledge graph AI can use TTD data to identify connections between food allergy and other drugs that already address a molecular pathway and suggest off-label applications. Briefly, TTD bridges the gap between molecular target and clinical outcome and allows AI to identify where and how to disrupt the allergic process and with what agents.\nTDC is an effort which provides a variety of benchmark datasets and tasks standardized to be AI-ready across the drug discovery and development pipeline ( TDC ). It spans across over 20 categories of learning tasks – from QSAR property prediction to DTI prediction to drug–drug interaction to clinical outcome modeling – and for each task it collects benchmark datasets with reproducible splits, evaluation measures, and baseline results. The advantage of TDC to AI in drug design for allergies is twofold: (a) It comes with pre-prepared data suitable for what we want to do (e.g., TDC has ADMET data which can be utilized to guarantee a new allergy drug is not highly toxic, or DTI data like DAVIS and KIBA which we have already discussed for binding affinity). (b) It gives a framework for testing models on these tasks in a fair manner. By applying TDC, a researcher can readily determine which models are best at, for instance, predicting binding to a particular allergy-related target or which model most accurately predicts a compound's side effect profile (some side effects such as drowsiness are important in allergy medication). In addition, TDC is continually expanding (now into multimodal and generative tasks), in line with the trend of using various forms of data (e.g., with chemical data along with cell pictures or with text). For instance, an Early Detection of Allergies initiative might import patient health records (if available) – TDC enables importing such clinical data sets and evaluating predictive models (for instance, who will have severe food allergies, based on their medical history – much like the risk stratification data set mentioned in clinical environments). On the whole, TDC does not import new domain-specific information, but rather seeks to most effectively use existing data. By using TDC's benchmarks and tasks, our AI allergy models can be rigorously trained and tested so that when we claim a new model finds, for example, better drug candidates or better allergy predictions, the claim is based on rigorous comparative evaluation.\nSTITCH is a database that integrates known and predicted protein–small molecule (and drug) interactions. It gets evidence from diverse sources: experimental evidence, metabolic pathway databases and binding assays, text mining of the literature, and computer predictions. Essentially, STITCH can be thought of as a big network with the nodes being proteins and chemicals and edges representing an interaction or binding relationship with some degree of confidence. For AI use, STITCH offers a precomputed data resource to train models against drug–target interaction (DTI) or perform network-based inference. In allergy research, STITCH can help identify what food chemicals or food additives cross-react with human proteins (e.g., do some food chemicals interact with immune receptors and act as adjuvants in allergy?). Or it can list all the proteins one anti-allergy drug binds to – useful for polypharmacology modeling. An AI system might use STITCH data to predict on novel interactions: i.e., finding that a food crop flavonoid would bind and block IgE or mast cell receptors and thus be a potential allergy drug. The inclusion of text-mined data leads to an extensive coverage with anecdotal or less-documented interactions that could fall through the cracks of other curated databases. Graph-based AI algorithms function well with such dense relationship data. We can train a graph neural network on the STITCH network and possibly get it to predict new edges (interactions) – e.g., predict which existing drugs would interact with the peanut allergen Ara h 2 (if we consider allergens as \"proteins\" in the network) to inhibit its IgE binding. While not an allergy-specific tool, STITCH is an essential one to enable systems pharmacology approaches, and their integration ensures that our AI platform is capable of considering the global interaction network in addressing allergy drug design allergy drug design .\nM3-20M is a very large open-access dataset of 20 million molecules, designed to support AI-driven drug design with a multi-modal approach ( M3-20M ). Each molecule in M3-20M is provided with multiple representations: its 1D SMILES string, 2D graph structure, 3D conformation, a set of computed physicochemical properties, and even a textual description generated to summarize the molecule’s features. The integration of these modalities (chemical, structural, and linguistic) offers a rich playground for modern AI models (like graph neural networks and transformer-based models) to learn chemical concepts. For allergy drug design, a dataset like M3-20M can be invaluable in training generative models to propose new compounds or in predictive models to estimate properties (e.g., oral bioavailability or toxicity) of candidate anti-allergy drugs. Since it’s multi-modal, one interesting use could be training a model that, given a desired function (like “histamine H 1 receptor antagonist” or “mast cell stabilizer”), can generate a molecule’s description and structure that fits the profile – effectively bridging natural language and chemical design. The sheer scale (20 million compounds) also means that an AI can be exposed to a wide chemical space, including many drug-like and lead-like molecules. This improves the chances of discovering novel molecules that could serve as next-generation allergy therapeutics. In summary, M3-20M is a cutting-edge resource pushing the boundaries of how AI can learn from big chemical data, directly benefiting the search for safe and effective anti-allergic compounds.\nSAIR is a recently released large dataset to accelerate AI in drug discovery, and it is particularly promising for allergy therapeutics ( SAIR ). SAIR consists of over 1 million protein–ligand pairs with experimentally measured binding affinities and 5.2 million 3D co-folded structures of the protein-ligand complexes. That is, for each protein target in the dataset, numerous small molecules (of known potency) are docked into it, providing a rich structural training set for deep learning models. For allergies, SAIR includes many protein targets of allergic disease – i.e., immunological enzymes, mast cell or basophil receptors, cytokines, etc. – and molecules that modulate them. Machine learning algorithms trained on SAIR have the potential to learn how to predict the affinity with which a given molecule will bind to a target, making them useful for virtual screening of candidate anti-allergy drugs. For instance, one may train on SAIR to build a model to find novel high-affinity blockers of the IgE–FcεRI interaction or inhibitors of key cytokines (e.g., IL-4 or TSLP) in allergic inflammation. The size of SAIR's structure–activity data (with millions of examples) also allows for the training of structure-aware AI models with better generalization. By spanning a vast chemical space and many protein conformations, SAIR allows AI models to more accurately predict binding even to new or slightly different targets. This makes it a very valuable resource for the design of small-molecule therapeutics in food allergy (e.g., mast cell stabilizers or IgE-neutralizing reagents).\nAllerBase is a database of allergenic proteins and their properties, built to integrate data from diverse sources (e.g., IUIS allergen listings, Allergome, and literature) with stringent experimental validation ( ALLerBase ). It houses comprehensive entries for known allergens and includes an extensive collection of validated IgE epitopes (over 1,100 IgE-binding peptide sequences from 117 allergens). This is a valuable asset for allergenicity prediction AI models: AllerBase positive (allergen) and negative (non-allergen) examples can be utilized to train machine learning algorithms to identify proteins inducing IgE responses. The fact that epitope data is also provided further allows AI to determine what regions of an allergen are immunoreactive, informing the design of hypoallergenic protein variants (by modifying or removing key epitopes). In summary, AllerBase provides the ground-truth allergen information driving many AI classification and epitope-mapping software in allergy research.\nThe AlgPred 2.0 dataset and webserver represent landmarks in allergen prediction using machine learning ( AlgPred 2.0 ). This dataset contains 10,075 experimentally confirmed allergen sequences and an equal number of non-allergens, and 10,451 experimentally confirmed IgE epitopes for training models. From this data, AlgPred 2.0 trains ensemble classifiers that combine BLAST similarity, epitope mapping, motif discovery, and machine learning to achieve high accuracy (AUC ~0.98) in discriminating allergens from non-allergens. From a practical perspective, this dataset is an AI goldmine: models trained on it can predict if a novel protein (e.g., novel food protein or biopharmaceutical) might be allergenic, guiding safer design. The fact that the recognized IgE epitope sites are among them also enables AI to highlight which areas of a protein are problematic, permitting bioengineers to edit those areas out. AlgPred 2.0 demonstrates how painstakingly curated allergen vs non-allergen datasets, when fed into modern algorithms, greatly enhance our ability for in silico screening for allergenicity risk.\nAllerCatPro 2.0 is a novel protein allergenicity prediction tool that is the first to combine sequence similarity and structure features ( AllerCatPro 2.0 ). It was trained based on so-called \"the most comprehensive dataset\" of allergenic proteins: 4,979 protein allergens, 162 low-allergenic proteins, and 165 autoimmune-allergen proteins, all strictly curated from authoritative databases (WHO/IUIS allergen list, COMPARE, FARRP, Allergome, etc.). AllerCatPro 2.0 leverages this dataset to forecast an input protein's allergenic potential by aligning it with familiar allergens founded on sequence motifs and 3D epitope surfaces. In allergy drug design AI, AllerCatPro 2.0's dataset and approach illustrate the power of multi-modal learning: models considering a protein's 3D structure in addition to sequence can more precisely identify allergenic proteins (or with certainty rule out truly non-allergenic ones). This is important for the development of therapeutic proteins or novel enzymes for food processing – AI algorithms can take into account whether the designed protein might inadvertently have the structure of a known allergen. In total, AllerCatPro 2.0 provides both a thorough allergen dataset and an example of an AI-based solution utilized in allergenicity risk assessment.\nAllergenAI is a platform and a deep learning model that was developed to predict, from just the amino acid sequence of a protein, its potential to be an allergen ( AllergenAI ). The developers of AllergenAI collated and preprocessed training data from three big allergen databases – SDAP 2.0, COMPARE, and AlgPred 2.0 – thereby utilizing thousands of sequences of known allergens and non-allergens as input for a convolutional neural network. By learning directly from sequence patterns, AllergenAI can recognize proteins as allergenic without relying on external features. This AI-by-sequence approach is especially useful for screening proteomes (e.g., proteins of a novel plant or novel protein sources like insects or lab-grown foods) to predict any allergenic hits. The model was also used to identify new potential allergens (e.g., the identification of proteins with high risk in foods like date palm or spinach that were not previously identified as allergens). For drug design and allergy therapy, the significance of AllergenAI is being able to direct protein engineering – one can try amino acid substitutions rapidly and receive an AI-predicted allergenicity score, which can direct vaccine candidate development or hypoallergenic variants with minimal IgE binding.\nNetAllergen-1.0 is a more recent machine learning pipeline (random forest-based) that integrates immunological context in allergen prediction ( NetAllergen-1.0 ). It was built by first collecting a filtered dataset of IgE-binding allergens from AllergenOnline (the official repository of IgE-inducing allergens) and then removing redundancy for the purpose of having a clean dataset. Most notably, NetAllergen includes a novel feature for each protein: its computationally predicted MHC class II presentation propensity (a critical step in the way T-cells are activated in allergies). A mix of traditional features (motifs, sequence similarity, etc.) with MHC-II presentation scores assisted NetAllergen in achieving improved accuracy, especially on allergens with low sequence similarity with established allergens. This approach – including immune processing data – is very relevant to AI drug design for allergy. It suggests models can take into account not just whether a protein is similar to a known allergen, but whether it would be seen by the immune system (through antigen presentation). The high-quality dataset NetAllergen is drawn from (constructed from AllergenOnline and filtered out of duplicates) provides a gold standard for developing next-generation allergen predictors. Overall, NetAllergen demonstrates how AI models may be constructed by combining immunological knowledge, paving the way for the creation of proteins or peptides that can be sidestepped from being able to cause T-cell and IgE responses.\nQM9 is a standard dataset in molecular machine learning and contains approximately 134,000 small organic molecules with high-accuracy quantum-mechanical properties . Each molecule has 3D geometries and 13 computed physical and electronic properties such as dipole moment, isotropic polarizability, energies (HOMO/LUMO), enthalpy, and free energy ( QM9 ). Molecules in the dataset are drawn from the GDB-17 chemical universe and are drug-like and chemically diverse, making QM9 a representative benchmark for graph neural networks , transformers , and equivariant models in chemistry.\nUnder the field of AI-based food allergy drug design , QM9 forms the core foundation to acquire quantum-accurate molecular representations which can then be improved upon domain-specific drug–target datasets such as DAVIS , PDBBind+ , and SAIR . Through learning inherent relationships between molecular structure and physicochemical properties, QM9-trained models can extrapolate stability , solubility , reactivity , and binding potential of novel anti-allergy compounds. As an example, quantum-level properties from QM9 can guide AI models to predict small molecules inhibiting IgE–FcεRI binding with favorable energetic and pharmacokinetic properties.\nBesides, QM9 is a significant pretraining dataset for generative AI models that create chemically reasonable, low-energy molecules of relevance to allergy therapeutics. Quantum characteristics of the dataset constraint physical plausibility on created compounds in a way that virtual screening or molecular optimization pipelines are maintained chemically plausible. Thus, QM9 is not directly aimed at allergy but forms the backbone of the molecular intelligence that new AI systems employ while designing secure and effective allergy medications."}
{"url": "https://huggingface.co/blog/huggingface/datacamp-ai-courses", "title": "Announcing Hugging Face Fundamentals: A New Learning Track on DataCamp", "date": "", "content": "We're excited to announce a partnership with DataCamp to bring comprehensive and interactive Hugging Face courses to AI/ML engineers and software engineers.\nThe new Hugging Face Fundamentals skill track combines hands-on courses with a real-world project designed to help you master building modern AI workflows with the Hugging Face ecosystem.\nAll courses feature interactive, real-world exercises that run directly in your browser, no local setup required, ensuring you gain practical skills applicable to your AI projects immediately.\n[!TIP] If you're new to Hugging Face, or working with folks who are, this skill track works through progressive courses from using the Hub to building multi-modal applications and AI agents. Plus, the first course is free until the end of 2025. Start learning today with zero cost or commitment!\nBuilt in collaboration with the Hugging Face team, the fundamental skill track consists of four comprehensive courses and a real world project that progressively build your expertise.\nThis 2-hour beginner course is your comprehensive introduction to the Hugging Face ecosystem. Perfect for newcomers to AI and machine learning, it walks you through the fundamentals of working with pre-trained models and datasets. Covered in this course:\nTake a deeper dive into LLMs, their transformer architecture, and Hugging Face’s transformer library. This course moves inference into custom fine-tuning and evaluation to set you up for real-world projects.\nThis advanced course expands your AI toolkit beyond text to modalities including mages, audio, video, and their combinations. As AI applications increasingly require understanding and generating content across multiple modalities, this course positions you at the forefront of modern AI development. What you’ll explore in the course:\nThis cutting-edge course introduces you to the future of AI: autonomous agents that can reason, plan, and execute complex tasks independently. Using Hugging Face's lightweight smolagents framework, you'll learn to build AI systems that go far beyond simple question-answering to become true digital assistants capable of interacting with the world. Your learning outcomes:\nThe skill track finishes with a real-world project to build a food image classification with the hub, transformers, and everything you’ve learnt!\nThe entire track is designed for progressive learning, but you can also take individual courses based on your specific skill level, interests, and goals. Check out the Hugging Face Fundamentals Track to get started."}
{"url": "https://huggingface.co/blog/isaacus/introducing-mleb", "title": "Introducing the Massive Legal Embedding Benchmark (MLEB)", "date": "2025-10-17", "content": "We're announcing the release of the Massive Legal Embedding Benchmark (MLEB) , the largest, most diverse, and most comprehensive benchmark for legal text embedding models.\nMLEB contains 10 datasets spanning multiple document types, jurisdictions, areas of law, and tasks.\nTo do well on MLEB, embedding models must demonstrate both extensive legal domain knowledge and strong legal reasoning skills.\nOn MLEB, our newly released Kanon 2 Embedder legal embedding model scores highest while simultaneously maintaining the lowest inference time of all commercial competitors, highlighting the extreme accuracy and efficiency gains to be had from domain adaptation.\nIn the process of training Kanon 2 Embedder , we found that the only two existing benchmarks for legal embeddings, LegalBench-RAG and the legal split of the Massive Text Embedding Benchmark (MTEB) , were either of low quality or low diversity.\nWith regard to LegalBench-RAG, we found that it included only 4 evaluation datasets, and all datasets consisted entirely of contracts. In practice, legal professionals and users seeking legal advice or knowledge tend to search for and be interested in a much broader range of document types, including legislation, regulations, cases, and general legal literature. Additionally, the datasets were largely dominated by US contracts, reflecting the broader overrepresentation of American law in legal benchmarks and public legal datasets.\nIn respect of the legal split of MTEB, we observed two key issues.\nFirst, we found a significant amount of mislabeling.\nAILA Casedocs and AILA Statutes , in particular, comprising 25% of the legal split and 40% of English data in the split, contain many query-passage pairs that are totally irrelevant to each other. Upon review of the authors' paper , we discovered the cause to be that the datasets had been created using an 'automated methodology' that paired 'facts stated in certain [Indian] Supreme Court cases' with cases and statutes that had been 'cited by the lawyers arguing those cases'. According to the authors, 'actually involving legal experts (e.g., to find relevant prior cases / statutes) would have required a significant amount of financial resources and time'.\nThe second issue we found with the legal split of MTEB was that it lacked diversity in the areas that matter most to legal practitioners and seekers of legal knowledge.\nOf the remaining English-language datasets after exclusion of AILA Casedocs and AILA Statutes, two deal with consumer terms of service (Consumer Contracts QA and Legal Summarization), leaving only one (Corporate Lobbying) that deals with legislation, and none dealing with case law. All such datasets are again largely representative of American law.\nRegarding the non-English-language datasets in the legal split of MTEB, we argue that, in many cases, the legal systems of different cultures may fundamentally differ in ways that make cross-jurisdictional comparisons (e.g., between the common law system used by Anglosphere countries and Sharia law) of the effectiveness of legal embeddings inappropriate.\nFurthermore, given that the legal split contains two German datasets, one Chinese dataset, and no other non-English datasets, and that those datasets are concentrated on three select legal tasks, we argue that the inclusion of non-English datasets largely introduces bias and noise in ways that are unlikely to be conducive to real-world performance on most English-language legal information retrieval tasks.\nLearning from the limitations of existing legal embedding benchmarks, we designed MLEB with four key objectives in mind, namely to:\nTo that end, MLEB contains 10 different evaluation sets spanning a range of difficulties (including tasks requiring legal reasoning as well as tasks requiring lexical analysis), problem types (specifically, retrieval, zero-shot classification, and question answering), jurisdictions (the US, UK, Australia, Ireland, Singapore, and the EU) and document types (decisions, legislation, regulations, contracts, and literature).\nOf the 10 datasets in MLEB, 7 are entirely new, constructed either by having subject matter experts hand-label data or by adapting existing expert-labeled data.\nOne of the most valuable constituents of MLEB is the Australian Tax Guidance Retrieval dataset. This dataset pairs 112 real-life tax questions posed by Australian taxpayers with 105 relevant Australian Government guidance and policy documents.\nWe constructed this dataset by sourcing questions from the Australian Taxation Office's community forum, where Australian taxpayers ask accountants and ATO officials their tax questions. We found that, in most cases, such questions can be answered by Australian Government guidance materials that, for whatever reason, taxpayers were unable to locate themselves. Accordingly, we manually went through a stratified sample of challenging forum questions and extracted guidance materials linked to by tax experts that we confirmed to answer such questions.\nWhat makes this dataset so valuable is that, unlike the vast majority of legal information retrieval evaluation sets currently available, this dataset consists of genuine, challenging real-world user-created queries, rather than artificially constructed queries that, at times, diverge considerably from the types of tasks embedding models are actually used for.\nThe queries are valuable and challenging precisely because users have gone to the effort of asking them on a forum, indicating that traditional search engines failed to surface the answers they were looking for. The relevant materials are, in turn, also valuable because accountants and ATO officials have confirmed them to be relevant, and we have independently affirmed their relevance.\nThis dataset is just one of several others that we invested considerable, painstaking effort into ensuring the usefulness and quality of.\nBelow, we present an overview of all the datasets included in MLEB alongside all the various features that make them unique.\nAs part of our long-standing commitment to open and accessible legal data, AI, and tech, we've ensured that MLEB and all its constituent datasets are licensed as permissively as possible. We've also publicly released our evaluation code and raw results on GitHub to assist with the consistent and reproducible evaluation of models on MLEB.\nAs of 16 October 2025, Kanon 2 Embedder , our newly released legal embedding model, ranks first on MLEB with an NDCG@10 score of 86%, followed by Voyage 3 Large at 85.7%.\nInterestingly, we find that the qualities that make an embedding model good at general multilingual information retrieval tasks are not necessarily the same as those that make a model good at legal information retrieval.\nGemini Embedding ranks 1 st on MTEB and Voyage 3.5 ranks 23 rd , whereas on MLEB, Gemini is only 7 th and Voyage 3.5 is 3 rd .\nWe observe that strong performance on MLEB seems to correlate with legal domain adaptation.\nLast year, Harvey announced they had partnered with Voyage to train a custom embedding model on private legal data, which may explain, in some part, why they outperform Qwen, Gemini, OpenAI, and Jina models.\nWe note, however, that there is, unfortunately, a serious risk that Voyage's models were trained on some of the evaluation sets in MLEB, particularly SCALR and Consumer Contracts QA, which are both also part of MTEB, due to the fact that Voyage trains on their customers' private data by default (which would invariably include benchmarks). This is also a risk for Cohere and Jina models.\nNevertheless, despite also being much smaller in size than Voyage 3 Large, Kanon 2 Embedder manages to punch far above its weight thanks to the enormous amounts of high-quality, licensed legal data it was trained on alongside several design improvements our team made to the standard recipe for building an embedding model.\nKanon 2 Embedder takes the number one spot in overall performance as well as in the domains of caselaw and regulation. It is also third best at tasks involving contracts, coming slightly under Voyage 3.5 and Voyage 3 Large.\nAdditionally, due to its extreme parameter efficiency, Kanon 2 Embedder manages to set the new Pareto frontier in balancing inference time with accuracy. Indeed, of all the commercial models we tested, Kanon 2 Embedder was the fastest. Compared to Voyage 3 Large, in particular, our model was four times faster.\nThis is only the beginning for us. Our vision is to solve every common AI- and data-related pain point of the legal tech industry, from building reliable and robust domain benchmarks to training state-of-the-art legal AI models that redefine what's possible.\nOver the coming months, we aim to release the world's first legal grounding API, allowing legal tech professionals to plug their models into the Blackstone Corpus, our private, living repository of high-quality legal data, to build their own legal AI applications, from search engines to chatbots.\nWe will be baking Kanon 2 Embedder directly into our grounding API to ensure our customers always get what they're looking for. We also plan on contributing portions of the Blackstone Corpus, as we have already done, back into the open-source community, including into MLEB.\nIf you'd like to be part of our journey to push the boundaries in legal AI, we encourage you to join our platform , explore our docs , and reach out .\nYou can also stay up to date on our progress by following us on LinkedIn and Reddit .\nNote: unfortunately, because Cohere's terms of service entirely forbid benchmarking as well as forbidding competitors from joining their platform, none of their models could be included in MLEB. We note that Cohere was the only commercial provider we encountered with such terms."}
{"url": "https://huggingface.co/blog/hansolosan/granite-embedding-r2", "title": "Granite Embedding R2: Setting New Standards for Enterprise Retrieval", "date": "", "content": "In this post:\nWhen it comes to enterprise information retrieval, organizations face a persistent challenge: existing embedding models force you to choose between accuracy and speed, between long-context support and commercial licensing, between general-purpose performance and domain-specific excellence.\nOn August 15th 2025, we’ve introduced the Granite Embedding R2 models — a comprehensive family of retrieval models designed to reduce the impact of these tradeoffs.\nThe Granite Embedding R2 release includes three models, all available under Apache 2.0 license:\nThese models deliver three improvements over our first-generation release:\n(Want to skip straight to the code? We get it — jump to the examples and start embedding things.)\nThe R2 models leverage the ModernBERT architecture, incorporating recent advances in encoder design:\nWe trained these models on 2 trillion tokens from high-quality sources including GneissWeb , Wikipedia, and Granite Code data. Every dataset underwent comprehensive governance review, with screening for personal information and profanity — because enterprise deployments demand transparency and responsible AI practices.\nWhat sets Granite R2 apart is our five-stage training methodology:\nThis pipeline enables a single model family to excel across remarkably diverse tasks.\nWe evaluated Granite R2 on six open source retrieval benchmarks part of MTEB benchmarks (MTEB v2, CoIR, TableIR, LongEmbed, MTRAG, and MLDR), and the results demonstrate clear leadership in both accuracy and speed, as shown below.\nSolid bars represent models under 500M parameters, and hashed bars are for models under 100M parameters. Corresponding families share the same fill color.\nAs the chart shows, the granite-embedding-english-r2 model achieves the highest average performance at 59.5 NDCG@10, outperforming all comparable open-source models — including models that are twice its size. Even our efficient granite-embedding-small-english-r2 scores an average of 55.6, surpassing many larger open-source competitors. As of this writing (October 6th, 2025) — if one builds an English benchmark on the MTEB site with Reranking and Retrieval as tasks (which are the two objectives of the R2 granite embedding models) — the granite-embedding-english-r2 model is ranked first (as seen below) among the models with less than 500M parameters¹.\nSimilarly, the granite-embedding-small-english-r2 is ranked second for models under 100M parameters:\nPerformance benchmarks often overlook a critical real-world constraint: encoding speed. When you’re ingesting millions of documents with frequent updates, speed directly impacts operational costs and user experience.\nWe benchmarked text embedding speed on a dataset of 23,000 IBM technical documents (averaging 6,393 characters, ranging from 10 to 475,001 characters, details in — you guessed it! — our paper ):\nThe R2 models are 19–44% faster than leading competitors and as fast as the R1 models, despite the R2 models having slightly more parameters. The ModernBERT architecture’s optimizations — particularly Flash Attention — enable this efficiency gain.\nThe speed advantage becomes even more pronounced with the small model, which processes nearly 200 documents per second while maintaining competitive accuracy. This makes it ideal for real-time applications and high-throughput ingestion pipelines. All experiments were run on a H100, with a context size of 512 tokens and a batch of 128.\nThe reranker model completes the retrieval pipeline. Built on the granite-embedding-english-r2 model, it uses a PListMLE loss objective for position-aware ranking. Below is the comparison of the performance of the granite-embedding-english-reranker-r2 compared with a few open source rerankers of similar size:\nThis retrieve-and-rerank framework maximizes both recall and precision without severe computational overhead.\nGranite models prioritize enterprise requirements, including\nMore about IBM’s open source LLM policy: https://www.ibm.com/granite/trust\nAll Granite Embedding R2 models are available now on Hugging Face under Apache 2.0 license:\nFor technical details, architecture description, and comprehensive benchmark results, see our research paper .\nHead to granite embedding R2 models jupyter notebook to test/deploy these models, and visit the links above to read the models' cards. Please consider giving us a ❤️ if you find the model useful!\nInformation retrieval isn’t just about finding documents — it’s about enabling AI systems to access relevant knowledge efficiently. Whether you’re building RAG applications, semantic search engines, or recommendation systems, embedding quality and speed determine what’s possible.\nGranite R2 models don’t force you to choose between accuracy and speed, between long-context support and efficiency, between general-purpose capability and domain-specific performance — they deliver all of it.\nIn an era where milliseconds matter and accuracy cannot be compromised, Granite R2 models don’t just meet the standard — they set it!\nThe Granite Embedding R2 models represent collaborative work across IBM Research teams in multiple geographies. For questions or feedback, visit our GitHub repository. This work is a collaboration with many people at IBM Research including (in alphabetical order): Parul Awasthy, Ken Barker, Riyaz Bhat, Meet Doshi, Martin Franz, Bhavani Iyer, Vishwajeet Kumar, Yulong Li, Rudra Murthy, Vignesh P, Salim Roukos, Jaydeep Sen, Aashka Trivedi, Todd Ward, and Yushu (Elaine) Yang.\n¹To generate the comparison tables above, go to the MTEB leaderboard, select “General Purpose”/English on the left, then open the tie “Customize this Benchmark” on the right and remove every task but “Retrieval” and “Reranking”. Finally, open the tie for “Advanced Model Filters” and select models “<500M” Model Parameters."}
{"url": "https://huggingface.co/blog/driaforall/mem-agent-blog", "title": "mem-agent: Equipping LLM Agents with Memory Using RL", "date": "", "content": "Ömer Kaya — Dria, omer@dria.co\nOctober 09, 2025\nCode · Model\nLarge language models (LLMs) equipped with tools and a multi-turn action-feedback loop have been a popular direction of research, especially after the recent \"Cambrian explosion\" of reasoning models following the success of Deepseek-R1. With the increasing capabilities of small, open LLMs, the research in this area has become more accessible to a wider audience, leading to a wide effort in both open and closed research in LLM agents trained with reinforcement learning (RL). One area LLMs fundamentally lack is persistent state over conversations, which could be thought of in the form of a memory. This paper introduces mem-agent , a 4B LLM agent trained with GSPO on a scaffold that uses Python tools and markdown files to equip an LLM with memory. We outline a scaffold, data generation pipeline, the training process, and the application of the resulting agent. We also introduce md-memory-bench , a hand-crafted benchmark to evaluate LLMs on their proficiency in this scaffold. Using the benchmark, we demonstrate the success of our training setup as our model scores (75%) on the benchmark, second only to Qwen3-235B-A22B-Thinking-2507.\nAfter the introduction of Deepseek-R1 (Guo et al., 2025), the research on LLM agents trained with RL or more specifically, Reinforcement Learning with Verifiable Rewards (RLVR) (Lambert et al., 2024) has become the de-facto standard in post-training research, eclipsing the dominance of SFT (Ouyang et al., 2022) only pipelines in post-training. Since then, many open source frameworks (Brown, 2025; Hu et al., 2024; Sheng et al., 2025; von Werra et al., 2020) have either been released or adopted the techniques used Deepseek-R1, mainly multi-turn GRPO (Shao et al., 2024) with interleaved reasoning. The adoption of GRPO and its derivatives like Dr.GRPO (Liu et al., 2025) and GSPO (Zheng et al., 2025) by both open and closed source labs and researchers set the stage for a \"Cambrian explosion\" of LLM agents trained with multi-turn RL with interleaved reasoning in the open source research scene.\nThe biggest use-cases of these LLM agents came, to no surprise, from closed labs and their model and product offerings. State-of-the-art (SOTA) models like GPT-5 (OpenAI, 2025a), Claude 4 Opus & Sonnet (Anthropic, 2025b) are natively trained with multi-turn tool and thinking interleaved RL and have reached wide usage in applications like Claude Code (Anthropic, 2025a), Cursor and open-source options like Zed and Cline.\nThe overall promise of these LLM agents with multi-turn tool calling, whether they are open or closed, is immense. SOTA models get better and better over time in terms of completing longer and more complex tasks (Kwa et al., 2025), and open models like the Qwen3 series (Yang et al., 2025),, GLM-4.5  (Zeng et al., 2025) and Kimi K2 (Team et al., 2025b) have equalled or surpassed the performance of their closed counterparts on many agentic tasks. With all the promise these agents might deliver, an everlasting problem with LLMs and their applications is the problem of context. Context has been an issue to deal with in language modeling long before there were transformer-based LLMs, and it is a problem of three facets:\nThese problems make it clear that the ideal LLM agent should be context efficient. This becomes even harder when one tries to imbue the agent with extra knowledge with methods like continual pre-training (Ke et al., 2023) or SFT with on-policy samples, as they are gradient-based methods whose compute requirements scale exponentially with increasing context length. A possible remedy for this is Retrieval Augmented Generation (RAG) (Lewis et al., 2020), which sets up a (usually embedding model-based) scaffold for relevant content/document(s) to be retrieved according to the user query, and given to the LLM so it can generate a response accordingly. This method evolved into a sub-field called Agentic RAG (Singh et al., 2025), where the agent is equipped with a scaffold including tools and sometimes graph components. Overall, most of these methods are not dynamic: they give static knowledge to LLMs which cannot be deleted or modified, which can be thought of as equipping them with a read-only database. Ideally, an LLM agent would have a dynamic memory: one that can evolve over time with the usage of the agent, which would be a step forward in the direction of the ever-far promise of \"continual learning\".\nMotivated by the above problems and conclusions we have crafted a scaffold inspired by Obsidian, with Python tools and markdown files in folders. Based on this scaffold, we have determined three major tasks for such an agent to perform:\nAfter determining the tasks, we built a graph-based data generation pipeline for the synthetic generation of training samples for the aforementioned tasks. The data generation process was followed by building the training pipeline and improving it over many experiments. Finally, we've handcrafted a new benchmark, md-memory-bench , to evaluate the performance of our trained agents and other open and closed SOTA models.\nOverall, our contributions are fivefold:\nTo understand how we arrived at the scaffold we have in terms design choices, we must first mention the state of research in the sub-field of memory in LLMs. The most recent survey on the topic came out last year (Zhang et al., 2024). The paper goes into great detail about different formats of memory (textual and parametric), memory operations (writing, reading and management), memory sources and applications of LLM agents with memory, for example a Personal Assistant. In it, many important papers in the field are mentioned, like Voyager (Wang et al., 2023),, Generative Agents (Park et al., 2023), MemGPT (Packer et al., 2023) and Reflexion (Shinn et al., 2023). The paper, as detailed a survey as it is, does not make a distinction between declarative, procedural and episodic memory, which we deem important when discussing memory in LLM agents.\nIn Voyager, a Minecraft-playing agent is equipped with a skill library consisting of executable JavaScript code that describes in-game actions that the agent can perform. The model can add new \"skills\" to this library and modify the ones already present, effectively giving it a procedural memory. In Generative Agents, the agent writes observations to a memory stream which are then retrieved by the model with a deterministic mechanism that takes recency, importance and relevance into account. This method equips the agent with a semi-episodic memory in a declarative format. MemGPT, which was a primary inspiration for this work, has an intricate scaffold with a queue, a working memory of sorts, and an explicit flow of memory from working memory to long-term memory. In it, the agent is given tools to add data into, modify data in and delete data from the memory system. Finally, in Reflexion, there's a 3 LLM training setup with an Actor, an Evaluator and a Self-Reflector. The Self-Reflector LLM summarizes the experience from a trajectory in a shorter text, which is then added to an \"Experience\" buffer, which serves as a long-term memory for the Actor LLM.\nThe argument could be made that the memory systems in Generative Agents and Reflexion are more declarative than they are episodic, as they are not the raw conversation/trajectory data but the LLM generated summaries of that data. For the purposes of this paper, they will be considered declarative and we will deal with a binary classification, with only declarative and procedural memory being considered. This is mainly because true \"episodic\" memory in LLMs is the trajectory itself, which we don't want to save fully in its raw form due to aforementioned problems with increasing context length. The trajectory can also be considered as the working memory of the LLM, but that is beyond the scope of this paper.\nThe road to the elusive Artificial General Intelligence (AGI) or for our purposes, an Artificial Generalist Agent (AGA), no doubt crosses through the research and successful application of procedural memory in SOTA agents, LLM or not. The ideal of continual/lifelong learning realistically requires an agent to not only be able to continuously acquire new information, but also be able to acquire new skills, which themselves can be a combination of already acquired skills. The work in this field is even thinner than the work on declarative memory, as crafting training pipelines for procedural memory agents requires an order of magnitude more effort in scaffold design, data/task generation, reward shaping and the overall training setup. AlphaEvolve (Novikov et al., 2025), where a library of programs is continuously \"evolved\" by an ensemble of LLMs, and DynaSaur (Nguyen et al., 2024), where a library of programs (seeing a pattern here) is continuously updated and used by single LLM, can be attributed to research in procedural memory in LLM agents.\nLLM agents, more often than not trained with some successor of GRPO and if not, PPO (Schulman et al., 2017), are the main focus of LLM research in 2025, especially in open source research. With improvements both in the frameworks mentioned before and the models themselves, the research on LLM agents has grown in breadth and depth. Following the success of OpenAI's DeepResearch (OpenAI, 2025b), a plethora of papers have been published on \"Deep Research\" agents (Muennighoff et al., 2025; Xia et al., 2025), which remains a popular task/scaffold in LLM agent research. Beyond deep research, other major application areas for LLM agents are computer use and software engineering, both of which are ever-growing fields with training pipeline after training pipeline, benchmark after benchmark and product after product released in each one (Lin et al., 2025; Liu et al., 2024a; Sager et al., 2025).\nLLM agents in their most basic form are LLMs with a list of tool specifications. These agents, given a task/user query:\nTurn after turn until they deem the task completed and reply to the user with a final answer, or until they run out of available conversation turns/tool calls. A lot of open research and products have focused on JSON-based tool calling, which itself was inspired by the first \"Function Calling\" standard set by OpenAI. In their closed-source offerings like o3, GPT-5 and Codex models however, they opt for Python-based function calling, in which the agent/model responds with a Python code block as an action. The Claude models in their web interface also use JavaScript code blocks for their \"analysis\" tool, which also suggests that using direct blocks of code for acting is a better choice than providing a JSON with arguments for a single function/tool. This claim is supported by the results of the Dria Pythonic Agent Benchmark (DPAB) (Tekparmak and andthattoo, 2025a), where all but one model evaluated performed better in the same task when using Python code blocks instead of JSON. This is a slightly more effort-taking direction of research as many of the open source LLM RL training frameworks support only JSON-based tool calling by default, and need modification/extension of the codebase to support Python code blocks as actions. Given the fact that the reasoning ability in LLMs are driven by the amount of procedural knowledge in their pre-training data, most of which is in the form of code  (Ruis et al., 2024), and the fact that most of that code is probably Python code (GitHub, 2024), and the proven effectiveness of Pythonic tool calling over JSON-based tool calling, we have decided to build our scaffold with Python code as actions.\nWith the LLM agent research booming after the advances following GRPO and Deepseek-R1, and the paradigm shift from gradient-based methods to test-time compute methods in the field after the release of OpenAI's o1 (Jaech et al., 2024); the ideas of \"learning to learn\", \"lifelong learning\" and \"test-time evolution\" are pursued more than ever. Studies like AlphaEvolve, GEPA (Agrawal et al., 2025) have further shown the effectiveness of test-time compute while lifelong learning itself is less of a focus. To reach this goal, a reliable memory system is absolutely necessary. Such a system should be:\nGiven that a memory system that abides by these criteria would be used by a ReAct-like agent with multiple turns of thinking, tool calling and feedback from the environment, such an agent could be trained with RLVR on how well it can use a memory system like that. This, we think, is the \"low-hanging fruit\" in the field of LLM agents research and we believe research in this direction is a key one on the road to AGI. Because of this, we have decided to design a scaffold that would embody an LLM agent with tools to interact with a dynamic memory system and then create a training pipeline to train an agent with RLVR to learn how to use this scaffold.\nWe were mainly inspired by our own usage of Obsidian, a popular note-taking application based on markdown files and folders that contain them. In it, users can link files with others via a special syntax, and then open the \"Graph View\" of their knowledge base, which has files as nodes and links as edges. The users can also browse the files in a hierarchical filesystem-like view, which provides a 2-dimensional way to organise and update their knowledge base. Because of its many ways to organise data, the human-readability of the markdown format and its ease of use, it's very popular among many users worldwide. For the same reasons, we think a memory system resembling that would be a great fit for an LLM agent to use, because:\nDue to these reasons, we have decided to design our scaffold around an Obsidian-like system managed by Python tools given as blocks of code by the agent.\nThe memory lives under a single root directory and follows a structure:\nbased on the following file conventions:\nLinks are Obsidian-style but must be written exactly, with the full relative path from the memory root and the file extension included:\nUser and entity files contain sections and subsections declared with # and ## , and key-value pairs with the keys following the snake_case . Entity files are also snake_case , and are named after the entity. Below are example user and entity files.\nuser.md.\nentities/dria.md.\nWe equip the agent with a set of tools that cover the interactions we expect in an Obsidian-like memory: creating, reading, updating and deleting files, navigating folders, and jumping across links.\nThe main agent loop is as follows:\nThe agent can output content in three different XML blocks:\nThe full system prompt for the agent can be found in Appendix — Agent System Prompt.\nGiven the memory format, the tools in the scaffold, and the agent loop, we outline three main tasks that an effective agent using this scaffold should be able to perform:\nThese tasks, we believe, cover the majority of functionality our memory agent should be able to have for continuous, reliable and effective use of the memory scaffold.\nIn this section we will talk about our inspiration for the data generation pipeline given our preliminaries, the do's and don'ts we learned throughout our iterative development process and the final data generation pipeline we ended up with. Given that the entirety of our training data is synthetically generated, we find the lessons learned during this stage of the study to be highly valuable and relevant to discuss.\nWe reached to two main conclusions after finishing the development and release of dria-agent-a:\nThe first conclusion helped us with the design of our scaffold. The second conclusion, was the main driver for our data generation pipeline research and development process. Our initial work with dria-agent-a had a pipeline similar to the one in  APIGen (Liu et al., 2024b), where the focus was on generating Q&A pairs given a library of functions and prompts, as the focus of our initial agent was to train a decent function-calling agent that proved Python is a better medium than JSON for the task. For this work, because our focus was on instilling the memory management capabilities in the agent, we had to take a different approach to data.\nOur initial idea was to implement an entire type system and resolution engine like the one in Prolog (Colmerauer and Roussel, 1993), and add a bunch of primitives and possible constraints to generate the data for an entire sample with only a set of initial constraints. This was partially inspired by Absolute Zero  (Zhao et al., 2025), where the authors give the system a single triplet of input, Python function and output, and then have a proposer propose tasks based on that initial triplet and the solver solve them. The success of that paper proved that an LLM could keep learning not only the given tasks but also overall problem solving and reasoning capabilities with synthetic data.\nThis idea of generating synthetic data with a resolution engine based on static primitives turned out to be nothing more than a \"nerd-snipe\" (Munroe, 2007) for the purposes of generating data for this scaffold and task formulation. Not to say that it's a dead-end avenue of research overall, but for the purposes of this study and the potential effort it would take to implement reliably, we opted for a simpler data generation pipeline that would be powered by LLMs and their structured output capabilities.\nThe base data schema we first implemented was Persona , which contained personal information of a person and their relationships with other personas. Using these personas, we generated Momentary Stories : logs of what happened in a persona's life with a description and a timestamp. The personas and their momentary stories were then used to generate Facts about this persona, in a key-value format wherever applicable. These would then be combined into a Knowledge Base (KB) with all the personas and their facts. This knowledge base would then be used to generate multi-turn conversations between the personas, assumed by two different LLMs. The initial goal of generating multi-turn conversations was due to our thinking that a successful small language model (SLM) would need some SFT warmup, as was the norm in post-training research at the time. The momentary stories generated this way were found to be incoherent and inaccurate, which harmed the downstream multi-turn conversation data too. We then got rid of momentary stories and generated the conversations from a KB comprised of only personas and their facts.\nWe iterated and iterated on this base pipeline, hoping to alleviate the cascading unreliability due to many stages of synthetic data creation without explicit constraints that depend on a single initial seed, which was the \"scenario\". The data generated, both the KB itself and the multi-turn conversations did not reach the quality we wanted for our training. First, we got rid of the SFT pipeline as we trusted RLVR alone to be enough for training the model we wanted (which it was), and then we started working on a more reliable and deterministic, graph-based pipeline for our data generation.\nAfter iterating through various approaches that proved unreliable, we converged on a graph-based pipeline that provides deterministic, high-quality synthetic data generation. The pipeline leverages knowledge graphs as an intermediate representation. Our core insight is to decompose the data generation task into manageable subtasks, each processed by LLMs with focused, limited context. These subtask outputs are then composed at the macro level through a deterministic graph-based framework that preserves structural integrity and prevents hallucination of multi-hop relationships and attributes.\nThe pipeline accepts scenario configurations containing world descriptions and generation parameters. Each scenario generates 3--5 user-centric memory instances with 10--30 Q&A pairs per iteration. The critical design choice is using NetworkX directed multigraphs as the intermediate representation, enabling deterministic traversal and modification while supporting multiple edges between nodes, essential for modeling complex relationships like works_with and manages between the same entities.\nGraph construction occurs in three deliberately separated phases to prevent cascading hallucinations. First, person and entity stubs are generated with minimal attributes (only id and name), constraining the LLMs' creative scope. We employ Pydantic models with strict type validation, rejecting any response that includes extraneous fields, a common LLM tendency that corrupts downstream processing.\nThe relationship inference phase presented unique challenges. LLMs frequently generated edges referencing non-existent nodes or used names instead of IDs. Our solution involves a fallback mechanism: when an edge references an invalid ID, the system attempts name-based resolution before rejection.\nAttribute enrichment leverages the graph's neighborhood context. For each node, we construct a human-readable representation including all 1-hop relationships and neighbor attributes. This localized context prevents the LLM from inventing contradictory attributes while maintaining consistency. For instance, ensuring restaurant employees have food-service-related skills rather than unrelated expertise.\nThe memory export generates a 2-hop markdown knowledge base for each focal person node. The system includes all nodes at exactly distance 2 from the focal node, ensuring these entities are only reachable through intermediate connections rather than direct relationships. By definition, these 2-hop neighbors provide information about the focal person's extended network that cannot be accessed through direct 1-hop traversal.\nCross-references use Obsidian's double-bracket syntax, but enforcement proved challenging. LLMs consistently attempted shortcuts like [[john]] instead of the required [[entities/john_smith.md]] . We address this through post-processing validation that enforces full relative paths and adds missing extensions.\nGenerating coherent multi-hop questions requires careful path construction. The system traverses the graph to identify valid paths, then constructs questions that naturally follow relationship chains. A critical issue emerged with relationship directionality: questions must respect edge direction to remain semantically valid. For instance, \"What is the age of the person who manages John?\" requires an incoming \"manages\" edge to John, not an outgoing one.\nThe two-stage generation process prevents a common failure mode where LLMs generate plausible but structurally invalid questions. By constraining initial generation to graph-verified paths, we ensure all questions have corresponding answers in the memory structure.\nUpdate generation presented the most complex technical challenges. For relationship updates, we must maintain graph consistency when replacing nodes. The system creates a copy of the graph, removes the targeted edge, generates a new node with basic attributes (name and type), and establishes the new connection. While the new node receives minimal initialization, the surrounding context from the world description ensures semantic consistency.\nThe diff computation required special handling for file creation and deletion. Standard diff libraries assume file existence, but our updates frequently create new entity files. We handle non-existent files by treating them as empty strings in the diff computation, enabling proper diff generation for file creation scenarios.\nThe pipeline implements multi-level validation to catch errors early. JSON Schema validation catches structural issues, while Pydantic models enforce type constraints. Beyond these standard approaches, we implement domain-specific validation including duplicate name detection, which would cause conflicts in the filesystem-based storage model. During graph construction, we attempt name-based node resolution for invalid edge references to recover otherwise valid relationships.\nThe clarification generator addresses a critical training need: teaching the model to recognize knowledge boundaries. We generate samples by deliberately querying for non-existent entities and attributes, with responses that acknowledge the missing information rather than hallucinating plausible answers. The challenge lies in generating genuinely ambiguous queries. LLMs tend toward overly specific questions that telegraph their invalid nature.\nFilter augmentation implements privacy-aware information access. The technical challenge involves generating filters that meaningfully restrict information while maintaining query answerability. We solve this through a three-tier obfuscation model: complete blocking (no information revealed), partial revelation (some attributes visible), and null filters (all information accessible). This distribution ensures the model learns to respect filters without becoming overly restrictive.\nThis deterministic pipeline achieves reliable, high-quality data generation by carefully managing LLM interactions, implementing robust validation at each stage, and maintaining structural consistency through graph-based orchestration. The key insight is that constraining LLM creativity through limited context and strict validation produces more reliable outputs than attempting to generate complex structures in a single pass.\nIn the setup process for our training we have tried a plethora of open-source frameworks to get a minimal Qwen3-4B and GRPO setup on a 8xH100 node working. We tried verifiers, Nemo-RL (NVIDIA, 2025), SkyRL (Cao et al., 2025) and eventually settled on OpenRLHF. This was due to various reasons:\nOpenRLHF dealt with all the problems we had with other frameworks: It supported the Qwen3 series, the API was super simple (literally only needed to implement a single step() function when we first tried the framework) and the multi-GPU setup was a breeze to figure out compared to the rest. After we managed to get the desired Qwen3 + GRPO on a 8xH100 node working, we were able to properly start our experiments.\nIn our initial experiments, we had only retrieval (without filters) and update data in the training pipeline and dataset. The retrieval data format and the reward mechanism did not go through any changes throughout our experiments until we added the filter task to the training. The retrieval judge: given a user query, the ground truth and the agent reply, responds with a JSON in the following format:\nThe agent is instructed to follow the following guideline from our retrieval judge prompt:\nSee the full retrieval judge prompt in Appendix — Retrieval Judge System Prompt.\nThe update task we initially thought of, generated data for and then trained our models on was drastically different than the final one in the \"right formula\" we found for this study. The initial write_to_file() tool required a git-style diff as one of the arguments and consequently the judge was given all the Python blocks used by the agent in concatenated form and a target ground truth diff to look for in those Python blocks. We were seeing no improvements in update performance in the trained model and whatever gains we were making in the training were in retrieval. The initial update judge prompt can be seen in Appendix — First Update Judge System Prompt.\nWe started with Qwen3-4B, then switched to Qwen3-8B for about 20 runs where we played around with hyperparameters, training algorithms and reward shaping. First off, with conflicting results and opinions on whether to have format rewards or not, we opted to go for a format reward as we found the models we trained on had better reward curves than the ones trained without it. Our initial format reward consisted of rewarding the existence of <python> blocks and <think> blocks. It came back to bite us later, which we will discuss later in a dedicated reward shaping lessons section. We also added a specific potential reward for the first turn, in which the agent would get an extra reward if it checked the existence of or read the user.md file. This worked very well and enforced the \"check first act second\" behaviour we wanted the agent to have.\nAfter the first few experiments, we noticed that the training would degrade around 20 steps in, totally collapse (incoherent model responses and 0 reward) and then the training would crash with the \"Token id  is out of vocabulary\" error (AlexPiche, 2025). This issue is present in both Qwen2.5 and Qwen3 series of models when generating with vLLM. We've tried many solutions to bypass the issue and in the end implemented the workaround suggested in the README of the s1 repository (simplescaling, 2025). This allowed us to continue with our experimentation without crashes, but the model was still degrading and collapsing after some steps. We identified a positive correlation with the length of the <think> blocks in the agent's responses and the reward, so we implemented a minimum length for the contents inside and made that a hyperparameter also. That didn't work either, so we started playing around with many variables at the same time. Around this time, the score (validation set reward) and reward (training reward) curves started to diverge, and we switched to using Reinforce Leave One Out (RLOO) (Ahmadian et al., 2024) as our training algorithm.\nWe modified hyperparameters like the initial Kullback-Leibler (KL) coefficient, KL target, KL horizon, critic and actor learning rates, number of epochs and number of episodes. We also employed epsilon clipping in this stage, with none of our modifications to the training pipeline yielding any success. It seemed like at one point, our training was making the model worse every step. At the time, it also became apparent to the open source community that RL with Qwen3 models was tricky, as it was expected for the pipeline to remove all the <think> blocks in the agent's responses except the last one (@vivekkalyansk, 2025). This, followed by the fact that Qwen2.5 models are exceptionally performant in RLVR settings (Shao et al., 2025), led us to switch to Qwen2.5-Coder  (Hui et al., 2024) models.\nWith the switch to Qwen2.5-Coder, specifically the 7B model, we also changed our RL algorithm to Dr.GRPO. After our first run with this model we saw the score and reward curves align again, which was an improvement. What's more important though, is we discovered a particular behaviour from the model after looking at sample trajectories from each global step: the model was optimizing to get the maximum possible reward by adhering to the format for the maximum allowed turns, 8 turns at the time. The total amount of rewards over 8 steps the model could earn by just following the format was higher than getting the task right in the fifth turn for instance. Reward hacking is studied very well and often in the field of RL, and is considered to be almost inevitable in the first few iterations of an RL environment. To mitigate this, we first tried a crude minus reward if the agent reached maximum turns. The results were immediate, as before the model started the degrade & collapse phase after only 12 steps whereas with a max turns penalty, the scores kept going up way past 20 steps.\nWe also tried the Qwen2.5-Coder-14B model, and played around with many hyperparameters again, which yielded little to no improvement. The reward and score curves would plateau not much higher than the initial reward and score from the first step. This prompted us to reassess the data and the reward mechanisms for the implemented tasks of retrieval and update. We then decided to completely rewamp the update reward mechanism, where instead of <python> blocks and a target diff to be given to the update judge we give it a ground truth and the dump of the agent's memory folder before and after the trajectory. This took some time to implement with async training and async file operations on the same memories by different agents, but in terms of the data it needed no effort. Our data already contained all we needed which was a ground truth answer. The final update judge prompt can be seen in Appendix — Second Update Judge System Prompt.\nAfter we found out the models were abusing the format rewards by using the maximum amount of turns, we had to tabulate the possible rewards for each turn in different scenarios the agent could be in. After an iterative approach to improving, we implemented decreasing rewards for each turn until half of the allowed turns, then increasing negative rewards from half until the maximum turns. We had to meticulously tweak each format reward and success reward and tabulate all possible scenarios afterward, so we could be sure that successfully solving the task, as intended, with the least amount of tool call turns would be preferred by the agent over fishing for maximum format rewards. We also removed the reward for the <think> blocks entirely. Finally, we doubled the reward for successful task completion.\nWe recommend every researcher in the field, if they are not already doing so, to employ similar methods to be aware of all possible scenarios for rewards in all stages of a trajectory. This helps greatly in identifying possible avenues for reward hacking and in the effort of encouraging behaviours you want the agent to have. The recipe for a good LLM RL setup is three-pronged:\nMost of time all of these are improved iteratively during the experimentation process, which also uses compute. To save compute, time, and effort a preliminary investigation of the reward mechanism and all possible scenarios is essential in finding the right reward functions. In terms of tabulating per-turn cumulative rewards for different scenarios, we've found SOTA proprietary LLMs used inside Cursor to be very capable and reliable.\nAfter fixing and shaping the reward mechanisms, we opted for another switch in the algorithm-model pair. We did run with Qwen3-4B-Thinking-2507 and GSPO, where we saw a healthy increasing reward curve until around 15 steps. This looked like one of our best runs so far, especialy after evaluating a few selected checkpoints. With this, and us having some free VRAM with the current setup, we decided to increase the batch size and did another run. This run was a \"bitter lesson\" (Sutton, 2019) for us, as we saw the effects of the scale firsthand. The reward curve did not collapse, even after 120 steps at which point we manually stopped the run. We ran evaluations on selected checkpoints and we saw major improvements in retrieval and update tasks, with varying clarification performance across checkpoints.\nAs problems to solve, the retrieval and update tasks were practically solved. We then worked on generating the data for clarification, which we talked about in the data generation section, and the reward mechanism for the task. The clarification judge we came up with is very simple and foolproof: Given a user query and an agent reply, the clarification judge has to check if the agent reply asks for a clarification or not. The prompt can be seen in Appendix — Clarification Judge System Prompt.\nAfter getting clarification setup done, we only had to do two more runs to have a model whose evaluation performance we were satisfied with. In-between these two runs we:\nwhich all contributed positively to the training and evaluation performance of the model. Our final set of hyperparameters and flags can be found in the config.json and train_agent.sh files in our repository.\nGiven that the entirety of our training data was synthetically generated we decided to handcraft a benchmark of a few samples for each of our tasks. We thought of evaluation samples for both the \"personal assistant\" agent application domain we thought of for the training data but also real-life scenarios like customer service and technical support. For the purposes of our evaluation, we considered retrieval with filtering as its own task, filter . In this section we will discuss our benchmark, md-memory-bench , which in total is comprised of 56 samples. The breakdown of the samples per task is as follows:\nAll samples, except samples for the update task in the benchmark follow the same, simple JSON format. The format is as follows:\nThe answer and judge fields in effect indicate the task type to the judge, and because of this data format-based versatility we can use a single system prompt for all tasks except update . The format for the update task is slightly more complex:\nThe detailed procedure for both update and non-update tasks, and the designations of update agent and retrieval agent are explained in the following subsection. The full system prompt can be seen in Appendix — Evaluation Judge System Prompt.\nWhen a model is evaluated, it iterates over tasks and inside them, over samples sequentially. In the final JSON report generated; the overall score out of 1, per-task scores out of 1 and per-sample metadata like agent reply and judge reasoning are included.\nThe basic sequence of a model's evaluation on a sample, in all tasks except the update task, is as follows:\nThe evaluation procedure for the update task is as follows:\nThis double trajectory evaluation process not only measures the update capabilities of the evaluated model but also its retrieval capabilities, as both the update agent and the retrieval agent are initialised from the same model. The implementation of our evaluation can be found in the evaluation/ directory in our repository. This also applies to our evaluation data, which can be found at data/eval/ .\nIncluding our trained model, we evaluated a total of 11 models on md-memory-bench . The results are shown in Table 1, where models marked with a * were evaluated on a H100 SXM machine using vLLM and the rest were evaluated using OpenRouter's API.\nPerformance of tested models on md-memory-bench .\nThe Qwen3-235B-A22B-Thinking model leads the pack with an overall score of 0.79 and also in the retrieval tasks alone with a score of 0.91. It also shares the top spot for the filter task alone with Gemini 2.5 Pro, both with a perfect score of 1.00. The update task is done best by GLM-4.5 with a score of 0.82 and the clarification task by Claude Opus 4.1 with the same score. Kimi-K2 is the worst performer overall, but is also the only \"non-thinking\" model we evaluated.\nOur model, mem-agent , is the second best performer overall with a score of 0.75. The overall score, and the per-task scores all see a drastic improvement over the base Qwen3-4B-Thinking model. The overall score improves from 0.39 to 0.75, the retrieval score improves from 0.45 to 0.86, filter score improves from 0.75 to 0.92 and the clarification score improves from 0.27 to 0.36. The update task is where we see the biggest improvement, with the score improving from nothing to 0.73. The improvements overall and per task category suggest that our task formulation, data generation and training pipeline were designed and set up correctly.\nGiven the scores of the tested models on the benchmark, the performance of these models on other benchmarks and their known, general capabilities, we can outline a few main points of discussion:\nOur resulting model has 4 billion parameters, which in bf16 takes around 8GB of memory. For 8-bit precision it's 4GB and for 4-bit precision it's 2GB. This small memory footprint enables for this model to be used in multi-agent applications as well as standalone use. Driven by this, we built a MCP server around our model and released the code in our repository mem-agent-mcp . The repository is available here .\nThe main logic of the MCP server is simple: there's a single method available for use use_memory_agent(question: str) -> str that takes the user query and returns the reply of mem-agent after it finishes its trajectory. The only responsibility of the main model used by the user is to determine whether or not to delegate the task to the background memory agent or not. In our testing experience, even small models like Qwen3-4B-Thinking can use this MCP server with great reliability and SOTA proprietary models like Claude Opus 4.1 use it seamlessly. In terms of user experience, different precisions of our model from bf16 to 4-bit perform similarly, with the precision having the most effect on the clarification performance of the model. The 4-bit and 8-bit models tend to hallucinate facts sometimes instead of asking the user for clarification.\nIn the repository for the MCP server, there's also a script chat_cli.py that allows the user to interact directly with mem-agent through a command-line interface. This interface provides the tool interaction turns that the agent takes as well as the final response of the agent, which is useful when observing the model's behaviour and identifying pitfalls. Due to the configuration setup provided in the repository, both the MCP server usage and the standalone CLI usage interact with the same memory so that the user can choose what mode of interaction they prefer.\nIn this work, we share excerpts, designs and lessons learned throughout the iterative development process that led to mem-agent . Our training data is fuly synthetically generated following many works in post-training research that employ a synthetic data generation pipeline for training LLM agents, and also only generated around RLVR without any SFT warmup data following the findings in the Deepseek-R1 paper. The data generation pipeline, much like the training pipeline, was updated and improved over many training runs and their results. These results gave us many insights about the end-to-end pipeline of training an LLM agent with RLVR in a custom environment, and helped us find the \"perfect formula\" of training Qwen3-4B-Thinking-2507 with GSPO. The benchmark we crafted for this study, md-memory-bench , proved to be a reliable way to evaluate the performance of LLM agents on their performance using our scaffold for the tasks we determined. Finally, we released mem-agent-mcp , an MCP server for any LLM to be able to use mem-agent when needed for managing the memory of the user.\nThe potential limitations of our work include (but are not limited to):\nFor future research, we would like to explore the following directions:\nAgrawal, L. A., Tan, S., Soylu, D., Ziems, N., Khare, R., Opsahl-Ong, K., Singhvi, A., Shandilya, H., Ryan, M. J., Jiang, M., et al. (2025). Gepa: Reflective prompt evolution can outperform reinforcement learning. arXiv preprint arXiv:2507.19457 .\nAhmadian, A., Cremer, C., Gallé, M., Fadaee, M., Kreutzer, J., Pietquin, O., Üstün, A., and Hooker, S. (2024). Back to basics: Revisiting reinforce style optimization for learning from human feedback in llms. arXiv preprint arXiv:2402.14740 .\nAlexPiche (2025). Bug: Qwen/qwen2.5-1.5b-instruct generates out of vocabulary tokens. GitHub issue, vllm-project/vllm. https://github.com/vllm-project/vllm/issues/13175\nAn, C., Zhang, J., Zhong, M., Li, L., Gong, S., Luo, Y., Xu, J., and Kong, L. (2024). Why does the effective context length of llms fall short? arXiv preprint arXiv:2410.18745 .\nAnthropic (2024). Introducing the model context protocol. https://www.anthropic.com/news/model-context-protocol\nAnthropic (2025a). Claude code. https://www.anthropic.com/news/claude-3-7-sonnet\nAnthropic (2025b). System card: Claude opus 4 & claude sonnet 4. https://www.anthropic.com/claude-4-system-card\nBrockman, G., Cheung, V., Pettersson, L., Schneider, J., Schulman, J., Tang, J., and Zaremba, W. (2016). OpenAI gym. https://arxiv.org/abs/1606.01540\nBrown, W. (2025). Verifiers: Reinforcement learning with llms in verifiable environments. https://github.com/willccbb/verifiers\nCao, S., Hegde, S., Li, D., Griggs, T., Liu, S., Tang, E., Pan, J., Wang, X., Malik, A., Neubig, G., Hakhamaneshi, K., Liaw, R., Moritz, P., Zaharia, M., Gonzalez, J. E., and Stoica, I. (2025). SkyRL-v0: Train real-world long-horizon agents via reinforcement learning.\nChen, M., Shao, W., Xu, P., Wang, J., Gao, P., Zhang, K., and Luo, P. (2024). Efficientqat: Efficient quantization-aware training for large language models. arXiv preprint arXiv:2407.11062 .\nColmerauer, A. and Roussel, P. (1993). The birth of prolog. ACM SIGPLAN Notices , 28(3):37–52. Historical account of Prolog's creation.\ndetectivemittens (2016). Resources for learning recursion. Elixir forum discussion. https://elixirforum.com/t/resources-for-learning-recursion/210?utm_source=chatgpt.com\nGitHub (2024). Octoverse: Ai leads python to top language as the number of global developers surges. https://github.blog/news-insights/octoverse/octoverse-2024/\nGuo, D., Yang, D., Zhang, H., Song, J., Zhang, R., Xu, R., Zhu, Q., Ma, S., Wang, P., Bi, X., et al. (2025). Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948 .\nHu, J., Wu, X., Shen, W., Liu, J. K., Zhu, Z., Wang, W., Jiang, S., Wang, H., Chen, H., Chen, B., et al. (2024). Openrlhf: An easy-to-use, scalable and high-performance rlhf framework. arXiv preprint arXiv:2405.11143 .\nHui, B., Yang, J., Cui, Z., Yang, J., Liu, D., Zhang, L., Liu, T., Zhang, J., Yu, B., Lu, K., et al. (2024). Qwen2.5-coder technical report. arXiv preprint arXiv:2409.12186 .\nPrime Intellect (2025). Environments hub: A community hub to scale rl to open agi. Blog post. https://www.primeintellect.ai/blog/environments\nJaech, A., Kalai, A., Lerer, A., Richardson, A., El-Kishky, A., Low, A., Helyar, A., Madry, A., Beutel, A., Carney, A., et al. (2024). Openai o1 system card. arXiv preprint arXiv:2412.16720 .\nKe, Z., Shao, Y., Lin, H., Konishi, T., Kim, G., and Liu, B. (2023). Continual pre-training of language models. arXiv preprint arXiv:2302.03241 .\nKwa, T., West, B., Becker, J., Deng, A., Garcia, K., Hasin, M., Jawhar, S., Kinniment, M., Rush, N., Von Arx, S., et al. (2025). Measuring ai ability to complete long tasks. arXiv preprint arXiv:2503.14499 .\nLambert, N., Morrison, J., Pyatkin, V., Huang, S., Ivison, H., Brahman, F., Miranda, L. J. V., Liu, A., Dziri, N., Lyu, S., et al. (2024). Tulu 3: Pushing frontiers in open language model post-training. arXiv preprint arXiv:2411.15124 .\nLewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin, V., Goyal, N., Küttler, H., Lewis, M., Yih, W., Rocktäschel, T., et al. (2020). Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in neural information processing systems , 33:9459–9474.\nLi, T., Zhang, G., Do, Q. D., Yue, X., and Chen, W. (2024). Long-context llms struggle with long in-context learning. arXiv preprint arXiv:2404.02060 .\nLin, K. Q., Li, L., Gao, D., Yang, Z., Wu, S., Bai, Z., Lei, S. W., Wang, L., and Shou, M. Z. (2025). Showui: One vision-language-action model for gui visual agent. In Proceedings of the Computer Vision and Pattern Recognition Conference , pages 19498–19508.\nLiu, J., Wang, K., Chen, Y., Peng, X., Chen, Z., Zhang, L., and Lou, Y. (2024a). Large language model-based agents for software engineering: A survey. arXiv preprint arXiv:2409.02977 .\nLiu, Z., Chen, C., Li, W., Qi, P., Pang, T., Du, C., Lee, W. S., and Lin, M. (2025). Understanding r1-zero-like training: A critical perspective. arXiv preprint arXiv:2503.20783 .\nLiu, Z., Hoang, T., Zhang, J., Zhu, M., Lan, T., Tan, J., Yao, W., Liu, Z., Feng, Y., RN, R., et al. (2024b). Apigen: Automated pipeline for generating verifiable and diverse function-calling datasets. Advances in Neural Information Processing Systems , 37:54463–54482.\nMoritz, P., Nishihara, R., Wang, S., Tumanov, A., Liaw, R., Liang, E., Elibol, M., Yang, Z., Paul, W., Jordan, M. I., et al. (2018). Ray: A distributed framework for emerging AI applications. In 13th USENIX symposium on operating systems design and implementation (OSDI 18) , pages 561–577.\nMowshowitz, Z. (2025). OpenAI Model Differentiation 101. Don't Worry About the Vase (Substack). https://thezvi.substack.com/p/openai-model-differentiation-101 . Explains OpenAI model names; uses the colloquial \"big model smell\".\nMuennighoff, N., Yang, Z., Shi, W., Li, X. L., Fei-Fei, L., Hajishirzi, H., Zettlemoyer, L., Liang, P., Candès, E., and Hashimoto, T. (2025). s1: Simple test-time scaling. arXiv preprint arXiv:2501.19393 .\nMunroe, R. (2007). Nerd sniping. xkcd (Comic #356). https://xkcd.com/356/ . Introduces the slang \"nerd sniping\" / \"nerd-snipe\".\nNguyen, D., Lai, V. D., Yoon, S., Rossi, R. A., Zhao, H., Zhang, R., Mathur, P., Lipka, N., Wang, Y., Bui, T., et al. (2024). Dynasaur: Large language agents beyond predefined actions. arXiv preprint arXiv:2411.01747 .\nNovikov, A., Vũ, N., Eisenberger, M., Dupont, E., Huang, P., Wagner, A. Z., Shirobokov, S., Kozlovskii, B., Ruiz, F. J., Mehrabian, A., et al. (2025). Alphaevolve: A coding agent for scientific and algorithmic discovery. arXiv preprint arXiv:2506.13131 .\nNVIDIA (2025). Nemo rl: A scalable and efficient post-training library. GitHub repository. https://github.com/NVIDIA-NeMo/RL\nOpenAI (2025a). GPT-5 system card. https://openai.com/index/gpt-5-system-card/\nOpenAI (2025b). Introducing deep research. https://openai.com/index/introducing-deep-research/\nOuyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., et al. (2022). Training language models to follow instructions with human feedback. Advances in neural information processing systems , 35:27730–27744.\nPacker, C., Fang, V., Patil, S. G., Lin, K., Wooders, S., and Gonzalez, J. E. (2023). Memgpt: Towards llms as operating systems. arXiv preprint arXiv:2310.08560 .\nPark, J. S., O'Brien, J., Cai, C. J., Morris, M. R., Liang, P., and Bernstein, M. S. (2023). Generative agents: Interactive simulacra of human behavior. In Proceedings of the 36th annual acm symposium on user interface software and technology , pages 1–22.\nRuis, L., Mozes, M., Bae, J., Kamalakara, S. R., Talupuru, D., Locatelli, A., Kirk, R., Rocktäschel, T., Grefenstette, E., and Bartolo, M. (2024). Procedural knowledge in pretraining drives reasoning in large language models. arXiv preprint arXiv:2411.12580 .\nSager, P. J., Meyer, B., Yan, P., von Wartburg-Kottler, R., Etaiwi, L., Enayati, A., Nobel, G., Abdulkadir, A., Grewe, B. F., and Stadelmann, T. (2025). A comprehensive survey of agents for computer use: Foundations, challenges, and future directions. arXiv preprint arXiv:2501.16150 .\nSchulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O. (2017). Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347 .\nShao, R., Li, S. S., Xin, R., Geng, S., Wang, Y., Oh, S., Du, S. S., Lambert, N., Min, S., Krishna, R., et al. (2025). Spurious rewards: Rethinking training signals in rlvr. arXiv preprint arXiv:2506.10947 .\nShao, Z., Wang, P., Zhu, Q., Xu, R., Song, J., Bi, X., Zhang, H., Zhang, M., Li, Y., Wu, Y., et al. (2024). Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300 .\nSheng, G., Zhang, C., Ye, Z., Wu, X., Zhang, W., Zhang, R., Peng, Y., Lin, H., and Wu, C. (2025). Hybridflow: A flexible and efficient rlhf framework. In Proceedings of the Twentieth European Conference on Computer Systems , pages 1279–1297.\nShinn, N., Cassano, F., Gopinath, A., Narasimhan, K., and Yao, S. (2023). Reflexion: Language agents with verbal reinforcement learning. Advances in Neural Information Processing Systems , 36:8634–8652.\nsimplescaling (2025). s1: Simple test-time scaling, known issues section in readm. GitHub issue, simplescaling/s1. https://github.com/simplescaling/s1?tab=readme-ov-file#known-issues\nSingh, A., Ehtesham, A., Kumar, S., and Khoei, T. T. (2025). Agentic retrieval-augmented generation: A survey on agentic rag. arXiv preprint arXiv:2501.09136 .\nSutton, R. (2019). The bitter lesson. Online blog. http://www.incompleteideas.net/IncIdeas/BitterLesson.html\nTeam, G., Kamath, A., Ferret, J., Pathak, S., Vieillard, N., Merhej, R., Perrin, S., Matejovicova, T., Ramé, A., Rivière, M., et al. (2025a). Gemma 3 technical report. arXiv preprint arXiv:2503.19786 .\nTeam, K., Bai, Y., Bao, Y., Chen, G., Chen, J., Chen, N., Chen, R., Chen, Y., Chen, Y., Chen, Y., et al. (2025b). Kimi k2: Open agentic intelligence. arXiv preprint arXiv:2507.20534 .\nTekparmak, A. and andthattoo (2025a). Dria pythonic agent benchmark (dpab). Hugging Face Blog. https://huggingface.co/blog/andthattoo/dpab-a . Published January 15, 2025.\nTekparmak, A. and andthattoo (2025b). Python is all you need? introducing dria-agent-a. Hugging Face Blog. https://huggingface.co/blog/andthattoo/dria-agent-a . Published January 10, 2025.\n@vivekkalyansk (2025). Reply on X.com. https://x.com/vivekkalyansk/status/1948296877180670039\nvon Werra, L., Belkada, Y., Tunstall, L., Beeching, E., Thrush, T., Lambert, N., Huang, S., Rasul, K., and Gallouédec, Q. (2020). TRL: Transformer reinforcement learning. https://github.com/huggingface/trl\nWang, G., Xie, Y., Jiang, Y., Mandlekar, A., Xiao, C., Zhu, Y., Fan, L., and Anandkumar, A. (2023). Voyager: An open-ended embodied agent with large language models. arXiv preprint arXiv:2305.16291 .\nXia, Z., Luo, K., Qian, H., and Liu, Z. (2025). Open data synthesis for deep research. arXiv preprint arXiv:2509.00375 .\nYang, A., Li, A., Yang, B., Zhang, B., Hui, B., Zheng, B., Yu, B., Gao, C., Huang, C., Lv, C., et al. (2025). Qwen3 technical report. arXiv preprint arXiv:2505.09388 .\nYao, S., Zhao, J., Yu, D., Du, N., Shafran, I., Narasimhan, K., and Cao, Y. (2023). React: Synergizing reasoning and acting in language models. In International Conference on Learning Representations (ICLR) .\nZeng, A., Lv, X., Zheng, Q., Hou, Z., Chen, B., Xie, C., Wang, C., Yin, D., Zeng, H., Zhang, J., et al. (2025). Glm-4.5: Agentic, reasoning, and coding (arc) foundation models. arXiv preprint arXiv:2508.06471 .\nZhang, Z., Bo, X., Ma, C., Li, R., Chen, X., Dai, Q., Zhu, J., Dong, Z., and Wen, J. (2024). A survey on the memory mechanism of large language model based agents. URL https://arxiv.org/abs/2404.13501 .\nZhao, A., Wu, Y., Yue, Y., Wu, T., Xu, Q., Lin, M., Wang, S., Wu, Q., Zheng, Z., and Huang, G. (2025). Absolute zero: Reinforced self-play reasoning with zero data. arXiv preprint arXiv:2505.03335 .\nZheng, C., Liu, S., Li, M., Chen, X., Yu, B., Gao, C., Dang, K., Liu, Y., Men, R., Yang, A., et al. (2025). Group sequence policy optimization. arXiv preprint arXiv:2507.18071 .\nZhou, C., Liu, P., Xu, P., Iyer, S., Sun, J., Mao, Y., Ma, X., Efrat, A., Yu, P., Yu, L., et al. (2023). Lima: Less is more for alignment. Advances in Neural Information Processing Systems , 36:55006–55021.\nZheng, L., Chiang, W., Sheng, Y., Zhuang, S., Wu, Z., Zhuang, Y., Lin, Z., Li, Z., Li, D., Xing, E. P., Zhang, H., Gonzalez, J. E., and Stoica, I. (2023). Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena. arXiv preprint arXiv:2306.05685 .\nKwon, W., Li, Z., Zhuang, S., Sheng, Y., Zheng, L., Yu, C. H., Gonzalez, J., Zhang, H., and Stoica, I. (2023). Efficient memory management for large language model serving with pagedattention. In Proceedings of the 29th symposium on operating systems principles , pages 611–626."}
{"url": "https://huggingface.co/blog/treble-technologies/treble10-announcement-treble-x-hugging-face", "title": "High-Quality Datasets for Far-Field ASR (Treble Technologies x Hugging Face)", "date": "", "content": "We are thrilled to announce the start of a collaboration between Treble Technologies and Hugging Face. In connection with the collaboration, we are publishing the Treble10 dataset , which contains high fidelity room-acoustic simulations from 10 different furnished rooms:\n🗣️🎤 Code examples can be found at the Dataset cards 🗣️🎤\nAccurate room-acoustic data is the foundation for far-field speech recognition, dereverberation, speech enhancement, or source separation. Yet most existing datasets are limited either in scale or realism. Measured corpora, such as the BUT ReverbDB or the CHIME3 challenge dataset capture acoustic conditions of the measured scenes reliably, but only coarsely cover selected areas of the rooms. For example, BUT ReverbDB contains around 1400 measured room impulse responses (RIRs) from 9 rooms, recorded with a sound source and a few dozen microphones inside spatially constrained areas. Expanding or replicating these datasets is extremely time-consuming and expensive. The CHiME datasets offer much larger amounts of noisy, reverberant speech but usually do not provide the underlying RIRs. The lack of matching RIR data limits the usability for tasks like dereverberation, where both original (aka “dry”) and reverberant versions of the same utterance are required. Additionally, systematic data generation and controlled ablations are not possible in such cases.\nThe Treble10 dataset bridges this gap by combining physical accuracy with the scalability of advanced simulation. Using the Treble SDK’s hybrid wave-based and geometrical-acoustics engine, we model sound propagation in 10 realistic, fully furnished rooms. In contrast to other simulation tools, which typically rely on simplified geometrical acoustics modeling, our hybrid approach models physical effects such as scattering, diffraction, interference, and the resulting modal behavior. Each room of the Treble10 dataset is densely sampled across receiver grids at multiple heights, resulting in over 3000 distinct transfer paths / RIRs per subset. The dataset includes 6 subsets: mono, 8th-order Ambisonics, and 6-channel device RIRs, along with corresponding reverberant speech signals from the LibriSpeech test set ( test.clean & test.other ). All responses are broadband (32 kHz sampling rate), accurately modeling both low-frequency wave behavior and high-frequency reflections.\nThe Treble10 dataset contains high fidelity room-acoustic simulations from 10 different furnished rooms (see the table below for more details).\nThe dataset contains six subsets:\nFig1: Sketch of the multi channel device included in the dataset. The device consists of 6 microphones evenly spaced with a radius of 3 cm. The coordinates of the microphones are present in the metadata for the 6ch split and also in the dataset card.\nAll RIRs (mono/HOA/device) were simulated with the Treble SDK, and more details on the tool can be found in the dedicated section below. We use a hybrid simulation paradigm that combines a numerical wave-based solver (discontinuous Galerkin method, DGM) at low to midrange frequencies with geometrical acoustics (GA) simulations at high frequencies. For the Treble10 dataset, the transition frequency between the wave-based and the GA simulation is set at 5 kHz. The resulting hybrid RIRs are broadband signals with a 32 kHz sampling rate, thus covering the entire frequency range of the signal and containing audio content up to 16 kHz.\nA small subset of simulations from the same rooms has previously been released as part of the Generative Data Augmentation (GenDA) challenge at ICASSP 2025 . The Treble10 dataset differs from the GenDA dataset in three fundamental aspects:\nThe Treble10 dataset contains broadband RIRs from a hybrid simulation paradigm (wave-based below 5 kHz, GA above 5 kHz), covering the entire frequency range of a 32 kHz signal. In contrast to the GenDA subset, which only contained the wave-based portion, the Treble10 dataset therefore more than doubles the usable frequency range.\nThe Treble10 dataset consists of 6 subsets in total. While three of those subsets contain RIRs (mono, 8th-order Ambisonics, 6-channel device), the other three contain pre-convolved scenes in identical channel formats. The GenDA subset was limited to mono and 8th-order Ambisonics RIRs, and no pre-convolved scenes were provided.\nWith Treble10, we publish the entire dataset, containing approximately 3100 source-receiver configurations. The GenDA subset only contained a small fraction of approximately 60 randomly selected source-receiver configurations.\nWhich use cases benefit from the Treble10 dataset? Let us explain this briefly with an example scenario. Consider the difference between far-field and near-field automatic speech recognition (ASR).\nIn near-field ASR, a user speaks directly into a smartphone or headset, and the captured speech signal is relatively clean. The microphone is close to the mouth, so the direct sound dominates while reverberation and background noise are comparatively weak. In these conditions, ASR models may perform well even with limited room-acoustic diversity in the training data. However, in far-field ASR, as in smart speakers or conference-room devices, the microphone may be located several meters from the talker. The speech signal reaching the microphone is a complex mixture of direct sound, reverberation, and background noise, making the recognition task substantially more challenging.\nThe difference between near-field and far-field conditions is not just a matter of distance, but also a matter of physics. In far-field setups, sound interacts heavily with the room: it reflects off walls, diffracts around furniture, and decays over time. RIRs comprise all of these effects and encode how sound propagates from the source to the receiver. By convolving a dry audio signal with an RIR, we can simulate reverberant speech for a specific room configuration, replicating the far-field scenario.\nFig 2: The transformation of a clean audio signal into a reverberant one via convolution with a simulated Room Impulse Response in Treble is shown here. The initial clean speech (left) is convolved with the RIR (center), a sonic fingerprint of a room. This operation yields augmented clean speech (right) that incorporates the acoustic characteristics of that particular room.\nFor far-field ASR systems to be robust, they must be trained on data that accurately represents the complex far-field behavior. Similarly, the performance of far-field ASR systems can only be reliably determined when evaluating them on data that is accurate enough to model sound propagation in complex environments.\nThis example illustrates the need for accurate RIR datasets in far-field ASR. Other speech-related tasks, such as speech enhancement, dereverberation and source separation, in real-world conditions benefit analogously from high-quality training data.\nWith this blog post, we introduce the Treble10 dataset, a curated and quality-screened collection of highly accurate RIRs designed to support research and development of audio and speech algorithms. The dataset is accessible via the Hugging Face Hub, enabling straightforward integration into existing pipelines. This accessibility allows researchers and practitioners to directly evaluate the data’s fidelity and applicability within their own workflows. Beyond providing an initial hands-on opportunity to explore high accuracy simulated room acoustics, the remainder of this article outlines how similar datasets can be systematically generated, and what challenges arise during that process.\nThe figure above shows that room-acoustic data exhibits many degrees of freedom. To make algorithms perform well in a broad range of realistic conditions, we want the underlying training data to cover different source-receiver configurations, different source and receiver directivities or devices, different room volumes, different room shapes, different wall absorption properties, and different geometric details like scattering objects.\nHowever, when developing audio algorithms or training machine learning models for acoustic tasks, a central question inevitably arises: Where can we obtain sufficiently large and high-quality room-acoustic datasets that cover all outlined dataset dimensions? Room-acoustic measurements capture the physical sound pressure within a space at a specific moment in time, and therefore, faithfully represent the actual acoustic conditions of that environment at the measurement time. Unfortunately, conducting such measurements is both labor-intensive and time-consuming.\nEven with recent advances in automated room-acoustic measurements, large-scale, measurement-based dataset acquisition that sufficiently covers the abovementioned dataset dimensions remains impractical. Such large measurement campaigns would not only require tremendous amounts of manual labor, but also quickly reach practical limits regarding scalability. For example, a research team may have physical access to ten or even a hundred different rooms, but scaling such a campaign to the order of ten or even a hundred thousand rooms is infeasible. Setting up device-specific multi-channel datasets further multiplies the measurement effort, as new measurements have to be conducted for every device configuration.\nRoom-acoustic simulations are a scalable tool for generating large amounts of synthetic data under controlled, reproducible, and diverse acoustic conditions. This makes them a powerful alternative to physical measurements for training and evaluating audio machine learning models. A wide range of simulation methods exist, from geometrical acoustics techniques like the image-source method and ray tracing, to more advanced, wave-based approaches that capture the underlying physics of sound propagation in greater detail.\nReproducing real-world acoustics with simulations requires accurate modeling of key wave phenomena that shape how sound interacts with the environment, including:\nHowever, many existing data augmentation pipelines still rely on simplified simulation techniques, such as basic image-source models and ray tracing . Although these approaches can be effective for simple environments, they often fail to capture the full physical complexity of real-world acoustics , particularly in spaces with irregular geometries, frequency-dependent materials, or scattering objects.\nAt Treble Technologies, we believe that advancing audio machine learning requires revisiting the underlying physics of sound propagation. A physically accurate simulation paradigm is crucial, especially when modelling the complex behavior of sound in realistic rooms and with multi-channel devices. Tools like the Treble SDK bridge the gap between high scalability and high accuracy when working with room-acoustic simulations.\nThe Treble SDK offers a flexible, Python-based framework powered by an advanced acoustic simulation engine. It enables engineers and researchers to:"}
{"url": "https://huggingface.co/blog/not-lain/vlms", "title": "Visualizing How VLMs Work", "date": "2025-10-13", "content": "Visual Language Models (VLMs) are autoregressive AI models that process both text and images as input. In this post, we’ll take a closer look at how VLMs like Idefics3 and SmolVLM operate under the hood exploring how they merge visual and textual information to generate coherent outputs.\nAs model reference in this blogpost we will use HuggingFaceTB/SmolVLM-256M-Instruct .\nThe processor prepares both text and image data into a unified format suitable for the model. For images, it performs a sequence of transformations before converting them into token-like representations the model can understand.\nThe image pipeline can be visualized as follows:\nA key step in this process is image splitting , as seen in this code snippet .\nEach image is divided into smaller patches (or “splits”), which are individually encoded.\nWhen text accompanies images, each image is first represented by a <image> token in the text sequence. Depending on the number of splits, each image is then expanded into: 64 × number_of_splits 64 \\times \\text{number\\_of\\_splits} 64 × number_of_splits\nThe constant 64 comes from the relation: 512 2 / 16 2 4 2 = 64 \\frac{512^2 / 16^2}{4^2} = 64 4 2 51 2 2 /1 6 2 ​ = 64\nWe’ll get back to this formula later, but for now, think of it as:\nEach image split is represented by 64 tokens.\nWhen an image is included, the text must also reflect it using the <image> placeholder. The processor follows these steps:\nTo fully grasp how image tokens are expanded, check the encoding example below and review the following references:\nBefore being fed into the model, both the image and text data are prepared and aligned.\nAs in most autoregressive models, the output sequence is shifted to the right to teach the model how to predict the next token based on all previous ones.\nHowever, since image representations cannot be directly predicted, they are masked using the <pad> token.\nThis prevents the model from computing a loss over non-text (visual) tokens.\nFrom a high-level view, SmolVLM consists of five main components (illustrated on the right).\nThe text processing starts with the prompt , which is tokenized and passed through an embedding layer . This layer transforms discrete tokens into high-dimensional vector representations, producing a tensor of shape: [ sequence_length , 576 ] [\\text{sequence\\_length}, 576] [ sequence_length , 576 ]\nThis tensor now encodes the vector representation of the input text, and it serves as the foundation for all subsequent computations, note that this tensor already has 13 x 64 placeholder tokens for the upcoming processed image (where 13 is the number of image splits)\nPatch Embedding\nFor the visual branch, the input tensor has the shape [splits, num_channels, height, width] .\nFor example, for an image that can be split into 13 splits of RGB images is represented as [13, 3, 512, 512] .\nThe patch embedding layer transforms this tensor into a sequence of visual tokens , making it compatible with the Transformer architecture.\nIt does this by dividing the image into small, non-overlapping patches and projecting each one into a high-dimensional vector space, the core principle behind this is to adapt the RGB channel as an embedding dim and expand that 3 -> 768\nThis operation is implemented as a 2D convolution , where kernel_size and stride are both set to the patch size. This ensures each convolutional window processes one patch without overlap:\nWhen applied to [13, 3, 512, 512] , the transformation proceeds as follows:\nThe resulting tensor [13, 1024, 768] represents each image as a sequence of 1024 embedded patches , ready to be processed alongside text embeddings within the Transformer.\nPositional Encoder\nThe positional encoder injects information about patch order and spatial layout just as positional encodings do for words in a text model.\nSince transformers have no inherent sense of order, these encodings allow the model to understand where each patch is located within the image.\nEncoder\nThe encoder operates in a straightforward manner:\nThis attention implementation is close to how BERT functions, it is essential note that the output of the vision_model is [13,1024,768] given the example input we are using.\nThe connector serves as the bridge between the vision encoder and the language model , ensuring both modalities share a compatible embedding space.\nIts two main functions are:\nPixel Shuffle\nThe pixel shuffle operation compresses the spatial dimension of the visual features while preserving important spatial relationships.\nIn practical terms, it reduces the number of image tokens from 1024 → 64 , drastically lowering the sequence length while maintaining representational richness.\nThis transformation unfolds as follows:\nby doing so we get a final dimension equal to [ splits , H’ / scale × W’ / scale , C × scale 2 ] [\\text{splits}, \\text{H'} / \\text{scale} \\times \\text{W'} / \\text{scale}, \\text{C} \\times \\text{scale}^2 ] [ splits , H’ / scale × W’ / scale , C × scale 2 ] resulting in an output dimension equal to\n[ 13 , 1024 / 4 2 , 768 ∗ 4 2 ] → [ 13 , 64 , 12288 ] [13, 1024/4^2, 768 * 4^2] \\rightarrow [13, 64, 12288] [ 13 , 1024/ 4 2 , 768 ∗ 4 2 ] → [ 13 , 64 , 12288 ]\nBy progressively shuffling and regrouping pixels, this operation compresses both height and width dimensions while maintaining the spatial order of visual information in each direction.\nThe result is a more compact tensor that preserves essential image features.\nModality Projection\nThe connector also applies a modality_projection layer which is a linear transformation to match the embedding_dimension of the text tokens, by doing so we get [ 13 , 64 , 12288 ] → [ 13 , 64 , 576 ] [13, 64, 12288] \\rightarrow [13, 64, 576] [ 13 , 64 , 12288 ] → [ 13 , 64 , 576 ]\nThe input merger is a non-learnable layer, and is in fact a function responsible for integrating visual and textual embeddings into a single input sequence.\nIt replaces each <image> placeholder token in the text sequence with the corresponding visual embeddings produced by the connector.\nIn practice, this function scans the token IDs for <image> tokens and substitutes them with their preprocessed image representations. This step effectively merges both modalities into a single, continuous tensor that can be fed directly into the decoder.\nThe decoder functions similarly to that of a traditional autoregressive language model. It is composed of stacked Masked Multi-Head Attention (MHA) layers followed by a Language Modeling (LM) head .\nThis allow the VLM to generate coherent multimodal outputs seamlessly grounding textual predictions in visual context.\nThe key thing to keep on mind here, since we have no way of figuring out the output for the image tokens we use a pad token in the target to skip calculating loss over it.\nIn this post, we explored the inner workings of Visual Language Models (VLMs) like SmolVLM , breaking down how they process and integrate multimodal data from raw pixels and text to coherent, grounded outputs.\nHere’s a quick recap of each stage:\nAt their core, VLMs don’t just see and read, they reason across modalities .\nThis architecture allows them to handle multiple images, text-only prompts, or even image-only inputs, making SmolVLM a flexible and powerful foundation for a wide range of multimodal applications."}
{"url": "https://huggingface.co/blog/nvidia/nemotron-personas-india", "title": "Nemotron-Personas-India: Synthesized Data for Sovereign AI", "date": "", "content": "India represents one of the world's largest AI opportunities — with over 700 million internet users, a multitude of languages, and a rapidly growing developer ecosystem. Yet, most open datasets reflect Western norms and English-only contexts, creating a data gap that limits AI adoption in India's multilingual, multi-script environment.\nToday, we're releasing Nemotron-Personas-India , the first open synthetic dataset of Indic personas aligned to India's real-world demographic, geographic, and cultural distributions. Licensed under CC BY 4.0 , this dataset offers a privacy-preserving, regulation-ready foundation for scaling AI systems that reflect Indian society—without relying on sensitive personal data.\nBuilt with NeMo Data Designer , NVIDIA's enterprise-grade synthetic data generation microservice, Nemotron-Personas-India extends our global collection of Sovereign AI datasets. It builds on the success of our US and Japan persona datasets and includes new features designed specifically for India's culturally rich landscape.\nThis dataset integrates seamlessly with Nemotron models and other open-source LLMs, making it easy to fine-tune AI systems for Indian use cases—from multilingual chatbots to culturally-grounded specialized copilots.\nThis release complements our earlier suite of Hindi evaluation datasets — including ChatRAG-Hi , IFEval-Hi , MT-Bench-Hi , GSM8K-Hi , and BFCL-Hi — supporting a complete pipeline from synthetic data generation to rigorous model evaluation for Indian AI systems.\nProduced using NeMo Data Designer , NVIDIA's microservice for synthetic data generation. This compound AI system enables generation with complex Jinja templating, Pydantic validation, structured outputs, automated retries, and supports multiple generation backends – the necessary tooling to scale a synthetic dataset of this size. We also leveraged the following models:\nThis dataset was aligned to India’s official demographic distributions from the 2011 Census and expanded to include attributes essential for trustworthy AI training:\nNo real names. No re-identification risk.\nAll personas are fully synthetic. While grounded in real-world distributions from the 2011 Census and Parsed Indian Electoral Rolls data, no data is tied to any living or deceased individual. This ensures developers can safely train AI systems without privacy risks or regulatory barriers.\nBuilt for India, Ready for the World\nNemotron‑Personas‑India is designed for developers building Sovereign AI systems for the Indian market , as well as global teams looking to adapt models to India’s unique linguistic, cultural, and social context.\nMost open datasets today reflect English-speaking, Western norms—limiting AI performance in India’s multilingual, multi-script, and demographically complex environments.\nWith Nemotron‑Personas‑India, teams can:\nIndia's 1.4 billion people speak hundreds of languages and live across vast cultural, economic, and geographic divides. India's National AI Portal estimates over 7,000 AI startups and research institutions are working to build locally relevant AI systems, and the Digital India initiative and government programs like IndiaAI are accelerating adoption.\nBut progress is constrained by a fundamental gap: high-quality, culturally grounded training data that reflects India's demographic reality. Without representative datasets, AI systems struggle with code-switching between English and Hindi, fail to understand regional occupational categories, and miss cultural context essential for trust and adoption.\nThe dataset improves diversity of synthetically-generated data, mitigates biases, and prevents model collapse (degradation caused by uncurated training on another model's outputs) by reflecting India's real geographic and demographic distributions.\nNemotron-Personas-India supports Indian model builders in developing Sovereign AI systems that incorporate important region-specific demographics and cultural context.\nWant to build AI systems that understand India's culture, languages, and people?\nTo start experimenting today:\nWhether you're an Indian model builder developing Sovereign AI or a global developer seeking better regional adoption, Nemotron-Personas-India provides the authentic, privacy-safe foundation your applications need.\nDownload it. Fine-tune it. Build AI that understands India. If you’re ready to go deeper, an extended version of Nemotron-Personas-India (which includes e.g., first/last names, religion, and synthetic addresses) is available in NeMo Data Designer ."}
