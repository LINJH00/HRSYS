{"url": "https://huggingface.co/papers/2510.14528", "title": "PaddleOCR-VL: Boosting Multilingual Document Parsing via a 0.9B\n  Ultra-Compact Vision-Language Model", "date": "2025-10-17", "content": "PaddleOCR-VL, a vision-language model combining NaViT-style visual encoder and ERNIE-4.5 language model, achieves state-of-the-art performance in document parsing with minimal resource consumption.\nIn this report, we propose PaddleOCR-VL, a SOTA and resource-efficient model\ntailored for document parsing. Its core component is PaddleOCR-VL-0.9B, a\ncompact yet powerful vision-language model (VLM) that integrates a NaViT-style dynamic resolution visual encoder with the ERNIE-4.5 -0.3B language model to\nenable accurate element recognition . This innovative model efficiently supports\n109 languages and excels in recognizing complex elements (e.g., text, tables,\nformulas, and charts), while maintaining minimal resource consumption. Through\ncomprehensive evaluations on widely used public benchmarks and in-house\nbenchmarks, PaddleOCR-VL achieves SOTA performance in both page-level document\nparsing and element-level recognition . It significantly outperforms existing\nsolutions, exhibits strong competitiveness against top-tier VLMs, and delivers\nfast inference speeds . These strengths make it highly suitable for practical\ndeployment in real-world scenarios."}
{"url": "https://huggingface.co/papers/2510.12323", "title": "RAG-Anything: All-in-One RAG Framework", "date": "2025-10-15", "content": "RAG-Anything is a unified framework that enhances multimodal knowledge retrieval by integrating cross-modal relationships and semantic matching, outperforming existing methods on complex benchmarks.\nRetrieval-Augmented Generation ( RAG ) has emerged as a fundamental paradigm\nfor expanding Large Language Models beyond their static training limitations.\nHowever, a critical misalignment exists between current RAG capabilities and\nreal-world information environments. Modern knowledge repositories are\ninherently multimodal , containing rich combinations of textual content , visual\nelements, structured tables , and mathematical expressions . Yet existing RAG frameworks are limited to textual content , creating fundamental gaps when\nprocessing multimodal documents. We present RAG -Anything, a unified framework\nthat enables comprehensive knowledge retrieval across all modalities. Our\napproach reconceptualizes multimodal content as interconnected knowledge\nentities rather than isolated data types. The framework introduces dual-graph\nconstruction to capture both cross-modal relationships and textual semantics\nwithin a unified representation. We develop cross-modal hybrid retrieval that\ncombines structural knowledge navigation with semantic matching . This enables\neffective reasoning over heterogeneous content where relevant evidence spans\nmultiple modalities. RAG -Anything demonstrates superior performance on\nchallenging multimodal benchmarks , achieving significant improvements over\nstate-of-the-art methods. Performance gains become particularly pronounced on long documents where traditional approaches fail. Our framework establishes a\nnew paradigm for multimodal knowledge access, eliminating the architectural\nf rag mentation that constrains current systems. Our framework is open-sourced\nat: https://github.com/HKUDS/ RAG -Anything."}
{"url": "https://huggingface.co/papers/2510.14975", "title": "WithAnyone: Towards Controllable and ID Consistent Image Generation", "date": "2025-10-17", "content": "A diffusion-based model addresses copy-paste artifacts in text-to-image generation by using a large-scale paired dataset and a contrastive identity loss to balance identity fidelity and variation.\nIdentity-consistent generation has become an important focus in text-to-image research, with recent models achieving notable success in producing images\naligned with a reference identity. Yet, the scarcity of large-scale paired\ndatasets containing multiple images of the same individual forces most\napproaches to adopt reconstruction-based training . This reliance often leads to\na failure mode we term copy-paste , where the model directly replicates the\nreference face rather than preserving identity across natural variation s in\npose, expression, or lighting. Such over-similarity undermines controllability\nand limits the expressive power of generation. To address these limitations, we\n(1) construct a large-scale paired dataset MultiID-2M , tailored for\nmulti-person scenarios, providing diverse references for each identity; (2)\nintroduce a benchmark that quantifies both copy-paste artifacts and the\ntrade-off between identity fidelity and variation ; and (3) propose a novel\ntraining paradigm with a contrastive identity loss that leverages paired data\nto balance fidelity with diversity. These contributions culminate in\nWithAnyone, a diffusion-based model that effectively mitigates copy-paste while\npreserving high identity similarity. Extensive qualitative and quantitative\nexperiments demonstrate that WithAnyone significantly reduces copy-paste artifacts, improves controllability over pose and expression, and maintains\nstrong perceptual quality . User studies further validate that our method\nachieves high identity fidelity while enabling expressive controllable\ngeneration."}
{"url": "https://huggingface.co/papers/2510.12747", "title": "FlashVSR: Towards Real-Time Diffusion-Based Streaming Video\n  Super-Resolution", "date": "2025-10-15", "content": "Diffusion models have recently advanced video restoration, but applying them\nto real-world video super-resolution (VSR) remains challenging due to high\nlatency, prohibitive computation, and poor generalization to ultra-high\nresolutions. Our goal in this work is to make diffusion-based VSR practical by\nachieving efficiency, scalability, and real-time performance. To this end, we\npropose FlashVSR, the first diffusion-based one-step streaming framework\ntowards real-time VSR. FlashVSR runs at approximately 17 FPS for 768x1408\nvideos on a single A100 GPU by combining three complementary innovations: (i) a\ntrain-friendly three-stage distillation pipeline that enables streaming\nsuper-resolution, (ii) locality-constrained sparse attention that cuts\nredundant computation while bridging the train-test resolution gap, and (iii) a\ntiny conditional decoder that accelerates reconstruction without sacrificing\nquality. To support large-scale training, we also construct VSR-120K, a new\ndataset with 120k videos and 180k images. Extensive experiments show that\nFlashVSR scales reliably to ultra-high resolutions and achieves\nstate-of-the-art performance with up to 12x speedup over prior one-step\ndiffusion VSR models. We will release the code, pretrained models, and dataset\nto foster future research in efficient diffusion-based VSR."}
{"url": "https://huggingface.co/papers/2510.13678", "title": "FlashWorld: High-quality 3D Scene Generation within Seconds", "date": "2025-10-16", "content": "FlashWorld generates 3D scenes from single images or text prompts quickly and with high quality by combining MV-oriented and 3D-oriented generation methods.\nWe propose FlashWorld, a generative model that produces 3D scenes from a single image or text prompt in seconds, 10~100times faster than previous\nworks while possessing superior rendering quality. Our approach shifts from the\nconventional multi-view-oriented (MV-oriented) paradigm, which generates\nmulti-view images for subsequent 3D reconstruction, to a 3D-oriented approach where the model directly produces 3D Gaussian representations during multi-view\ngeneration. While ensuring 3D consistency, 3D-oriented method typically suffers\npoor visual quality. FlashWorld includes a dual-mode pre-training phase\nfollowed by a cross-mode post-training phase, effectively integrating the\nstrengths of both paradigms. Specifically, leveraging the prior from a video\ndiffusion model, we first pre-train a dual-mode multi-view diffusion model ,\nwhich jointly supports MV-oriented and 3D-oriented generation modes. To bridge\nthe quality gap in 3D-oriented generation, we further propose a cross-mode\npost-training distillation by matching distribution from consistent 3D-oriented\nmode to high-quality MV-oriented mode. This not only enhances visual quality\nwhile maintaining 3D consistency, but also reduces the required denoising steps for inference. Also, we propose a strategy to leverage massive single-view\nimages and text prompt s during this process to enhance the model's\ngeneralization to out-of-distribution inputs . Extensive experiments demonstrate\nthe superiority and efficiency of our method."}
{"url": "https://huggingface.co/papers/2510.11690", "title": "Diffusion Transformers with Representation Autoencoders", "date": "2025-10-14", "content": "Replacing VAEs with pretrained representation encoders in Diffusion Transformers enhances generative quality and convergence speed without auxiliary losses.\nLatent generative modeling , where a pretrained autoencoder maps pixels into a\nlatent space for the diffusion process, has become the standard strategy for Diffusion Transformers (DiT) ; however, the autoencoder component has barely\nevolved. Most DiTs continue to rely on the original VAE encoder , which\nintroduces several limitations: outdated backbones that compromise\narchitectural simplicity, low-dimensional latent spaces that restrict\ninformation capacity, and weak representations that result from purely\nreconstruction-based training and ultimately limit generative quality. In this\nwork, we explore replacing the VAE with pretrained representation encoders (e.g., DINO , SigLIP , MAE ) paired with trained decoders, forming what we term Representation Autoencoders (RAEs) . These models provide both high-quality\nreconstructions and semantically rich latent spaces, while allowing for a\nscalable transformer-based architecture . Since these latent spaces are\ntypically high-dimensional, a key challenge is enabling diffusion transformers to operate effectively within them. We analyze the sources of this difficulty,\npropose theoretically motivated solutions, and validate them empirically. Our\napproach achieves faster convergence without auxiliary representation alignment\nlosses. Using a DiT variant equipped with a lightweight, wide DDT head, we\nachieve strong image generation results on ImageNet : 1.51 FID at 256x256 (no\nguidance) and 1.13 at both 256x256 and 512x512 (with guidance). RAE offers\nclear advantages and should be the new default for diffusion transformer\ntraining."}
{"url": "https://huggingface.co/papers/2510.14979", "title": "From Pixels to Words -- Towards Native Vision-Language Primitives at\n  Scale", "date": "2025-10-17", "content": "NEO, a novel family of native Vision-Language Models, addresses fundamental constraints and integrates vision and language within a unified framework, achieving competitive performance with limited data.\nThe edifice of native Vision-Language Models (VLMs) has emerged as a rising\ncontender to typical modular VLMs , shaped by evolving model architectures and\ntraining paradigms. Yet, two lingering clouds cast shadows over its widespread\nexploration and promotion: (-) What fundamental constraints set native VLMs apart from modular ones, and to what extent can these barriers be overcome? (-)\nHow to make research in native VLMs more accessible and democratized, thereby\naccelerating progress in the field. In this paper, we clarify these challenges\nand outline guiding principles for constructing native VLMs . Specifically, one\nnative VLM primitive should: (i) effectively align pixel and word\nrepresentations within a shared semantic space ; (ii) seamlessly integrate the\nstrengths of formerly separate vision and language modules; (iii) inherently\nembody various cross-modal properties that support unified vision-language\nencoding, aligning, and reasoning. Hence, we launch NEO, a novel family of native VLMs built from first principles, capable of rivaling top-tier modular\ncounterparts across diverse real-world scenarios. With only 390M image-text\nexamples, NEO efficiently develops visual perception from scratch while\nmitigating vision-language conflicts inside a dense and monolithic model crafted from our elaborate primitives. We position NEO as a cornerstone for\nscalable and powerful native VLMs , paired with a rich set of reusable\ncomponents that foster a cost-effective and extensible ecosystem. Our code and\nmodels are publicly available at: https://github.com/EvolvingLMMs-Lab/NEO."}
{"url": "https://huggingface.co/papers/2510.13802", "title": "Trace Anything: Representing Any Video in 4D via Trajectory Fields", "date": "2025-10-16", "content": "Trace Anything, a neural network, predicts video trajectories in a single pass, achieving state-of-the-art performance and demonstrating efficiency and emergent abilities like motion forecasting.\nEffective spatio-temporal representation is fundamental to modeling,\nunderstanding, and predicting dynamics in videos. The atomic unit of a video,\nthe pixel, traces a continuous 3D trajectory over time, serving as the\nprimitive element of dynamics. Based on this principle, we propose representing\nany video as a Trajectory Field : a dense mapping that assigns a continuous 3D\ntrajectory function of time to each pixel in every frame. With this\nrepresentation, we introduce Trace Anything , a neural network that predicts the\nentire trajectory field in a single feed-forward pass. Specifically, for each\npixel in each frame, our model predicts a set of control points that\nparameterizes a trajectory (i.e., a B-spline ), yielding its 3D position at\narbitrary query time instants. We trained the Trace Anything model on\nlarge-scale 4D data, including data from our new platform, and our experiments\ndemonstrate that: (i) Trace Anything achieves state-of-the-art performance on\nour new benchmark for trajectory field estimation and performs competitively on\nestablished point-tracking benchmarks ; (ii) it offers significant efficiency\ngains thanks to its one-pass paradigm, without requiring iterative optimization\nor auxiliary estimators; and (iii) it exhibits emergent abilities, including goal-conditioned manipulation , motion forecasting , and spatio-temporal fusion .\nProject page: https://trace-anything.github.io/."}
{"url": "https://huggingface.co/papers/2510.09608", "title": "StreamingVLM: Real-Time Understanding for Infinite Video Streams", "date": "2025-10-13", "content": "StreamingVLM is a real-time vision-language model that efficiently processes infinite video streams using a compact KV cache and supervised fine-tuning, achieving high performance on long videos and diverse benchmarks.\nVision-language models ( VLMs ) could power real-time assistants and autonomous\nagents, but they face a critical challenge: understanding near-infinite video\nstreams without escalating latency and memory usage. Processing entire videos\nwith full attention leads to quadratic computational costs and poor performance\non long videos. Meanwhile, simple sliding window methods are also flawed, as\nthey either break coherence or suffer from high latency due to redundant\nrecomputation. In this paper, we introduce StreamingVLM, a model designed for\nreal-time, stable understanding of infinite visual input. Our approach is a\nunified framework that aligns training with streaming inference. During\ninference, we maintain a compact KV cache by reusing states of attention sinks ,\na short window of recent vision tokens , and a long window of recent text\ntokens. This streaming ability is instilled via a simple supervised fine-tuning ( SFT ) strategy that applies full attention on short, overlapped video chunks,\nwhich effectively mimics the inference-time attention pattern without training\non prohibitively long contexts. For evaluation, we build Inf-Streams-Eval , a\nnew benchmark with videos averaging over two hours that requires dense,\nper-second alignment between frames and text. On Inf-Streams-Eval , StreamingVLM\nachieves a 66.18% win rate against GPT-4O mini and maintains stable, real-time\nperformance at up to 8 FPS on a single NVIDIA H100 . Notably, our SFT strategy\nalso enhances general VQA abilities without any VQA-specific fine-tuning,\nimproving performance on LongVideoBench by +4.30 and OVOBench Realtime by\n+5.96. Code is available at https://github.com/mit-han-lab/streaming-vlm."}
{"url": "https://huggingface.co/papers/2510.08668", "title": "Hulu-Med: A Transparent Generalist Model towards Holistic Medical\n  Vision-Language Understanding", "date": "2025-10-17", "content": "Hulu-Med, a transparent medical vision-language model, integrates diverse data modalities and achieves state-of-the-art performance across various clinical tasks with efficient training.\nReal-world clinical decision-making grapples with integrating information\nfrom diverse data modalities, including medical text, 2D/3D images, and video,\nleading to inefficiencies and potential diagnostic oversights. While generalist vision-language models ( VLMs ) offer promise, their medical development faces\nchallenges of opaque pipelines, data scarcity, and architectural inflexibility.\nHere we present Hulu-Med, a transparent medical VLM that unifies understanding\nacross all these modalities. Built upon a unified patch-based vision encoder and an LLM decoder , Hulu-Med was progressively trained on 16.7 million (M)\nsamples to scale from 2D to 3D and video comprehension. The medical-aware token\nreduction enables efficient training, requiring only 4,000 to 40,000 GPU hours\nfor 7B to 32B parameter variants. Extensive evaluation across 30 benchmarks\nexhibits state-of-the-art performance, surpassing leading open-source models\nand competing with proprietary systems in tasks spanning visual\nquestion-answering, medical report generation , and complex reasoning in multilingual and rare disease scenarios . By open-sourcing our complete\npipeline, we establish that high-performance medical VLM can be achieved\ntransparently, providing a foundational tool for accessible and impactful\nclinical AI. Code is released on\nhttps://github.com/ZJUI-AI4H/Hulu-Med{https://github.com/ZJUI-AI4H/Hulu-Med}."}
{"url": "https://huggingface.co/papers/2510.01171", "title": "Verbalized Sampling: How to Mitigate Mode Collapse and Unlock LLM\n  Diversity", "date": "2025-10-15", "content": "Typicality bias in preference data causes mode collapse in LLMs, and Verbalized Sampling is introduced as a prompting strategy to enhance diversity without compromising accuracy or safety.\nPost-training alignment often reduces LLM diversity, leading to a phenomenon\nknown as mode collapse . Unlike prior work that attributes this effect to\nalgorithmic limitations, we identify a fundamental, pervasive data-level\ndriver: typicality bias in preference data , whereby annotators systematically\nfavor familiar text as a result of well-established findings in cognitive\npsychology. We formalize this bias theoretically, verify it on preference\ndatasets empirically, and show that it plays a central role in mode collapse .\nMotivated by this analysis, we introduce Verbalized Sampling , a simple,\ntraining-free prompting strategy to circumvent mode collapse . VS prompts the\nmodel to verbalize a probability distribution over a set of responses (e.g.,\n``Generate 5 jokes about coffee and their corresponding probabilities'').\nComprehensive experiments show that VS significantly improves performance\nacross creative writing (poems, stories, jokes), dialogue simulation , open-ended QA , and synthetic data generation , without sacrificing factual\naccuracy and safety . For instance, in creative writing , VS increases diversity\nby 1.6-2.1x over direct prompting. We further observe an emergent trend that\nmore capable models benefit more from VS. In sum, our work provides a new\ndata-centric perspective on mode collapse and a practical inference-time remedy\nthat helps unlock pre-trained generative diversity."}
{"url": "https://huggingface.co/papers/2510.12798", "title": "Detect Anything via Next Point Prediction", "date": "2025-10-15", "content": "Object detection has long been dominated by traditional coordinate\nregression-based models, such as YOLO, DETR, and Grounding DINO. Although\nrecent efforts have attempted to leverage MLLMs to tackle this task, they face\nchallenges like low recall rate, duplicate predictions, coordinate\nmisalignment, etc. In this work, we bridge this gap and propose Rex-Omni, a\n3B-scale MLLM that achieves state-of-the-art object perception performance. On\nbenchmarks like COCO and LVIS, Rex-Omni attains performance comparable to or\nexceeding regression-based models (e.g., DINO, Grounding DINO) in a zero-shot\nsetting. This is enabled by three key designs: 1) Task Formulation: we use\nspecial tokens to represent quantized coordinates from 0 to 999, reducing the\nmodel's learning difficulty and improving token efficiency for coordinate\nprediction; 2) Data Engines: we construct multiple data engines to generate\nhigh-quality grounding, referring, and pointing data, providing semantically\nrich supervision for training; \\3) Training Pipelines: we employ a two-stage\ntraining process, combining supervised fine-tuning on 22 million data with\nGRPO-based reinforcement post-training. This RL post-training leverages\ngeometry-aware rewards to effectively bridge the discrete-to-continuous\ncoordinate prediction gap, improve box accuracy, and mitigate undesirable\nbehaviors like duplicate predictions that stem from the teacher-guided nature\nof the initial SFT stage. Beyond conventional detection, Rex-Omni's inherent\nlanguage understanding enables versatile capabilities such as object referring,\npointing, visual prompting, GUI grounding, spatial referring, OCR and\nkey-pointing, all systematically evaluated on dedicated benchmarks. We believe\nthat Rex-Omni paves the way for more versatile and language-aware visual\nperception systems."}
{"url": "https://huggingface.co/papers/2510.12403", "title": "Robot Learning: A Tutorial", "date": "2025-10-15", "content": "Robot learning transitions from model-based to data-driven methods, leveraging reinforcement learning and behavioral cloning to develop versatile, language-conditioned models for diverse tasks and robot types.\nRobot learning is at an inflection point, driven by rapid advancements in\nmachine learning and the growing availability of large-scale robotics data.\nThis shift from classical, model-based methods to data-driven, learning-based\nparadigms is unlocking unprecedented capabilities in autonomous systems. This\ntutorial navigates the landscape of modern robot learning, charting a course\nfrom the foundational principles of Reinforcement Learning and Behavioral\nCloning to generalist, language-conditioned models capable of operating across\ndiverse tasks and even robot embodiments. This work is intended as a guide for\nresearchers and practitioners, and our goal is to equip the reader with the\nconceptual understanding and practical tools necessary to contribute to\ndevelopments in robot learning, with ready-to-use examples implemented in\nlerobot."}
{"url": "https://huggingface.co/papers/2510.07962", "title": "LightReasoner: Can Small Language Models Teach Large Language Models\n  Reasoning?", "date": "2025-10-13", "content": "LightReasoner uses behavioral differences between large and small language models to identify and amplify high-value reasoning moments, improving LLM accuracy and efficiency without ground-truth labels.\nLarge language models (LLMs) have demonstrated remarkable progress in\nreasoning, often through supervised fine-tuning (SFT). However, SFT is\nresource-intensive, relying on large curated datasets, rejection-sampled\ndemonstrations, and uniform optimization across all tokens, even though only a\nfraction carry meaningful learning value. In this work, we explore a\ncounterintuitive idea: can smaller language models (SLMs) teach larger language\nmodels (LLMs) by revealing high-value reasoning moments that reflect the\nlatter's unique strength? We propose LightReasoner , a novel framework that\nleverages the behavioral divergence between a stronger expert model (LLM) and a\nweaker amateur model (SLM). LightReasoner operates in two stages: (1) a\nsampling stage that pinpoints critical reasoning moments and constructs supervision examples capturing the expert's advantage through expert-amateur\ncontrast, and (2) a fine-tuning stage that aligns the expert model with these\ndistilled examples, amplifying its reasoning strengths. Across seven mathematical benchmarks , LightReasoner improves accuracy by up to 28.1%, while\nreducing time consumption by 90%, sampled problems by 80%, and tuned token\nusage by 99%, all without relying on ground-truth labels. By turning weaker\nSLMs into effective teaching signals, LightReasoner offers a scalable and\nresource-efficient approach for advancing LLM reasoning. Code is available at:\nhttps://github.com/HKUDS/ LightReasoner"}
{"url": "https://huggingface.co/papers/2510.12764", "title": "AnyUp: Universal Feature Upsampling", "date": "2025-10-17", "content": "AnyUp is a feature-agnostic upsampling method that generalizes across different vision features and resolutions without requiring re-training.\nWe introduce AnyUp, a method for feature upsampling that can be applied to\nany vision feature at any resolution, without encoder-specific training.\nExisting learning-based upsamplers for features like DINO or CLIP need to be\nre-trained for every feature extractor and thus do not generalize to different\nfeature types at inference time. In this work, we propose an inference-time feature-agnostic upsampling architecture to alleviate this limitation and\nimprove upsampling quality. In our experiments, AnyUp sets a new state of the\nart for upsampled features, generalizes to different feature types, and\npreserves feature semantics while being efficient and easy to apply to a wide\nrange of downstream tasks ."}
{"url": "https://huggingface.co/papers/2510.14974", "title": "pi-Flow: Policy-Based Few-Step Generation via Imitation Distillation", "date": "2025-10-17", "content": "Policy-based flow models enable efficient and high-quality image generation by distilling teacher models into student models with dynamic flow velocities, improving diversity and quality.\nFew-step diffusion or flow-based generative models typically distill a velocity-predicting teacher into a student that predicts a shortcut towards\ndenoised data. This format mismatch has led to complex distillation procedures\nthat often suffer from a quality-diversity trade-off. To address this, we\npropose policy-based flow models (pi-Flow). pi-Flow modifies the output\nlayer of a student flow model to predict a network-free policy at one timestep.\nThe policy then produces dynamic flow velocities at future substeps with\nnegligible overhead, enabling fast and accurate ODE integration on these\nsubsteps without extra network evaluations. To match the policy's ODE\ntrajectory to the teacher's, we introduce a novel imitation distillation approach, which matches the policy's velocity to the teacher's along the\npolicy's trajectory using a standard ell_2 flow matching loss. By simply\nmimicking the teacher's behavior, pi-Flow enables stable and scalable\ntraining and avoids the quality-diversity trade-off. On ImageNet 256^2, it\nattains a 1- NFE FID of 2.85, outperforming MeanFlow of the same DiT\narchitecture. On FLUX.1-12B and Qwen-Image-20B at 4 NFE s, pi-Flow achieves\nsubstantially better diversity than state-of-the-art few-step methods, while\nmaintaining teacher-level quality."}
{"url": "https://huggingface.co/papers/2510.13998", "title": "BitNet Distillation", "date": "2025-10-17", "content": "BitNet Distillation fine-tunes large language models to 1.58-bit precision using SubLN, multi-head attention distillation, and continual pre-training, achieving comparable performance with significant memory and inference speed improvements.\nIn this paper, we present BitNet Distillation ( BitDistill ), a lightweight\npipeline that fine-tunes off-the-shelf full-precision LLMs (e.g., Qwen ) into\n1.58-bit precision (i.e., ternary weights {-1, 0, 1}) for specific downstream\ntasks, achieving strong task-specific performance with minimal computational\ncost. Specifically, BitDistill incorporates three key techniques: the SubLN module, as introduced in BitNet; multi-head attention distillation , based on\nMiniLM; and continual pre-training , which serves as a crucial warm-up step to\nmitigate the scalability issue of the performance gap between finetuned\nfull-precision and 1.58-bit LLMs on specific tasks. Experimental results show\nthat BitDistill achieves performance comparable to the full-precision\ncounterpart models across model size, while enabling up to 10x memory savings and 2.65x faster inference on CPUs. Code is available at\nhttps://github.com/microsoft/BitNet."}
{"url": "https://huggingface.co/papers/2510.11712", "title": "DiT360: High-Fidelity Panoramic Image Generation via Hybrid Training", "date": "2025-10-14", "content": "DiT360 framework enhances panoramic image generation by hybrid training on perspective and panoramic data, incorporating cross-domain knowledge and hybrid supervision to improve boundary consistency and image fidelity.\nIn this work, we propose DiT 360, a DiT -based framework that performs hybrid\ntraining on perspective and panoramic data for panoramic image generation. For\nthe issues of maintaining geometric fidelity and photorealism in generation\nquality, we attribute the main reason to the lack of large-scale, high-quality,\nreal-world panoramic data , where such a data-centric view differs from prior\nmethods that focus on model design. Basically, DiT 360 has several key modules\nfor inter-domain transformation and intra-domain augmentation, applied at both\nthe pre-VAE image level and the post-VAE token level. At the image level, we\nincorporate cross-domain knowledge through perspective image guidance and panoramic refinement , which enhance perceptual quality while regularizing diversity and photorealism . At the token level, hybrid supervision is applied\nacross multiple modules, which include circular padding for boundary\ncontinuity, yaw loss for rotational robustness, and cube loss for distortion\nawareness. Extensive experiments on text-to-panorama , inpainting , and outpainting tasks demonstrate that our method achieves better boundary\nconsistency and image fidelity across eleven quantitative metrics. Our code is\navailable at https://github.com/Insta360-Research-Team/ DiT 360."}
{"url": "https://huggingface.co/papers/2510.11696", "title": "QeRL: Beyond Efficiency -- Quantization-enhanced Reinforcement Learning\n  for LLMs", "date": "2025-10-14", "content": "QeRL, a quantization-enhanced reinforcement learning framework, accelerates RL training for large language models by combining NVFP4 quantization with Low-Rank Adaptation and an Adaptive Quantization Noise mechanism, achieving significant speedups and improved performance.\nWe propose QeRL, a Quantization-enhanced Reinforcement Learning framework for large language models (LLMs) . While RL is essential for LLMs' reasoning\ncapabilities, it is resource-intensive, requiring substantial GPU memory and\nlong rollout durations. QeRL addresses these issues by combining NVFP4\nquantization with Low-Rank Adaptation (LoRA) , accelerating rollout phase of RL\nwhile reducing memory overhead. Beyond efficiency, our findings show that\nquantization noise increases policy entropy , enhancing exploration , and\nenabling the discovery of better strategies during RL. To further optimize exploration , QeRL introduces an Adaptive Quantization Noise (AQN) mechanism,\nwhich dynamically adjusts noise during training. Experiments demonstrate that\nQeRL delivers over 1.5 times speedup in the rollout phase . Moreover, this is\nthe first framework to enable RL training of a 32B LLM on a single H100 80GB\nGPU, while delivering overall speedups for RL training. It also achieves faster reward growth and higher final accuracy than 16-bit LoRA and QLoRA, while\nmatching the performance of full-parameter fine-tuning on mathematical\nbenchmarks such as GSM8K (90.8%) and MATH 500 (77.4%) in the 7B model. These\nresults establish QeRL as an efficient and effective framework for RL training\nin LLMs."}
{"url": "https://huggingface.co/papers/2510.08673", "title": "Thinking with Camera: A Unified Multimodal Model for Camera-Centric\n  Understanding and Generation", "date": "2025-10-13", "content": "Puffin, a unified multimodal model, integrates language regression and diffusion-based generation to enhance camera-centric spatial understanding and generation by treating camera parameters as language.\nCamera-centric understanding and generation are two cornerstones of spatial\nintelligence, yet they are typically studied in isolation. We present Puffin, a\nunified camera-centric multimodal model that extends spatial awareness along\nthe camera dimension. Puffin integrates language regression and diffusion-based\ngeneration to interpret and create scenes from arbitrary viewpoints. To bridge\nthe modality gap between cameras and vision-language , we introduce a novel\nparadigm that treats camera as language , enabling thinking with camera. This\nguides the model to align spatially grounded visual cues with photographic\nterminology while reasoning across geometric context . Puffin is trained on Puffin-4M , a large-scale dataset of 4 million vision-language -camera triplets.\nWe incorporate both global camera parameters and pixel-wise camera maps ,\nyielding flexible and reliable spatial generation . Experiments demonstrate\nPuffin superior performance over specialized models for camera-centric generation and understanding. With instruction tuning , Puffin generalizes to\ndiverse cross-view tasks such as spatial imagination , world exploration , and photography guidance . We will release the code, models, dataset pipeline, and\nbenchmark to advance multimodal spatial intelligence research."}
{"url": "https://huggingface.co/papers/2510.13054", "title": "VLA-0: Building State-of-the-Art VLAs with Zero Modification", "date": "2025-10-17", "content": "A simple VLA model, VLA-0, outperforms more complex models on robotic manipulation tasks by representing actions as text without additional modifications or large-scale training.\nVision-Language-Action models (VLAs) hold immense promise for enabling\ngeneralist robot manipulation. However, the best way to build them remains an\nopen question. Current approaches often add complexity, such as modifying the\nexisting vocabulary of a Vision-Language Model (VLM) with action tokens or\nintroducing special action heads . Curiously, the simplest strategy of\nrepresenting actions directly as text has remained largely unexplored. This\nwork introduces VLA-0 to investigate this idea. We find that VLA-0 is not only\neffective; it is surprisingly powerful. With the right design, VLA-0\noutperforms more involved models. On LIBERO , a popular benchmark for evaluating\nVLAs, VLA-0 outperforms all existing methods trained on the same robotic data,\nincluding pi_0.5-KI, OpenVLA-OFT and SmolVLA . Furthermore, without\nlarge-scale robotics-specific training, it outperforms methods trained on\nlarge-scale robotic data, like pi_0.5-KI, pi_0, GR00T-N1 and MolmoAct .\nThese findings also translate to the real world, where VLA-0 outperforms SmolVLA , a VLA model pre-trained on large-scale real data. This paper\nsummarizes our unexpected findings and spells out the specific techniques\nrequired to unlock the high performance of this simple yet potent VLA design.\nVisual results, code, and trained models are provided here:\nhttps://vla0.github.io/."}
{"url": "https://huggingface.co/papers/2510.10274", "title": "X-VLA: Soft-Prompted Transformer as Scalable Cross-Embodiment\n  Vision-Language-Action Model", "date": "2025-10-16", "content": "A novel Soft Prompt approach enhances Vision-Language-Action models by using learnable embeddings for diverse robotic data, enabling superior performance across simulations and real-world robots.\nSuccessful generalist Vision-Language-Action (VLA) models rely on effective\ntraining across diverse robotic platforms with large-scale, cross-embodiment ,\nheterogeneous datasets. To facilitate and leverage the heterogeneity in rich,\ndiverse robotic data sources, we propose a novel Soft Prompt approach with\nminimally added parameters, by infusing prompt learning concepts into cross-embodiment robot learning and introducing separate sets of learnable\nembeddings for each distinct data source. These embeddings serve as embodiment-specific prompts , which in unity empower VLA models with effective\nexploitation of varying cross-embodiment features. Our new X-VLA , a neat flow-matching-based VLA architecture , relies exclusively on soft-prompted\nstandard Transformer encoders , enjoying both scalability and simplicity.\nEvaluated across 6 simulations as well as 3 real-world robots, our 0.9B\ninstantiation- X-VLA -0.9B simultaneously achieves SOTA performance over a sweep\nof benchmarks, demonstrating superior results on a wide axes of capabilities,\nfrom flexible dexterity to quick adaptation across embodiments, environments,\nand tasks. Website: https://thu-air-dream.github.io/ X-VLA /"}
